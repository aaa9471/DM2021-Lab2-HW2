{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:è‘‰è»’ç‘‹\n",
    "\n",
    "Student ID: 110062540\n",
    "\n",
    "GitHub ID:aaa9471\n",
    "\n",
    "Kaggle name:jackson_yeh\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beggining the lab, please make sure to download the [Google News Dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) and place it in a folder named \"GoogleNews\" in the same directory as this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "We start by loading the csv files into a single pandas dataframe for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text emotion  intensity\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....   anger      0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...   anger      0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...   anger      0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...   anger      0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...   anger      0.896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing data\n",
    "anger_test = pd.read_csv(\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (3613, 4)\n",
      "Shape of Testing df:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", train_df.shape)\n",
    "print(\"Shape of Testing df: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     1585\n",
       "to      1273\n",
       "a       1145\n",
       "I       1042\n",
       "and      912\n",
       "of       778\n",
       "is       757\n",
       "in       588\n",
       "you      567\n",
       "my       453\n",
       "for      431\n",
       "that     419\n",
       "on       362\n",
       "it       359\n",
       "be       340\n",
       "me       304\n",
       "have     290\n",
       "so       279\n",
       "this     275\n",
       "with     272\n",
       "not      263\n",
       "at       249\n",
       "but      242\n",
       "I'm      238\n",
       "just     238\n",
       "was      219\n",
       "like     216\n",
       "are      213\n",
       "your     209\n",
       "all      198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "pd.Series(' '.join(train_df.text).split()).value_counts()[:30]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     138\n",
       "to      135\n",
       "I       118\n",
       "a       103\n",
       "and      89\n",
       "of       80\n",
       "is       72\n",
       "in       65\n",
       "you      59\n",
       "for      53\n",
       "my       48\n",
       "it       45\n",
       "on       42\n",
       "that     39\n",
       "be       37\n",
       "was      32\n",
       "with     29\n",
       "have     28\n",
       "at       27\n",
       "get      25\n",
       "your     24\n",
       "so       24\n",
       "all      24\n",
       "are      24\n",
       "just     23\n",
       "me       22\n",
       "will     22\n",
       "like     22\n",
       "i        22\n",
       "but      21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(test_df.text).split()).value_counts()[:30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2 Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save our data in Pickle format. The pickle module implements binary protocols for serializing and de-serializing a Python object structure.   \n",
    "  \n",
    "Some advantages for using pickle structure:  \n",
    "* Because it stores the attribute type, it's more convenient for cross-platform use.  \n",
    "* When your data is huge, it could use less space to store also consume less loading time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: https://reurl.cc/0Dzqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Exploratory data analysis (EDA)\n",
    "\n",
    "Again, before getting our hands dirty, we need to explore a little bit and understand the data we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "anger       857\n",
       "fear       1147\n",
       "joy         823\n",
       "sadness     786\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group to find distribution\n",
    "train_df.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "### Using Bag of Words\n",
    "Using scikit-learn ```CountVectorizer``` perform word frequency and use these as features to train a model.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build analyzers (bag-of-words)\n",
    "BOW_vectorizer = CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(train_df['text'])\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3613x10115 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51467 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the result\n",
    "train_data_BOW_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_BOW_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add .toarray() to show\n",
    "train_data_BOW_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 10115)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dimension\n",
    "train_data_BOW_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2k17', '2much', '2nd', '30', '300', '301', '30am', '30pm', '30s', '31']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names = BOW_vectorizer.get_feature_names()\n",
    "feature_names[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is done. We can technically feed this into our model. However, depending on the embedding technique you use and your model, your accuracy might not be as high, because:\n",
    "\n",
    "* curse of dimensionality  (we have 10,115 dimension now)\n",
    "* some important features are ignored (for example, some models using emoticons yeld better performance than counterparts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ðŸ˜‚\" in feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using another tokenizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3613, 500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 3, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_BOW_features_500.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change',\n",
       " 'cheer',\n",
       " 'cheerful',\n",
       " 'cheerfully',\n",
       " 'cheering',\n",
       " 'cheery',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'country']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ðŸ˜‚\" in feature_names_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change',\n",
       " 'cheer',\n",
       " 'cheerful',\n",
       " 'cheerfully',\n",
       " 'cheering',\n",
       " 'cheery',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'country']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=500, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "tfidf.fit(train_df['text'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "feature_names[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ðŸ˜‚\" in feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model\n",
    "### 3.1 Decision Trees\n",
    "Using scikit-learn ```DecisionTreeClassifier``` performs word frequency and uses these as features to train a model.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 500)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 500)\n",
      "y_test.shape:  (347,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'sadness', 'joy', 'anger', 'joy', 'fear', 'joy', 'sadness',\n",
       "       'fear', 'anger'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Results Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the results of our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.99\n",
      "testing accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.65      0.64        84\n",
      "        fear       0.66      0.69      0.68       110\n",
      "         joy       0.70      0.68      0.69        79\n",
      "     sadness       0.65      0.59      0.62        74\n",
      "\n",
      "    accuracy                           0.66       347\n",
      "   macro avg       0.66      0.66      0.66       347\n",
      "weighted avg       0.66      0.66      0.66       347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 20  5  4]\n",
      " [16 76  8 10]\n",
      " [ 7  8 54 10]\n",
      " [ 9 11 10 44]]\n"
     ]
    }
   ],
   "source": [
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAFaCAYAAACwk/5IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAyDElEQVR4nO3debxVZfn+8c/FoKDILIiiooKapgioIc6i4JSSGqCmOCQ556ypmaWphWUDOeCQZOacVg6A4oSmCJg4hvbLzAEQkEFQiOH+/bHWwQ1f2Gdzztl7nXW43r72i72G/ax7cfA+z77Xs56liMDMzMqrUdYBmJmtDZxszcwqwMnWzKwCnGzNzCrAydbMrAKcbM3MKqBJ1gGYmdXUBs3bxtJli2vVxpf/mz86Ig6so5BWy8nWzHJr6bLFdOvUq1ZtvP7Bc+3rKJyiXEYwM6sAJ1szswpwGcHMckxI+egzOtmaWa41QlmHUJJ8/EowM8s5J1szswpwGcHMckuAlI8ygpOtmeVaI18gMzMrMyk3Pdt8/EowM8s5J1szswpwGcHMck05GWfrZGtmuSXyc4EsH1GameWck62ZWQW4jGBmuZaXoV9OtmaWY6JRTpKtywhmZhXgZGtmVgEuI5hZbglQTvqMTrZmlmu+QGZmVm7CF8jMzBoCSdtIeq3gNU/SOZLaSnpS0nvpn22KteNka2ZWRERMiYidImInoBfwBfAwcAkwNiK6AWPT5dVysjWzHFOt/1tDfYH/FxEfAIcDI9P1I4EBxT7omq2Z5VYGE9EMBu5J33eMiKnp+2lAx2IfdM/WzNZ27SVNLHgNXdVOktYBDgMeWHlbRAQQxQ7inq2Zre1mRsTOJex3EPBqRExPl6dL6hQRUyV1Aj4t9mH3bM0s15Q+h6ymrzVwNF+VEAD+CgxJ3w8B/lLsw+7ZmlmOVWYiGknrAwcA3ytYfR1wv6STgQ+AgcXacLI1M6tGRCwA2q20bhbJ6ISSuIxgZlYB7tmaWW4lE9Hk43ZdJ1szy7W8PPDRydbM8kv5mfUrH78SzMxyzsnWzKwCnGytrCQ1l/Q3SXMl/Z/bHNegnWMljanL2LIiaU9JU7KOoyFQOs62Nq9KcbI1ACQdk94XPl/SVElPSNqjDpo+imSCjnYR8e2aNhIRd0dEvzqIp6wkhaSuxfaJiHERsU2lYmroKjzrV4052RqSzgN+BVxDkhg3A24kmUKutjYH3o2IJXXQVu5J8kXptZST7VpOUivgJ8AZEfHniFgQEYsj4m8RcWG6z7qSfiXpk/T1K0nrptv2kfSRpPMlfZr2ik9Mt/0YuAIYlPaYT5Z0paQ/Fhy/S9obbJIunyDp35I+l/S+pGML1r9Q8Lk+kiak5YkJkvoUbHtW0lWSXkzbGSOp/WrOvyr+iwriHyDpYEnvSvpM0qUF++8q6SVJc9J9h6ezQSHp+XS3yen5Dipo/2JJ04DfV61LP7NVeoye6fLGkmZI2qc2P1erf5xsbTegGcnM86tzGdAb2AnoDuwKXF6wfSOgFbAJcDLwO0ltIuJHJL3l+yKiRUTcXiyQ9P7z3wAHRcQGQB/gtVXs1xZ4LN23HfBL4DFJhbdTHgOcCHQA1gEuKHLojUj+DjYh+eVwK/Adkln59wR+KGmLdN+lwLlAe5K/u77A6QARsVe6T/f0fO8raL8tSS9/hen7IuL/ARcDf5S0HvB7YGREPFskXitQwYloasXJ1tqRTDFX7Gv+scBPIuLTiJgB/Bg4rmD74nT74oh4HJgP1LQmuQz4uqTmETE1It5axT6HAO9FxF0RsSQi7gH+CXyzYJ/fR8S7EfElcD/JL4rVWQz8NCIWA/eSJNJfR8Tn6fHfJvklQ0RMioiX0+P+B7gF2LuEc/pRRCxK41lBRNwK/AsYD3Qi+eVmJfIFMsuLWSSTJxerJW5MMqtRlQ/SdcvbWClZfwG0WNNA0sk+BgGnAlMlPSZp2xLiqYppk4LlaWsQz6yIWJq+r0qG0wu2f1n1eUlbS3pU0jRJ80h67qssURSYERELq9nnVuDrwG8jYlE1+1oOOdnaS8Aiij8/6ROSr8BVNkvX1cQCYL2C5Y0KN0bE6Ig4gKSH90+SJFRdPFUxfVzDmNbETSRxdYuIlsClUO0l7aIz+EtqQXKB8nbgyrRMYg2Mk+1aLiLmktQpf5deGFpPUlNJB0n6ebrbPcDlkjZMLzRdAfxxdW1W4zVgL0mbpRfnflC1QVJHSYentdtFJOWIZato43Fg63S4WhNJg4DtgEdrGNOa2ACYB8xPe92nrbR9OrDlGrb5a2BiRHyXpBZ9c62jXEtUTUTjoV+WCxHxC+A8koteM4APgTOBR9JdrgYmAq8DbwCvputqcqwngfvStiaxYoJslMbxCfAZSS105WRWNY/oocD5JGWQi4BDI2JmTWJaQxeQXHz7nKTXfd9K268ERqajFYpOJg0g6XDgQL46z/OAnlWjMKw6opEa1epVsUiT55SZmeVPm/XbR99tv1n9jkU89Oqdk0p8BlmtuGdrZlYBTrZmZhXgWwfNLLcEFR0rWxtOtmaWa3l5LI7LCGZmFeCe7Sqst8560apZq6zDKIt27darfqcca9J8naxDKJ+cfF2uiQ8/+YRZs+c03BPEyXaVWjVrxQm9h2QdRlmccEyvrEMoq/bdV76xrOFQ44b7RXS/o4+v4ScrO79BbTjZmlluifw88NHJ1sxyLS8924b7vcTMrB5xsjUzqwCXEcws1/IyztbJ1sxyS3LN1szMCjjZmplVgMsIZpZjlX1Cbm042ZpZrrlma2ZmyznZmplVgMsIZpZreRln656tmeVW1ZMaavOq9hhSa0kPSvqnpHck7SapraQnJb2X/tmmunacbM0s1yTV6lWCXwOjImJboDvwDnAJMDYiugFj0+WinGzNzFZDUitgL+B2gIj4X0TMAQ4HRqa7jQQGVNeWk62Z2eptAcwAfi/pH5Juk7Q+0DEipqb7TAM6VteQk62Z5Vct67Vpzba9pIkFr6EFR2gC9ARuiogewAJWKhlERABRXagejVBhF/3hYhZ9uYhly5axbOkyfnfmcPoetz+7HLQLC+YuAGDMHaOZMmFKxpGumamzZnLxbb9j1rw5CDFw7/05vt/BzJk/n/NuuoGPZ85gk/YbcsPp59Jq/RZZh1trPQ4ZQIv116Nxo0Y0btyYsXePrP5DObJ06VL6Hn08nTp04J7hN2QdzmrV0ZMaZkbEzqvZ9hHwUUSMT5cfJEm20yV1ioipkjoBn1Z3ECfbDNx64Qi+mPfFCute/PMLjHtwXEYR1V7jxo25eNBxbN9lS+Z/+SVH/vgS+my/Iw+/+Cy9t9uBoYcMYMRjj3DrY49wwcDvZB1unXjklhtp16Z11mGUxS1338vWW27B5/MXZB1KpiJimqQPJW0TEVOAvsDb6WsIcF3651+qa8tlBKsTHVq3YfsuWwLQonlztuq0CdPnfMbYf0xgwO57AzBg97156h8TsgzTSvDx9OmMGfcC3/nW4VmHUl+cBdwt6XVgJ+AakiR7gKT3gP3T5aLcs62wIDjp2pOBYPxjrzDh8VcA2O2wPvTYvycfv/sxj414jIXzv8w20Fr4aOanvPPf9+m+ZVdmzZ1Lh9bJEMQNW7Vm1ty5GUdXNyQ46oyzETDkyG8x5MhvZR1Snbns57/kynPPZv6CL6rfuR4o900NEfEasKoyQ981acfJtsJuOfdm5s2ax/qt1+fka7/LjA9nMP5vL/P03WMh4IAhB3DI0EN46JcPZh1qjSxYuJCzh/+CHxx9Ai2ar7fCtjUY11jvPXbHCDp16MCMzz7jqNPOoluXLvTp1SPrsGpt9HPjaN+2DTtt9zVemDAp63BK0ign/6TWujKCEpmd97xZ8wBYMGcBb/39LTbdpjPz58wnlgURwStPTKDztp2zCq9WFi9ZwtnDf8E3d9uTfjt/A4B2rVrx6ZzZAHw6ZzZtW7bMMsQ606lDBwA2bNuWg/fdh1ffeivbgOrI+NcmM+rZcex00GGccvGljJswge/94IdZh9Ug1JtkK+kRSZMkvVU19ELSfEk/lTRZ0suSOqbrt0qX35B0taT5Be1cKGmCpNcl/Thd10XSFEl/AN4ENs3iHJs2a8o6zddZ/r5bz25M/890Nmi7wfJ9tt99e6b/Z3oW4dVKRHD5729mq4034cT+hy5fv99OO/PIi88B8MiLz9G3xy5ZhVhnFnz5JZ8vWLD8/bMvj+drW22VcVR144rvn8mbTz7Ga0/8lVt/dg177rILt1x7VdZhNQj1qYxwUkR8Jqk5MEHSQ8D6wMsRcZmknwOnAFeT3D7364i4R9KpVQ1I6gd0A3YlGRXyV0l7Af9N1w+JiJcre1pfadF6A4770XEANGrciNeeeY13J77LwIsG0mmrjYkIZk+fzSO/fjirEGvs1fem8Je/P8/WnTdjwBUXAnDukUdzyiEDOPfGG3jo+afZuP2G3HDauRlHWnszZn3GkPMvAmDJ0qUceWB/+u6+W8ZRrb3yUppSMh43e5KuBKquMnQB+gPPAc0iIiQNAg6IiO9KmkVyB8cSSS2BTyKihaTrgaOAOWk7LYBrSe5dfiYitihy/KHAUICWzVr2On3P0+r6FOuFE47plXUIZdW+++ZZh1A2alxvvojWuf2OPp7X3np7jbNmxw06xuCdj63VsX/z7A2TioyzrTP1omcraR+S4RO7RcQXkp4FmgGL46vfBkupPl4B10bELSu134Xkzo/ViogRwAiATi071Y/fQGZWXI4uutaXX5WtgNlpot0W6F3N/i8DR6bvBxesHw2cJKkFgKRNJHWo82jNzNZQfUm2o4Amkt4hGRxcXV31HOC8dJBxV2AuQESMAf4EvCTpDZJb6zZYXSNmZpVSL8oIEbEIOGgVm1oU7PMgSfIE+BjondZyBwPbFOz3a5ILaCv7et1FbGb1RaOcPKmhXiTbGugFDFdSrJkDnJRtOGaWhTqaiKYicplsI2IcyYzpZma5UF9qtmZmDVoue7ZmZlVKeWhjfeBka2a5lpNc6zKCmVklONmamVWAywhmllvCNVszswpQ2Z/UUFecbM0sv5SfmxpcszUzqwAnWzOzCnAZwcxyzRfIzMzKLJmIJusoSuMygplZBTjZmplVgMsIZpZrrtmamVWAb2owMyszodz0bF2zNTOrACdbM7MKcBnBzPJL+Rln62S7Cu03XJ+h3+uTdRhlcdh512cdQlk99aersw6hbJb9b3HWIZRNLFla4896IhozM1vOydbMrAJcRjCzXMvL0C8nWzPLLU9EY2ZmK3DP1sysGpL+A3wOLAWWRMTOktoC9wFdgP8AAyNi9uracM/WzHKtkVSr1xrYNyJ2ioid0+VLgLER0Q0Ymy6vPs6anZ6ZWf2gWv5XC4cDI9P3I4EBxXZ2sjWz/JJQLV8lCmCMpEmShqbrOkbE1PT9NKBjsQZcszWztV17SRMLlkdExIiV9tkjIj6W1AF4UtI/CzdGREiKYgdxsjWztd3MgjrsKkXEx+mfn0p6GNgVmC6pU0RMldQJ+LRYGy4jmFluCWik2r2qPYa0vqQNqt4D/YA3gb8CQ9LdhgB/KdaOe7ZmlmsVmIimI/BwepwmwJ8iYpSkCcD9kk4GPgAGFmvEydbMrIiI+DfQfRXrZwF9S23HZQQzswpwz9bMci0v89k62ZpZbqnEi1z1gcsIZmYV4GRrZlYBqy0jSDqv2Acj4pd1H07DdtHvfsMzkybSrlUrRt3w2+XrRz7+KHeNepzGjRqxb6+dueS4E7ILsoa6bLkpw4ZfuXy582Yb87tf3sEf73iAY044gsHHfYuly5bx/NMvccO1N2cXaB255b77uftvjyGJr225Bb+69BKarbtu1mHVyLk/H8ZTL4+nfevWPHPHbQDMnjePU6+6mo+mTafzRh255Yof0nqDDTKOdNUaQs22fv7N5thR+/bl+IMO4YLf/mr5upfefJ0nJ4znsV/8mnWbNmXm3DmZxVcb//n3h3z74JMBaNSoEWPHP8TY0c+zy2492PeAPTjyoJNY/L/FtG3XOttA68DUGTO47cGHeP6Pf6D5uutyyg9/xCNjn2bwwQdlHVqNDOrfnxMHDOD71/1s+brh99zLHj16cNYxR/PbP93D8Hvu5fKhp2QY5erlJNeuPtlGxI8rGcjaYNfttuejT6evsO7u0aM49VtHsm7TpgC0b9U6g8jq1jd278WH//2EqR9P5/xLT+P2G+9mcfpk2M9mzck2uDqydOlSFi5aRNPGjfly0SI2at8+65BqrHf3Hflw2rQV1o1+8e88dMMvABjYvx9Hnnt+vU22eXksTrU1W0lbSxor6c10eUdJl5c/tLXD+1M/YcI7b/OtSy5g8BWXMvlf72UdUq0ddNh+PPHXsQBsvsWm9Nx1R+5+5GZ+f99v2H7HbTOOrvY6bbghpw0eTK8jB7LjgCNouf767LPrLlmHVadmzp5Nx3btAOjQti0zZ692TmwrUSkXyG4FfgAsBoiI14HB5QxqdSSdLekdSXdncfxyWLp0KXPnz+fP1w7jB8edwFm//DkRRScPqteaNG3CPvvvzpjHngGgcZPGtGrdkmMHnMovrrmJ62/M/xemOfM+Z9QLL/DK/fcy+ZE/88XChTw4ekzWYZXNGk5FaKtRSrJdLyJeWWndknIEU4LTgQMi4tiaNiCpXo0t3qhdO/p/ozeS6N5taxqpEZ/Nm5d1WDW25z69eefN95g1M+kJTZ86g6dGPQ/Am5PfIZYto03bVlmGWGvPT5zIZp060b5Na5o2acLBe+3JhDfezDqsOtW+TRumz5oFwPRZs2jXunW2Aa1GbScOr+Xk4WuklGQ7U9JWJJPnIukoYGrxj9Q9STcDWwJPSLpM0h2SXpH0D0mHp/t0kTRO0qvpq0+6fp90/V+BtysdezEH7PINXn7zDQD+/cnHLF6ymLYtW2YcVc0ddFhfnvjrU8uXnx4zjl136wHA5lt0pmnTpsz+bG5W4dWJzh07Mumtt/li4UIignGTXqVbl82zDqtO9euzG/envfX7R4+h/+59Mo5o9aTavSqllF7eGcAIYFtJHwPvAzXuWdZURJwq6UBgX+A84OmIOElSa+AVSU+RzCd5QEQslNQNuAeomqeyJ/D1iHi/0rFXOfuG6xn/1pvM/nwefYaexPcHHc2399ufi2/8LQeeexZNmzRh2Jnn5PYrW/Pmzdhtz535yaXXL1/38P2Pc9WwS/jzmDtZvHgJl51/TYYR1o2e22/HofvuTb+TTqFx48bssHVXjjvsm1mHVWOnXfVTXpo8mc/mzqXXwMGcf8IQzjx6MKf+5GrufWIUm3TswC1X/DDrMHNPpdYH03kcG0XE5+UNqWgM/yFJnqOAZnxVzmgL9Ac+AYYDO5E8BXPriFhP0j7AjyJi3yJtDwWGAmzcfsNeL9x8W1nOIWuHnXd99Tvl2FN/ujrrEMpmWTqioyE68NTTmTxlyhr3MjZv2zku6X9GrY59+r2XTqpu8vC6UG3PVlI74EfAHkBIegH4STq9WFYEHBkRU1ZYKV0JTCeZDq0RsLBg84JiDaaPwRgBsMNWXfN7hcpsbaIGNPQLuBeYARwJHJW+v6+cQZVgNHCW0u/bknqk61sBUyNiGXAc0Dij+MysQir0wMdaKyXZdoqIqyLi/fR1NdU8RbICrgKaAq9LeitdBrgRGCJpMrAt1fRmzcwqpZQLZGMkDQbuT5ePIulZVlxEdClY/N4qtr8H7Fiw6uJ0/bPAs2UMzcysqGIT0XxOMtxLwDnAH9NNjYD5wAXlDs7MrBjRMOZG8EQ0Zlbv5WWoZEl3U0lqA3QjGW4FQEQ8X66gzMxKlZcnNZQy9Ou7wPeBzsBrQG/gJWC/skZmZtaAlDIa4fvALsAH6U0BPYA55QzKzKyhKaWMsDC9/RVJ60bEPyVtU/bIzMyqo4ZVs/0onX/gEeBJSbOBD8oZlJlZKRrEaIQqEfGt9O2Vkp4huUtrVFmjMjNrYIqNs227itVvpH+2AD4rS0RmZg1QsZ7tJL66qaFK1XKQzC1rZpYh5WYimmI3NWxRyUDMzGqiIV0gMzOrl/J0gayUcbZmZlZLTrZmZhWwpqMRlosIj0Yws2w1kJsaCkcjbAbMTt+3Bv4L+AKamWUuJ7l29WWEiNgiIrYEngK+GRHtI6IdcCgwplIBmpk1BKXUbHtHxONVCxHxBFB/HyJvZlYPlTL06xNJl/PVkxqOJXlkuJlZ5vJyU0MpPdujgQ2Bh4E/p++PLmdQZmalqBpnW5tXSceRGkv6h6RH0+UtJI2X9C9J90lap7o2qk22EfFZRHwf2CMiekbEOR6JYGZrme8D7xQs/wy4ISK6kgweOLm6BqpNtpL6SHq76kCSuku6sWbxmpnli6TOwCHAbemySJ5U82C6y0hgQHXtlFJGuAHoD8wCiIjJwF5rHLGZWRmkDzao8asEvwIuApaly+2AORGxJF3+CNikukZKmhshIj5cKailpXwurxqv04QWndtlHUZZPPWnq7MOoayGX9Vwp1o+/eK+WYdQ/6xB3bWI9pImFiyPiIgRAJIOBT6NiEmS9qnNQUpJth9K6gOEpKb839qFmVlGSu6dFjMzInZezbbdgcMkHUzydPGWwK+B1pKapL3bzsDH1R2klDLCqcAZJN3kj4GdgNNL+JyZWa5FxA8ionNEdAEGA09HxLHAM8BR6W5DgL9U11YpyXabiDg2IjpGRIeI+A7wtRrGbmbWEFwMnCfpXyQ13Nur+0ApZYTfAj1LWGdmVnGVuqchIp4Fnk3f/xvYdU0+X2zWr91IbsvdUNJ5BZtaAo3XNFAzs7om8nMHWbGe7TokD3ZsAmxQsH4eX9UqzMysBMWeQfYc8JykOyPigwrGZGbW4JRygew2Sa2rFiS1kTS6fCGZmZWuEnMj1IVSLpC1j4g5VQsRMVtSh/KFZGZWohw9qaGUnu0ySZtVLUjanOQJDmZmVqJSeraXAS9Ieo7k4t+ewNCyRmVm1sBUm2wjYpSknkDvdNU5ETGzvGGZmZUmJ1WEouNst42If6aJFr56OsNmkjaLiFfLH56Z2eolk4fnI9sW69meD5wC/GIV24JkPkczs0zlJNcWHWd7SvrnvpULx8ysYSpWRjii2Acj4s91H46ZWcNUrIzwzfTPDiRzJDydLu8L/J3k4Y9mZpnKfc02Ik4EkDQG2C4ipqbLnYA7KxKdmVkxFb4LrDZKualh06pEm5oObLa6nc3M7P8q5aaGselcCPeky4OAp8oX0trhXx/8l1Muv2L58gcff8LFQ7/L9wYPzDCqunPLffdz998eQxJf23ILfnXpJTRbd92sw6qV80ZeyP++WMSyZctYtnQZN5/91UOm+xyxBwcNPZhrB17NF/O+yDDKNXfuz4fx1Mvjad+6Nc/ccRsAs+fN49SrruajadPpvFFHbrnih7TeYINqWrJiqu3ZRsSZwM1A9/Q1IiLOKndgDV3XzTfjmbvu5Jm77uSpO2+nebNmHLx3w3ho8dQZM7jtwYcYffsInrvrTpYuW8YjY5+u/oM5cMfFt3HjGcNXSLQt27eia6+uzJk+O8PIam5Q//7cfd21K6wbfs+97NGjBy/eNZI9evRg+D33ZhRddWr3ZN1K1ntLKSMAvAo8FhHnAqMl+VdcHXp+4iS6bLIJm3baKOtQ6szSpUtZuGgRS5Ys4ctFi9ioffusQyqbg793CGNuG5XbCUN6d9+RNi1X/F969It/Z2D/fgAM7N+PUS+8mEVo1Upuamggs35JOoVkLoS2wFYkD368GfBzlevII08+xRH99s86jDrTacMNOW3wYHodOZBm667DPrvswj677pJ1WLUXwZBrTiQCJj7+ChOfmMC2vb/GvFnzmPb+tKyjq1MzZ8+mY7t2AHRo25aZs+tvrz0vT2oopWd7BsnjfOcBRMR7JMPB6g1Jf886hpr63+LFjB73It/cr+HcOzJn3ueMeuEFXrn/XiY/8me+WLiQB0ePyTqsWrv1/BHcdObvuOvyO/nGN3uz+de7sNfgfRj7hyezDq2sKv11u6EqJdkuioj/VS1IakI9m2IxIvpkHUNNjX3pZXbYZms6tGubdSh15vmJE9msUyfat2lN0yZNOHivPZnwxptZh1Vrn8+aB8CCuQt4++9vs8WOW9BmozaccdPZnDfyQlq2b8lpw8+kRZsWGUdae+3btGH6rFkATJ81i3atW2cbUANQSrJ9TtKlQHNJBwAPAH8rb1hrRtJ8JYZJelPSG5IGpdv+IGlAwb53Szo8s2BX8vCYhlVCAOjcsSOT3nqbLxYuJCIYN+lVunXZPOuwaqXpuk1Zp/k6y9937dmVj6Z8xM8GX8Mvhwzjl0OGMW/mPG46czjzZ8/PONra69dnN+5Pv43cP3oM/Xevv/2ZBlOzJXk++neBN4DvAY8Dt5UzqBo6AtiJZMREe2CCpOdJnud+LvCIpFYkd8MNySrIQgu+/JLnXpnA9ZdcmHUodarn9ttx6L570++kU2jcuDE7bN2V4w77ZvUfrMdatGnBMVd8B4BGjRvx+jOT+dek9zKOqm6cdtVPeWnyZD6bO5deAwdz/glDOPPowZz6k6u594lRbNKxA7dc8cOsw1y1HD2pQRGrrwhIagy8FRHbVi6kNSdpPnAr8EZE3JGuuwt4ICL+KuktYB/gSKBrRFywijaGkk6K3nmjjr1efeShSoVfUcuWLM06hLIaftWorEMom9MvbrjXpA889XQmT5myxllz606bx41DflCrYx/ws9MmRcTOtWqkBEXLCBGxFJhS+FicnPoD8B3gROCOVe0QESMiYueI2Nn1KTOra6WUEdoAb0l6BVhQtTIiDitbVDUzDviepJEkw9T2Aqq+n98JvAJMi4i3swnPzOpa1TjbPCgl2dbTYs0KAngY2A2YnC5fFBHTACJiuqR3gEcyi9DMykKN8pFti81n2ww4FehKcnHs9ohYUqnASiWpHfBZJMXnC/mqN1u4z3pAN76a38HMrKKK1WxHAjuTJNqDWPXjcTIlaWPgJeD6IvvsD7wD/DYi5lYqNjOzQsXKCNtFxA4Akm4nqXnWKxHxCbB1Nfs8BeR7kKeZrVZDqNkurnoTEUvyMpbNzNYiORpnWyzZdpc0L30vkjvI5qXvIyJalj06M7Nq5CTXFn0sTuNKBmJm1pCVOp+tmZnVQinjbM3M6iWRn+kfnWzNLNdykmtdRjAzqwQnWzOz1ZDUTNIrkiZLekvSj9P1W0gaL+lfku6TtE51bTnZmlm+lXf28EXAfhHRnWS+7AMl9QZ+BtwQEV2B2cDJ1TXkZGtm+aWvnpFWjkeZR6Lq0RtN01cA+wEPputHAgOqC9XJ1sysCEmNJb0GfAo8Cfw/YE7BxFwfkTx1vCgnWzNb27WXNLHgNbRwY0QsjYidgM7ArkCNnlzjoV9mlmt1MPRrZimPxYmIOZKeIZk3u7WkJmnvtjPwcXWfd8/WzHJNjVSrV9G2pQ0ltU7fNwcOIJmy9RngqHS3IcBfqovTPVszy60KPBanEzAyffhtI+D+iHhU0tvAvZKuBv5B8hTvopxszcxWIyJeB3qsYv2/Seq3JXMZwcysAtyzNbP8aiCTh5uZ1Xs5ybUuI5iZVYJ7tqsQESxdtLj6HXNo2f8a5nlVOf3ivlmHUDaP/35S1iGUzdyZC7IOoeycbM0sxzx5uJlZReQk17pma2ZWCU62ZmYV4DKCmeVWcrtuPuoITrZmll8iN9/PnWzNLNfy0rPNye8EM7N8c7I1M6sAlxHMLNdyUkVwsjWzfHPN1szMlnOyNTOrAJcRzCy/5JqtmVkF5CfbOtmaWW4Jqn0ceX3hmq2ZWQU42ZqZVYDLCGaWazkp2bpnm6VbH3yQfU44kb2HnMCIBx7MOpxaO+/6X7Djt7/Nfqecsnzd3557nn2/ewqd+/Vn8pR3M4yuds79+TB2OOIo9j3pu8vXzZ43j0EXXsTuxw1h0IUXMefzzzOMsPYkMeCqIRxw3pErrO/9nb4cP+KcbIKqTvoo89q8KsXJNiP//Pf73P3oYzx+802Mvf12nnrpJd7/6OOsw6qVgf0O4O5rrllh3bZdunDrj66g9w47ZBRV3RjUvz93X3ftCuuG33Mve/TowYt3jWSPHj0Yfs+9GUVXN7bv34s5n8xaYV37LTZi3fWbZRRRw+Jkm5H3PviAnl/7Gus1a0aTJo3p3b07jz//fNZh1UrvHXek9QYbrLCu2+ab0XXTTTOKqO707r4jbVqueG6jX/w7A/v3A2Bg/36MeuHFLEKrE+u1acGm3bdiyrOvL18niV0G7cMr9z6bXWANiJNtRrbZYgvGv/4Gn82dyxcLF/L0y+P55NMZWYdla2Dm7Nl0bNcOgA5t2zJz9uyMI6q53sf25ZX7niUilq/b7oCe/Pcf/+LLufX7MeNS7V6VUu8ukEnqAjwaEV/POpZy2rrL5pxxzGAGX3Ah6zVrzvZdu9KosX/35VWl6391adOdtmLh518w6z/T2Wjb5FvIeq1b0GXXbXj8mnsyjq4EOfl7r3fJdm1yzCGHcMwhhwBwzYhb2XjDDTOOyNZE+zZtmD5rFh3btWP6rFm0a90665BqpGO3TdisR1c677gljZs2Zp3m63LEtSexdPESvj1sKABN1mnKt4edwgMX3ppxtPlVtmQraX3gfqAz0Bi4CtgG+CbQHPg78L2ICEm9gDvSj44paOME4DBgPWAr4OGIuCjd1g/4MbAu8P+AEyNivqTr0s8sAcZExAWSvg38CFgKzI2Ivcp13mti5uzZtG/Tho+mT+fxceN47MYbsw7J1kC/Prtx/+gxnHXM0dw/egz9d++TdUg1MvGB55n4QHK9YKNtN2WHg3flyV8+tMI+x484x4m2lsrZsz0Q+CQiDgGQ1Ap4MiJ+ki7fBRwK/A34PXBmRDwvadhK7ewE9AAWAVMk/Rb4Ergc2D8iFki6GDhP0u+AbwHbpkm8ddrGFUD/iPi4YF3mTv7hj5g9bx5NmzTm2nO+T6sNWmQdUq2c/tNreOn11/ls7lx6HX0MFxx/HK032IDLf3cjn82dy/GXX872W23Fn1a6qp8Hp131U16aPDk5t4GDOf+EIZx59GBO/cnV3PvEKDbp2IFbrvhh1mGulfJyu245k+0bwC8k/YykBjtO0pGSLiLpqbYF3pI0DmgdEVWX4u8CDipoZ2xEzAWQ9DawOdAa2A54Ma2TrQO8BMwFFgK3S3oUeDRt40XgTkn3A39eVbCShgJDATbp2LEOTr96fxn+m4ocp1JuvOzSVa4/aI89KhxJ3bvph5etcv39v1i5b5Bv0/75IdP++eH/Wf+Hob+qfDAlqPRFrtooW7KNiHcl9QQOBq6WNBY4A9g5Ij6UdCVQygC+RQXvl5LELJJe8tEr7yxpV6AvcBRwJrBfRJwq6RvAIcAkSb0iYoUBhRExAhgB0H3bbQIzy4H8ZNuyXf6WtDHwRUT8ERgG9Ew3zZTUgiQZEhFzgDmSqro/x5bQ/MvA7pK6psdaX9LWabutIuJx4Fyge7p9q4gYHxFXADOA/A/8NLNcKWcZYQdgmKRlwGLgNGAA8CYwDZhQsO+JwB2SgoILZKsTETPSi2f3SFo3XX058DnwF0nNSHq/56Xbhknqlq4bC0yu3amZma2ZcpYRRgOjV1o9kSQprrzvJNJeaOqidP2dwJ0F+x1a8P5pYJdVHHrXVbR/ROmRm1me5KSK4DvIzCzf1Ei1ehVtW9pU0jOS3pb0lqTvp+vbSnpS0nvpn22qi9PJ1sxs9ZYA50fEdkBv4AxJ2wGXkIyU6kZSmrykuoacbM3MViMipkbEq+n7z4F3gE2Aw4GR6W4jSa5HFeXbdc0sv9L5bCtyqGTelh7AeKBjRExNN00Dqh2c72RrZvlW+1zbXtLEguUR6bj7rw6RDCt9CDgnIuYVJvj0btVqx+Y72ZrZ2m5mROy8uo2SmpIk2rsjouoO1OmSOkXEVEmdgE+rO4hrtmZmq6GkC3s78E5E/LJg01+BIen7IcBfqmvLPVszy7Uy12x3B44D3pD0WrruUuA64H5JJwMfAAOra8jJ1sxyS5Q32UbEC6y+Ktx3TdpysjWz/BK5KYbmJEwzs3xzsjUzqwCXEcwsx/LzoE0nWzPLtbwkW5cRzMwqwMnWzKwCXEYws3zLRxXBydbMckz5eZS5ywhmZhXgnq2Z5ZtHI5iZWRX3bM0s13LSsXWyNbP8KvesX3XJyXYVXp/y7sxOe+/7QQUP2R6YWcHjVZLPLb8qeX6bV+g4mXGyXYWI2LCSx5M0sdhjOfLM55ZfDf38Ks3J1szyS4KcjLN1sjWzXMtLzdZDv+qHEdXvkls+t/xq6OdXUU629cDKz6hvSHxu+dXQz6/SXEYws3zLRxXBPVurOUlnS3pH0t1Zx1Jukv6edQzlIqmLpDezjqOmJNXqVSnu2eaUkn8liohlGYZxOrB/RHxU0wYkNYmIJXUYU1lERJ+sY7BV8Kxfay9Jj0iaJOktSUPTdfMl/VTSZEkvS+qYrt8qXX5D0tWS5he0c6GkCZJel/TjdF0XSVMk/QF4E9g0i3NMY7kZ2BJ4QtJlku6Q9Iqkf0g6vCDecZJeTV990vX7pOv/Cryd1TmsifRnKEnDJL2Z/swGpdv+IGlAwb53V/0dVDjG9SU9lv47e1PSIElXpP+O3pQ0Iv0ljaRe6X6TgTMK2jhB0p8ljZL0nqSfF2zrJ+ml9Gf5gKQW6frrJL2d/lu9Pl337fSYkyU9X+G/inrJybbunRQRvYCdgbMltQPWB16OiO7A88Ap6b6/Bn4dETsAy3uHkvoB3YBdgZ2AXpL2Sjd3A26MiO0jopJ3ua0gIk4FPgH2JTm/pyNi13R5mKT1gU+BAyKiJzAI+E1BEz2B70fE1pWNvFaOIPl5dAf2JznPTsDtwAkAkloBfYDHMojvQOCTiOgeEV8HRgHDI2KXdLk5cGi67++Bs9J/kyvbieTntQMwSNKmktoDl5N8k+kJTATOS/99fwvYPiJ2BK5O27gC6J+2f1g5TjZvnGzr3tlpb+Flkp5nN+B/wKPp9klAl/T9bsAD6fs/FbTRL339A3gV2DZtB+CDiHi5XMHXUD/gEkmvAc8CzYDNgKbArZLeIDnP7Qo+80pEvF/hOGtrD+CeiFgaEdOB54BdIuI5oJukDYGjgYcyKo28ARwg6WeS9oyIucC+ksanP4P9gO0ltQZaR0RVj/OuldoZGxFzI2IhyTePzYHeJD+/F9Of85B0/VxgIXC7pCOAL9I2XgTulHQK0LhM55uQaveqENds65CkfUh6PLtFxBeSniVJPIsjItLdllL937uAayPilpXa7wIsqMOQ64qAIyNiygorpSuB6SQ9wUYk/1NWqY/nURt/AL4DDAZOzCKAiHhXUk/gYOBqSWNJSgQ7R8SH6c+jWQlNLSp4X/XvVcCTEXH0yjtL2hXoCxwFnAnsFxGnSvoGcAgwSVKviJhVi9Nbjfw8ytw927rVCpidJtptSXoDxbwMHJm+H1ywfjRwUkFNbBNJHeo82rozGjiroB7YI13fCpiaXsQ7jnL3cMpvHMnX6sZpL3Yv4JV0253AOQARkUkdWtLGwBcR8UdgGEmpBmBm+m/pqDS+OcAcSXuk248tofmXgd0ldU2Ptb6krdN2W0XE48C5JL9YkbRVRIyPiCuAGWR4faG+cM+2bo0CTpX0DjCF5B9oMecAf5R0WfrZuQARMUbS14CX0vw1n6TXtLRMcdfWVcCvgNclNQLeJ6kN3gg8JOl4kvPLc282gIdJSj+T0+WLImIaQERMT3/uj2QWYVJjHSZpGbAYOA0YQHIxdRowoWDfE4E7JAUwprqGI2KGpBOAeyStm66+HPgc+IukZiS93/PSbcMkdUvXjSX5O1ur6atvt1ZpktYDvoyIkDQYODoiKn4V24pLLwK9GhGrnQYw/Vm+AfRMa6VWATt22zoe/81vqt+xiE0PPmhSJWY3c882W72A4enX7znASdmGYytLv5o/C1xfZJ/9SUYk3OBEW1nK0ThbJ9sMRcQ40hqX1U8R8QlQdHhaRDzFWjD5tdWOL5CZmVWAe7Zmlm85GfrlZGtmueZxtmYrkdRO0mvpa5qkjwuW16mjYzwrqeiVZUn/SW8/LbXNEyQNr310VudE8lic2rwqxD1bq5j0DqKdYPndZfMjYvlVfuVkBjCzmnDP1jIl6U5JN0saD/xc0pWSLijY/mZ6mzKSvqNkZrHXJN0iqegdaZJukjRRyQxsP15p80VKZu56peCuqA0lPaRklqwJknav49O1HFIyo92nKpjzV1JbSU8qmRntSUltqmvHydbqg85An4g4b3U7pHfUDQJ2j4idSO6mq+4208vSweo7AntL2rFg29x0trXhJHe/QTIL2w0RsQvJbdS31eBcrKJqN3F4ifXeO0lmVCt0CcmEPd1I7pC7pLpGXEaw+uCBiKjuVuS+JDeBTEj/B2lOMoVjMQOVzCncBOhEMmvV6+m2ewr+vCF9vz+wXcH/gC2r5qeweqzMZdeIeL7q21WBw4F90vcjSW58ubhYO062Vh8UzpmwhBW/cVXNUiVgZET8oJQGJW0BXEAyBeJsSXey4oxXsYr3jYDe6dSChW2Vckhbu3SMiKnp+2lAx+o+4DKC1Tf/IZ2tKp0ucIt0/VjgqKrZz9KaWbG7tlqSJPG5Sp6McdBK2wcV/PlS+n4McFbVDpJ2qvFZWJ60T2v7Va+ha/LhdPrUaieZcc/W6puHgOMlvQWMB96FZNpCSZcDY9KZxRaTzNW6yqdVRMRkSf8A/gl8SDKZdaE2kl4nmbu1ao7Ws4HfpeubkDxV49S6PDmre3XwzWNmDSaimS6pU0RMVfK0jupKWp71y8zyq/u228ToETfXqo1Oe+9X7axfac320fTxQkgaBsyKiOskXQK0jYiLirXhMoKZWRGS7iEpNW0j6SNJJwPXkTyC6D2SC6vXVdeOywhmlmvlvoC5qkcBpfquSTvu2ZqZVYB7tmaWbzkZmudka2a5JT9d18zMCjnZmplVgMsIZpZfVfPZ5oCTrZnlmmu2Zma2nJOtmVkFuIxgZvmWkzKCk62Z5ZpycoHMZQQzswpwz9bM8kvKTRnBPVszswpwz9bMci0v42ydbM0s33KSbF1GMDOrACdbM7MKcBnBzHItL+NsnWzNLL+Ea7ZmZvYVJ1szswpwGcHMciw/d5ApIrKOwcysRiSNAtrXspmZEXFgXcRTjJOtmVkFuGZrZlYBTrZmZhXgZGtmVgFOtmZmFeBka2ZWAf8fo3NHHmdIjSEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot your confusion matrix\n",
    "my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "plot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer here\n",
    "Obviously, the result is overfitting because the training result is very good but the test result is instead.\n",
    "The final picture show that we can predict correctly most of time, and we can find out which labels are confuse our machine easill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'joy', 'anger', 'fear', 'fear', 'fear', 'anger', 'joy',\n",
       "       'fear', 'fear'], dtype='<U7')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "## build DecisionTree model\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "## training!\n",
    "NB_model = NB_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = NB_model.predict(X_train)\n",
    "y_test_pred = NB_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.79\n",
      "testing accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.65      0.64        84\n",
      "        fear       0.73      0.77      0.75       110\n",
      "         joy       0.78      0.71      0.74        79\n",
      "     sadness       0.64      0.62      0.63        74\n",
      "\n",
      "    accuracy                           0.70       347\n",
      "   macro avg       0.70      0.69      0.69       347\n",
      "weighted avg       0.70      0.70      0.70       347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 15  5  9]\n",
      " [ 9 85  5 11]\n",
      " [11  6 56  6]\n",
      " [12 10  6 46]]\n"
     ]
    }
   ],
   "source": [
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot your confusion matrix\n",
    "my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "plot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer here\n",
    "The Naive Bayes model is better than another. Although its training accuracy is worse than another, its test accuracy is better. The Naive Bayes model is better in f1-score too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other things you can try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there are several things you can try that will affect your results. In order to yield better results, you can experiment by: \n",
    "    * Trying different features (Feature engineering)\n",
    "        -Eg. Word2Vec,PCA,LDA,FastText, Clustering......\n",
    "    * Trying different models\n",
    "    * Analyzing your results and interpret them to improve your feature engineering/model building process\n",
    "    * Iterate through the steps above until finding a satisfying result\n",
    "Remember that you should also consider the task at hand and the model you'll feed the data to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deep Learning\n",
    "\n",
    "We use [Keras](https://keras.io/) to be our deep learning framwork, and follow the [Model (functional API)](https://keras.io/models/model/) to build a Deep Neural Network (DNN) model. Keras runs with Tensorflow in the backend. It's a nice abstraction to start working with NN models. \n",
    "\n",
    "Because Deep Learning is a 1-semester course, we can't talk about each detail about it in the lab session. Here, we only provide a simple template about how to build & run a DL model successfully. You can follow this template to design your model.\n",
    "\n",
    "We will begin by building a fully connected network, which looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fully Connected Network](pics/pic1.png)\n",
    "\n",
    "(source: https://github.com/drewnoff/spark-notebook-ml-labs/tree/master/labs/DLFramework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare data (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 500)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 500)\n",
      "y_test.shape:  (347,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'fear' 'joy' 'sadness']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 3172    sadness\n",
      "3040    sadness\n",
      "3517    sadness\n",
      "1249       fear\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (3613,)\n",
      "y_test.shape:  (347,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (3613, 4)\n",
      "y_test.shape:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  4\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](pics/pic2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32064     \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,484\n",
      "Trainable params: 36,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 5s 9ms/step - loss: 1.3123 - accuracy: 0.3811 - val_loss: 1.2551 - val_accuracy: 0.4640\n",
      "Epoch 2/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.9559 - accuracy: 0.6521 - val_loss: 0.9108 - val_accuracy: 0.6686\n",
      "Epoch 3/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7963 - val_loss: 0.7812 - val_accuracy: 0.7089\n",
      "Epoch 4/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.4152 - accuracy: 0.8395 - val_loss: 0.7822 - val_accuracy: 0.7061\n",
      "Epoch 5/25\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.3340 - accuracy: 0.8766 - val_loss: 0.8357 - val_accuracy: 0.6859\n",
      "Epoch 6/25\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.2713 - accuracy: 0.9073 - val_loss: 0.8515 - val_accuracy: 0.6916\n",
      "Epoch 7/25\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.2279 - accuracy: 0.9272 - val_loss: 0.8965 - val_accuracy: 0.6859\n",
      "Epoch 8/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.1877 - accuracy: 0.9410 - val_loss: 0.9769 - val_accuracy: 0.6916\n",
      "Epoch 9/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9543 - val_loss: 1.0510 - val_accuracy: 0.6916\n",
      "Epoch 10/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9604 - val_loss: 1.1137 - val_accuracy: 0.6888\n",
      "Epoch 11/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.1205 - accuracy: 0.9640 - val_loss: 1.1273 - val_accuracy: 0.6830\n",
      "Epoch 12/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.1089 - accuracy: 0.9712 - val_loss: 1.2287 - val_accuracy: 0.6801\n",
      "Epoch 13/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9701 - val_loss: 1.2548 - val_accuracy: 0.6945\n",
      "Epoch 14/25\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.0956 - accuracy: 0.9712 - val_loss: 1.2847 - val_accuracy: 0.6772\n",
      "Epoch 15/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9709 - val_loss: 1.3368 - val_accuracy: 0.6888\n",
      "Epoch 16/25\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.0800 - accuracy: 0.9745 - val_loss: 1.3161 - val_accuracy: 0.6974\n",
      "Epoch 17/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0769 - accuracy: 0.9745 - val_loss: 1.3736 - val_accuracy: 0.6888\n",
      "Epoch 18/25\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.0773 - accuracy: 0.9732 - val_loss: 1.3810 - val_accuracy: 0.6916\n",
      "Epoch 19/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0661 - accuracy: 0.9762 - val_loss: 1.4495 - val_accuracy: 0.6916\n",
      "Epoch 20/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0729 - accuracy: 0.9765 - val_loss: 1.4786 - val_accuracy: 0.6772\n",
      "Epoch 21/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9801 - val_loss: 1.4678 - val_accuracy: 0.6974\n",
      "Epoch 22/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.9762 - val_loss: 1.5033 - val_accuracy: 0.6859\n",
      "Epoch 23/25\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9790 - val_loss: 1.5162 - val_accuracy: 0.6830\n",
      "Epoch 24/25\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0628 - accuracy: 0.9787 - val_loss: 1.5391 - val_accuracy: 0.6859\n",
      "Epoch 25/25\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0652 - accuracy: 0.9790 - val_loss: 1.5601 - val_accuracy: 0.6945\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.0860907e-07, 3.3097109e-07, 9.9995351e-01, 4.5579589e-05],\n",
       "       [7.8801532e-10, 3.1236912e-08, 9.9878234e-01, 1.2176486e-03],\n",
       "       [1.2124481e-01, 3.0582480e-02, 2.9748955e-01, 5.5068314e-01],\n",
       "       [2.1839760e-07, 9.9956506e-01, 5.6170727e-08, 4.3464446e-04],\n",
       "       [5.1281786e-01, 2.5355348e-01, 1.4480062e-01, 8.8828027e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'joy', 'sadness', 'fear', 'anger'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.381124</td>\n",
       "      <td>1.312321</td>\n",
       "      <td>0.463977</td>\n",
       "      <td>1.255136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.652090</td>\n",
       "      <td>0.955892</td>\n",
       "      <td>0.668588</td>\n",
       "      <td>0.910818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.796291</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.708934</td>\n",
       "      <td>0.781167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.839469</td>\n",
       "      <td>0.415228</td>\n",
       "      <td>0.706052</td>\n",
       "      <td>0.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.876557</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>0.835712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.907279</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>0.851474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.927207</td>\n",
       "      <td>0.227896</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>0.896487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.941046</td>\n",
       "      <td>0.187668</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>0.976906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.954332</td>\n",
       "      <td>0.155329</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>1.051029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.960421</td>\n",
       "      <td>0.138284</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>1.113744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.964019</td>\n",
       "      <td>0.120542</td>\n",
       "      <td>0.682997</td>\n",
       "      <td>1.127318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.971215</td>\n",
       "      <td>0.108907</td>\n",
       "      <td>0.680115</td>\n",
       "      <td>1.228687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.970108</td>\n",
       "      <td>0.098119</td>\n",
       "      <td>0.694524</td>\n",
       "      <td>1.254839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.971215</td>\n",
       "      <td>0.095552</td>\n",
       "      <td>0.677233</td>\n",
       "      <td>1.284652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.970938</td>\n",
       "      <td>0.087829</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>1.336838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.974536</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.697406</td>\n",
       "      <td>1.316097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.974536</td>\n",
       "      <td>0.076939</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>1.373598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.973153</td>\n",
       "      <td>0.077276</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>1.381021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.976197</td>\n",
       "      <td>0.066083</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>1.449465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.976474</td>\n",
       "      <td>0.072888</td>\n",
       "      <td>0.677233</td>\n",
       "      <td>1.478581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.980072</td>\n",
       "      <td>0.062111</td>\n",
       "      <td>0.697406</td>\n",
       "      <td>1.467814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.976197</td>\n",
       "      <td>0.067061</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>1.503315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.978965</td>\n",
       "      <td>0.063564</td>\n",
       "      <td>0.682997</td>\n",
       "      <td>1.516230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.978688</td>\n",
       "      <td>0.062754</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>1.539142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.978965</td>\n",
       "      <td>0.065216</td>\n",
       "      <td>0.694524</td>\n",
       "      <td>1.560096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  accuracy      loss  val_accuracy  val_loss\n",
       "0       0  0.381124  1.312321      0.463977  1.255136\n",
       "1       1  0.652090  0.955892      0.668588  0.910818\n",
       "2       2  0.796291  0.574000      0.708934  0.781167\n",
       "3       3  0.839469  0.415228      0.706052  0.782200\n",
       "4       4  0.876557  0.334000      0.685879  0.835712\n",
       "5       5  0.907279  0.271257      0.691643  0.851474\n",
       "6       6  0.927207  0.227896      0.685879  0.896487\n",
       "7       7  0.941046  0.187668      0.691643  0.976906\n",
       "8       8  0.954332  0.155329      0.691643  1.051029\n",
       "9       9  0.960421  0.138284      0.688761  1.113744\n",
       "10     10  0.964019  0.120542      0.682997  1.127318\n",
       "11     11  0.971215  0.108907      0.680115  1.228687\n",
       "12     12  0.970108  0.098119      0.694524  1.254839\n",
       "13     13  0.971215  0.095552      0.677233  1.284652\n",
       "14     14  0.970938  0.087829      0.688761  1.336838\n",
       "15     15  0.974536  0.080000      0.697406  1.316097\n",
       "16     16  0.974536  0.076939      0.688761  1.373598\n",
       "17     17  0.973153  0.077276      0.691643  1.381021\n",
       "18     18  0.976197  0.066083      0.691643  1.449465\n",
       "19     19  0.976474  0.072888      0.677233  1.478581\n",
       "20     20  0.980072  0.062111      0.697406  1.467814\n",
       "21     21  0.976197  0.067061      0.685879  1.503315\n",
       "22     22  0.978965  0.063564      0.682997  1.516230\n",
       "23     23  0.978688  0.062754      0.685879  1.539142\n",
       "24     24  0.978965  0.065216      0.694524  1.560096"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAubUlEQVR4nO3deXwV9dX48c9J2GUVoiIEEgREEFs1orUuiAugVaEoQmsfq7RS68ZTtVpLtVp9/FlqVawbVorUDRS1qLiL4goGVJRFRHakJuyIQEhyfn+cGxNClpvkzp27nPfrNa9779zhzhkY5sx3vpuoKs4559JXRtgBOOecC5cnAuecS3OeCJxzLs15InDOuTTnicA559Jco7ADqKsOHTpoTk5O2GG4FDV37tz1qpoVxr793HZBquncTrpEkJOTQ35+fthhuBQlIivD2ref2y5INZ3bgT0aEpGJIlIgIp/XsE1/EflERBaIyNtBxeKcc656QdYRTAIGVfeliLQF7gPOUtU+wLkBxuKcc64agSUCVZ0FbKxhk58Bz6jqqsj2BUHF4pxzrnphthrqCbQTkbdEZK6I/E91G4rIxSKSLyL5hYWFcQzRuWD5ue0SQZiJoBFwJHAGMBD4k4j0rGpDVZ2gqnmqmpeVFUqDDucC4ee2SwRhthpaA2xQ1e3AdhGZBfwAWBJiTM45l3bCLBH8BzhORBqJSAvgaGBRiPE451xaCqxEICJPAP2BDiKyBrgRaAygqg+o6iIReRmYD5QC/1TVapuaOuecq0ZpKUyfDmvWwGWX1fmPB5YIVHVkFNuMA8YFFYNzzqW04mKYMgVuuw0WLIC+feGSSyAzs04/42MNOedcstm1Cx56CHr1gvPPt3WPPQbz5tU5CUASDjHhnHNpq6gIHn4Ybr0V1q6FvDx49lk46yzIqP99vScC55xLdCUl8PjjcOONsHw5/PjHMHEinHoqiDT45z0ROOdcolKF556DsWNh4UI4/HCYMQMGDYpJAijjicA55xLNkiXwwgtWCpg7Fw4+GKZOhWHDGvQIqDqeCJxzLmxFRfDOO/Dii5YAvvzS1h96qD0C+sUvoFFwl2tPBM45F6bZs+1Of+1aaNIEBgyAK6+EM86AOE1U5InAOefCMmkSjB4NnTrBM89Y5W/LlnEPwxOBc87FW3ExXH013H03nHyydQpr3z60cDwROOdcLKlaW/+XXoKjjoITT7T2/o0b2/cbNsB558Ebb8CYMTBuXKDP/6PhicA552Jl5Ur41a/g9dfhgAPscQ9AixZw7LFw3HHwyCNWH/Cvf8EvfxlquGV8iAnnnGsoVZgwwcb6+eADuO8+u9gXFMDTT8OoUfb+z3+GnTvh7bcTJgmAlwicc65hKpYCBgywx0JlrX2ysqxF0LBh9nnTJmjeHJo1Cy3cqngicM65+vr4Y6sDUIX777cWQDX1+G3XLn6x1YEnAuecq4/iYnvks88+8P77kJsbdkT15onAOefq4847rUTw1FNJnQTAK4udc67uli2zkUDPPrv8+X8SCywRiMhEESkQkRqnnxSRo0SkWETOCSoW55yLGVWrC2jUCO69N6ajgIYlyBLBJGBQTRuISCZwO/BqgHE451zsTJ5sLYRuv92GhkgBgSUCVZ0FbKxls8uBaUBBUHE451zMFBTA735nE8OMHh12NDETWh2BiHQChgL3R7HtxSKSLyL5hYWFwQfnXJz4uZ1kxoyBbdus81gA8wKEJcwjuQu4VlVLa9tQVSeoap6q5mVlZQUfmXNx4ud2EnnpJXjiCfjjH6F377Cjiakwm4/mAU+KVbR0AE4XkWJVfS7EmJxzziaKWbIEFiyAzz+317fegkMOgeuuCzu6mAstEajq9w1vRWQS8IInAedcqEpL4cILbYrI4mJbl5kJ3bvb8BF//jM0bRpqiEEILBGIyBNAf6CDiKwBbgQaA6jqA0Ht1znn6u2vf7VWQaNG2YW/Tx+bLzjBxgaKtcASgaqOrMO2vwwqDueci8rbb9vz/+HD4aGHUqJ/QLRSp9rbOefq65tvYORIewT0z3+mVRIAH2vIOZcOVKu/uJeUwM9+ZkNEv/wytGoV39gSgJcInHOpbfJkGyH0wgth8eK9v7/pJnjzTZtM5rDD4h9fAvBE4JxLXe++a5PGdOliE8T37g3nnANz59r3r7wCt9xis4VdeGGooYbJHw0551LTsmUwdKgNEf3BB9Yc9O674R//gGnT4NRTbRjpPn1s8Lg05iUC5xLY9u3Qr5/VX7o62LoVzjzTnv8//7zNDJaVZXf/q1bB//t/MH8+7Nplcwq3aBF2xKHyROBcAmvRorxzq4tSSQmMGGE9g59+Gnr23PP71q3h2mthxQr46ivrJ5Dm/NGQcwlMxOZBX7ky7EiSyNVX27hADzxgncKq06xZyncUi5aXCJxLcF27eiKI2oQJcNddcMUVKTVMdNA8ETiX4Lp2tacYrgZFRdYr+JJLYOBAuOOOsCNKKp4InEtwOTnW12nbtrAjSVDz58NRR8H//R9ccIFNJt/In3rXhScC5xJc16726o+HKikuhttug7w8GyJi+nSYODEtewY3lCcC5xLZ7t303TSLbnzlj4cqWrIEjj8err8ehgyxZlVnnhl2VEnLE4Fziay0lD6/PZGf85iXCAC++w7+9Cfo2xe++MJmDJsyBTp0CDuypOaJwLlE1rQpeuCBHJSxPL1LBKrw3HM2RMQtt8C551oHixEj0m6k0CB4InAuwUluLgc3XZG+JYIvv4TTT7fhIlq1sikjH30UOnYMO7KU4YnAuUSXk0OOLk/PRPD443DoofDee3DnnTBvHpx4YthRpRxvY+VcosvNJWvXk6xeXkxa/ZctLrahIA47zFoEeQkgMIGVCERkoogUiEiVo6SIyM9FZL6IfCYi74vID4KKxbmklpNDppbQtHA1O3aEHUwcPfssrFljlcOeBAIV5KOhScCgGr5fDpyoqn2BvwATAozFueSVmwtADitYtSrkWOJp/Hg79jPOCDuSlBdYIlDVWcDGGr5/X1U3RT5+CHQOKhbnklpODgC5pFHLoXnzbFKZyy+HzMywo0l5iVJZPAp4qbovReRiEckXkfzCwsI4huVcsKI6t7Oz0YwMckijlkPjx5dPL+kCF3oiEJGTsERwbXXbqOoEVc1T1bysrKz4BedcwKI6txs3hs6d6SZpUiIoKLCOYhdcAG3bhh1NWgg1EYjIYcA/gbNVdUOYsTiXyCQ3l4ObpEmJYMIEG0308svDjiRthJYIRKQL8AzwC1VdElYcziWFdOlLUFQE991nQ0n36hV2NGkjsEbJIvIE0B/oICJrgBuBxgCq+gBwA9AeuE+si3ixquYFFY9zSS03l/ZFX/P18l1A07CjCc60abBunU/SHGeBJQJVHVnL978CfhXU/p1LKTk5ZKBkfr2aoqLuNGkSdkABGT8eevSAQTW1PHexFnplsXMuCt/3JVjOmjUhxxKUOXPgww+tbiDDL03x5H/bziWDSF+CHFakbsuh8eNtULkLLgg7krTjicC5ZNCpE9qoEbmkYIWxKnz8MUydChddBK1bhx1R2vFE4FwyyMyE7C6p06mspMRGFL3mGjj4YDjiCOsvcdllYUeWljwROJckpFsuBzdO8k5lO3faxb5jRzjuOLj7bqv/uO8+m3ege/ewI0xLaTSmrXNJLieHrrNeTO4SwR//CPfeC8OH20QzgwdDmzZhR5X2PBE4lyxycuiw+7+sW7YDaB52NHX35pvw97/DJZdYCcAlDH805FyyiDQhbbR2JSUlIcdSV5s3wy9/aX0Exo0LOxpXiScC55JFpAlpdslyvv463FDq7LLL4Ouvba7hffYJOxpXiScC55JFhQlqkqrCeMoUeOwxm2msX7+wo3FV8ETgXLI44ABKmzRNrr4Ea9danUC/fnD99WFH46rhicC5ZJGRAV27Jk9fgtJSm1hm1y7497+tn4BLSJ4InEsiGbk59GiUJH0J7r0XXnsN7rgDevYMOxpXA08EziWT3NzkKBGsXg3XXWf9BEaPDjsaVwtPBM4lk5wc2hWvp2DZt2FHUrOrr7ZHQ/fdBzbfiEtgngicSyaRlkMZq1ZQWhpyLNWZOdMGkLvuuu+bvLrE5onAuWQSubAeuHsFBQXhhlKl4mK44gqL8/e/DzsaF6XAEoGITBSRAhH5vJrvRUTGi8hSEZkvIkcEFYtzKSNSIsglQSuM778fPv/chpJonoTDYKSpIEsEk4Ca5psbDPSILBcD9wcYi3OpISuL0mYtErPCuLAQbrgBTj0VhgwJOxpXB4ElAlWdBWysYZOzgclqPgTaikjHoOJxLiWIoDk5idmp7Prr4dtvbaYxryBOKmHWEXQCVlf4vCaybi8icrGI5ItIfmFhYVyCcy4e6nNuZ3bL4aDMBBtmIj8fHn4YrrwSevUKOxpXR0lRWayqE1Q1T1XzsrKywg7HuZip17mdm5tYJYLSUhtUbr/97NGQSzphzkewFsiu8LlzZJ1LAKWlNnJwQQFs3EiVwx6LQLNmey7Nm9u2GzfasmlT+fudO2162tLS8teybQsL91y2bYP27e3asv/+5Uvz5rBli8W2ebP9/ubNUFRkIzCI7PmqWr6vivt9/HHo2ze+f6cxk5ND65LNbPhqM9A25GCAyZNh9myYNMnnG05SYSaC6cBlIvIkcDSwRVXXhRhP0iguhmXLYOHC8mXpUrv4NW0KTZqUv2Zmwu7dNtxLUVH5a0nJ3hdOEdi61S7E69dXffEPQuvWdsHPyrJWh0cdBa1awYYN8M031kl17lxLSiUllgzatoV27ex1v/3seCsnmdLS8uOrfKxNm8bn2AIRaTkkK1eg+sNwH8d/953NOnb00fCLX4QYiGuIwBKBiDwB9Ac6iMga4EagMYCqPgDMAE4HlgLfARcGFUsy2b7dLvJLl9rAjZXvlAsK7Ptdu8r/TJcuNt9HRoZd5Ldts4vorl124WzSZM/k0LJl+d1yxQunql1Uf/QjuyhnZdnn9u2hURVnSmmp7WPnTlt27LBXEdh33z2Xdu3sAl7VXXtGlA8oS0stCTZpEpu/66QV6Uuw/84VbNz4Q9q3DzGWu+6yeQamTIn+H9IlnFoTgYicCbyoqnXqx6iqI2v5XoFL6/Kbqeabb+DFF+Hdd+3C/9VX7DXhSNlFtezCfMghcOaZ0Lu3Lb162d1zOsjI8CQA7NWXILREsH493H47nHWWTUTvklY0JYLzgLtEZBowUVUXBxxTylK1xzjTp9sye3b5XfjBB8Npp0H37uVLdrYlgaruxl0aa9eOkn1akbN9BatWwZFHhhTHLbdYc9HbbgspABcrtV5iVPV8EWkNjAQmiYgC/wKeUNVtQQeY7EpK4IMP4Jln4D//scc6AHl5cNNNdjN12GHe7NrVgQilXXPJXbicFatr3zwQy5bZgHIXXWRFU5fUorrXVNWtIvI00BwYAwwFrhGR8ap6T4DxJaVdu+DNN+HZZ+3iX1BgjzROPhmuvRZ+8hM48MCwo3TJrFH3HHIXLeedsBLB2LFWVL3pppACcLEUTR3BWVhFbndgMtBPVQtEpAWwEPBEEPHf/8I//mE3Sps2WaXs6afD0KH26i3rXKxIbi65vMmqlQrEuTiZnw9PPGE9if2OJiVEUyIYBtwZGTLie6r6nYiMCias5LJ4sU3CNHmyNdUcOtRKzCefbG3rnYu5nBxa6rdsWb4RiGNtsaoVa9u399FFU0g0ieDPwPft+0WkObC/qq5Q1TeCCiwZzJlj9WXPP28X/FGj4H//15pyOheoSMuhxqu+Iq6J4JVX7Lnn3XdDmzbx268LVDQNf58CKjYdLYmsS1tFRdaH5kc/gvffhxtvhFWr7JGQJwEXF3l5APyw8FWKi+O0z5ISKw106wa/+U2cduriIZoSQSNVLSr7oKpFIpK2rbkXL4bzz7eerhddZP1p0qUdv0sgnTqxrvtxnLt0Kl9/PZYuXeKwzzvvhPnzrX7AO3SklGhKBIWRCmMARORsYH1wISUmVZtz44gjYMUKaw768MOeBFx4Np0ynMP4jPXvLAp+Zw8+CNdcYxVgw4cHvz8XV9Ekgt8A14vIKhFZDVwLjA42rMRSUGBNPn/7WzjhBPjsM/v/4FyYMocPoxSh8bNTg93RI4/Yo6DTT7fSgA8lkXKi6VD2FXCMiLSMfP428KgSyGefWRIoKIB77oFLL/XOXy4xdDzyQGZxAoe+MxUbyisATzxhz0BPOQWmTUvy0fpcdaLqUCYiZwB9gGYSuQqq6s0BxpUQZsyA886z9v/vvWePhZyLlojsA+xQ1VIR6Qn0Al5S1d2x+P3WreH5ZsPpX3CpzRN86KGx+Nly06bZiKLHH289I70tdMqqtYwnIg9g4w1djvVcORfoGnBcobvnHhvcrUcPaybqScDVwyzs5qkT8CrwC2wu75jJ7zqMEjJgaowfDz3/PIwYYcNLv/ACtGgR2993CSWah33Hqur/AJtU9SbgR0DPYMMKT3GxTbZ0xRWWCN55BzpVOYGmc7USVf0O+Clwn6qei5WsY2afbvszt1V/SwSqDfuxoiIbDXH4cBg2DA4/3IrFLVvGJFaXuKJJBDsjr9+JyIHAbiAlJ5nfts0u/vfeC1dfbSXjffYJOyqXxEREfgT8HHgxsi4zljvIzoYppcPhiy+sQquuVG1UxEsvteEizj4b3noLLrnEOo95p7G0EE0dwfMi0hYYB8wDFHgoyKDCsHUrDBpkj4EefBAuvjjsiFwKGAP8AXhWVReISDdgZix3kJ0Nd2//KX/L+C0yZYoNZRutzZutEnjuXHv+P2SI1Qmceio0bhzLMF2CqzERiEgG8IaqbgamicgLQDNV3RKP4OJlyxZLAvn5VsL+6U/DjsilAlV9G3gbvv+/tF5Vr4jlPrp0gfVkseOYAbSYOtXGPImmWVtJCYwcCZ9+Cg88YO99VMS0VeOjocisZPdW+LyrLklARAaJyBcislRErqvi+y4iMlNEPhaR+SJyep2ij4HNm+0GaO5ceOopTwIudkTkcRFpHWk99DmwUESuieU+srPtdeUx59k0d598Et0f/MMf4OWX7Tno6NGeBNJcNHUEb4jIMJG6tZ4XkUwsiQwGegMjRaTyDBZjgamqejgwArivLvtoqI0brWT8ySdWHzBkSDz37tJAb1XdCgwBXgJysZZDMVOWCD7JHQqZmTZ3cG0efRTGjbMekv4M1BFdIhiNDTK3S0S2isg2EdkaxZ/rByxV1WWRsYqeBM6utI0CZbcibYBKM/YGZ8MGSwKffWYTyJx5Zrz27NJIYxFpjCWC6ZH+Aw1s2rOnzp3tdemm9nZC19Z66KOP4Fe/gv79baAs54giEahqK1XNUNUmqto68jmacmQnoOL8SWsi6yr6M3C+iKwBZmB9FQJXVGSPgxYuhOeegzPOiMdeXRp6EFgB7APMEpGuQDQ3UVFr1szmvF69Gmv2uXy5Peesyrp1Vuzt2NGeg3qFsIuIpkPZCVUtMdr/SGCSqnYGTgf+HalUqxzDxSKSLyL5hYWFDd7pm2/Cxx/boHGDBzf455yrkqqOV9VOqnq6mpXASRW3icW5nZ1tw6AzdKhd3Kt6PLRzp1WAbdlivYQ7dKjXvlxqiqb5aMXKrWbYI5+5wIBa/txaILvC586RdRWNAgYBqOoHItIM6AAUVNxIVScAEwDy8vIaXLR+/nnrKDlsWEN/ybnqiUgbbBCgshunt4Gbge8bXMTi3M7OhiVLgHbtrKj70EPWDrqoqHzZvBnWrLHKsLo0MXVpIZpB5/Z4ei4i2cBdUfz2R0APEcnFEsAI4GeVtlkFnAxMEpFDsETT8Fv+GqhaIhg40IdOcYGbiLUWKhu3+RfAv7CexjHTpQu8/rqd23L11dYpRsTGSG/SpHwZPNibxbkqRTXoXCVrgENq20hVi0XkMuAVrDflxEinmpuBfFWdDlwFPCQi/4tVov1StaH95Gv26af2PPWmm4Lci3MAHKSqFcudN4nIJ7HeSXY2fPutPfVpe9JJNi6Kc3VQayIQkXsob+mQAfwQ62FcK1WdgVUCV1x3Q4X3C4EfRxlrTEyfbjdLXkHs4mCHiBynqu8CiMiPgR2x3klZE9LVq6Ft21j/uksH0ZQI8iu8LwaeUNX3AooncNOnwzHHWEsL5wL2G2BypK4AYBNwQax3UjZN5erV0LdvrH/dpYNoEsHTwE5VLQHrKCYiLSKjKiaVtWutZd1tt4UdiUsHqvop8AMRaR35vFVExgDzY7mfiiUC5+ojqp7FQPMKn5sDrwcTTrBeeMFevfOYiydV3RrpYQzwu1j/fseO1ql41apY/7JLF9EkgmYVp6eMvE/KWSqefx66dYPelQe6cC5+Yj7RaWamjSDtJQJXX9Ekgu0i8v38XCJyJAFUeAVt+3ZrYnfmmT7nsAtVIK3iunTxRODqL5o6gjHAUyLyNXY3cwA2dWVSef112LULzjorsmLtWutt2a6dTb6RGdP5QlwaE5FtVH3BF/Z8zBoz2dnWh8y5+oimQ9lHItILODiy6otYTb4dT9On2/X++OOx8dd/+9s9B+dq08aSwoEHwqhR8POfQ9OmocXrkpeqtor3PrOz4ZlnoLQUMqIp5ztXQTT9CC4FHlPVzyOf24nISFWN65DRDVFaahXFgwcpjf/6fzB2rHUkOPdc2LRpz+XTTy0RjB0LY8bYWO3JOl1fQYHVIB5xRN2vDhs2WBOrefNg0SLIyYEjj7Tf6tQpuOdrJSU2EuDLL9v+TjkFDjooNvtTtWEW5s61oRhSaB7SLl1sJInCQth//7CjcckmmkdDv1bVipPTbBKRXxPnuQMaYs4cKCwo5catV8HYu+D882HixKpHX1SF116Dv/4Vrr0Wbr0VfvMbG7Vx7VpYtsyW5cvttVEjq4HOzbXXbt3sf+WmTeXblG2/Zo1d6Cpr1Mh6ArVrV760bWvNQcp+MzcX9t239gvif/9r42o/9RS8/bZlwW7dbOjhCy+EAw7Y+8+UlNgF/803YfZsu1BWbILSsSN88439FlgnjCOPhJ49rUvr5s17JtMd1VQh9e4N55xjwxxUjmPHDpg8Gf72N5tgpUUL+Oc/7bsuXeDkky0pHH00tG9vE6lUl9xKSqyb7fr18PnndjxlSa1sYLdZsyLFw9RQsQmpJwJXV1LbiA4i8hlwWNnQD5EJZ+arap84xLeXvLw8zc/Pr33DCsZeu5ue437F/+hkuPJK+Pvfo7tDnjvXJvB46qnyiyDYhajs4lxSYhf6r76yMV4qa9QIuna17bOzq04+ZYOCVS6dbNu253atW9vv7Lff3klDBF580S5wqnaRPvdcu5t+5BFLCo0aWW35r39tv/PGG1Z5MnOm7R+ge3e7yJfd/R9xhO1j+3YrLZVdUOfOteNu3bo8hrJ4WrTYO2GVlMC778Lixfbd8cdbfKecYgOhjR9vJZi8PEvAQ4fa32nFGDdtKv89kT33WVJi32/ebEmg8r9Bnz52LGXHdfjhVQ42JSJzVTWvyvMhYPU5t8vMm2eHNm2aDyfkqlbTuR1NIhgHdMXGVgebqGaVql4d0yijVOf/LDt2MHP/8zhp2/Nw8832yKeujxmWLbMZbHJy7OJf1bR+quWlgJUr7eLUrZs9RmlUnyGdsESwfPneJYvCwj0TRlkpo+yO+9xz7cJX8TiXLLE77EmTyu+KwZLUKafYHfeAAcHfTi5YAE8/bcl1wYLy9YMHw+9/DyeeWPW/T0mJjR3++ed7J8xNmyyxV0yOZUuvXjbaZpQjDCZrIigstPuDu+6yex3nKmtoIsgALsZGCQXrFXmAql4a0yijVNf/LNsHDqX5q/9h5rB/cPLTvw0wspCo2t36d99FN25GUZF1qNi40S783bqF15520SK74z/hhIQZGjlZE4GqFcQuvdSerjlXWU3ndjSthkpFZDZwEDacbgdgWmxDDMh339H89ee5izGcfXsKJgGwi3jLlrZEo0mTxJmI4ZBDbHENJmJPHr0vgauPahOBiPTEZhAbCawHpgCo6knV/ZmEM28eGaUlfNW5PwcdFHYwzgXr+5nKnKujmmpMF2OzkP1EVY9T1XuAKpq8JC6dbT1s2pzaL+RInAue9y529VVTIvgpsA6YKSIPicjJBDBOSpB2vDWblXQh+6gqmkw6l2Kys21++t1J193Tha3aRKCqz6nqCKAXMBMbamI/EblfRE6LU3wNIh/NZjZH+yBzLi1kZ1sr56+/DjsSl2xqbUyvqttV9fHI3MWdgY+BawOPrKG++Ybm36xkNkd7faRLCz4vgauvOo07oKqbVHWCqp5c+9YgIoNE5AsRWSoi11WzzXARWSgiC0Tk8brEU6PICFyLW/UjKytmv+pcwqo4U5lzdVHPnk61i/RAvhc4FZvw/iMRmR6Zp7hsmx7AH4AfR4auiN0EkrNnU0wmO/sc6cNOu7TgJQJXX0GOU9gPWKqqy1S1CHgSOLvSNr8G7lXVTQCqWhCzvc+Zw6LMvhzUNynn0HGuzlq1svERvQmpq6sgE0EnoOK9yZrIuop6Aj1F5D0R+VBEBlX1QyJysYjki0h+YcXhEapTWkrp7Dm8X9LP6wdcQqvzuV0Lb0Lq6iPskcsbAT2A/ljHtYdEpG3ljSL1EnmqmpcVzQP/JUvI2LrFK4pdwqvzuV0L713s6iPIRLAWyK7wuXNkXUVrgOmqultVlwNLsMTQMJGKYm866tKN9y529RFkIvgI6CEiuSLSBBgBTK+0zXNYaQAR6YA9KlrW4D3Pns3Oxi1Z3aLX9xVozqWDgw6yOYUKYlfb5tJAYIlAVYuBy4BXgEXAVFVdICI3i0jZzMGvABtEZCHWae0aVd3Q4J3PmcPilkfR85BMbzHk0spJkZHAXn893Dhccgms+SiAqs4AZlRad0OF9wr8LrLExs6d8OmnvNvsKq8fcGnn8MNt3qRXXoGf/SzsaFyyCLuyOPY+/hh27+aNbf28fsClncxMm4751VdtjgLnopF6iaBCRbGXCFw6Ou00m7r6s8/CjsQli9RLBLNns71dJ9ZxoCcCl5ZOiwwJ+eqr4cbhkkdKJoLlWUfTuDE+GY1LS5062ZTVr7wSdiQuWaRWIli/HpYtIz/zaHr2rP+c8c4lu9NOg3fesamsnatNaiWCSP3Aa1t8aAmX3gYOhF27LBk4V5vUSgSzZ6MZGTy/Ls8TgUtrxx8PTZv64yEXndRKBHPmsPOgPmzTlp4IXFpr0QJOOMErjF10UicRqMKcOazrbBPVex8Cl+5OOw0WLIA1a8KOxCW61EkES5fCxo18vs/RiEDPnmEH5Fy4ypqRvvZauHG4xJc6iSBSUfxO0dHk5kLz5iHH41zI+vaFAw7wegJXu9RJBLNnQ4sWvP51b38s5BwgYqWC116DkpKwo3GJLHUSQZ8+lP56NIu+bOQVxc5FDBwIGzfCvHlhR+ISWeokgtGj+erSv7NrF54InIs45RR79dZDriapkwiARYvs1ROBc2a//Wxoak8EriaeCJxLcQMHwvvvw9atYUfiElXKJYIDD4Q2bcKOxLnEcdppUFwMb70VdiQuUQWaCERkkIh8ISJLReS6GrYbJiIqInkN2d/ChV4acK6yY4+FffbxZqSueoElAhHJBO4FBgO9gZEislfDThFpBVwJzG7I/lRh8WJPBM5V1rSpzVo2ZQps2RJ2NC4RBVki6AcsVdVlqloEPAmcXcV2fwFuB3Y2ZGdr18K2bT60hHNVGTsWNmyAcePCjsQloiATQSdgdYXPayLrviciRwDZqvpiTT8kIheLSL6I5BcWFla5jVcUu2QUzbkdC0ceCSNGwJ13wrp1ge3GJanQKotFJAP4O3BVbduq6gRVzVPVvKysrCq3WbjQXj0RuGQSzbkdK3/5CxQVwc03B7obl4SCTARrgewKnztH1pVpBRwKvCUiK4BjgOn1rTBetAjatbN20865vXXvDqNHw0MPwZIlYUfjEkmQieAjoIeI5IpIE2AEML3sS1XdoqodVDVHVXOAD4GzVDW/Pjs77ji48kobX8U5V7U//QmaNbM6A+fKBDarr6oWi8hlwCtAJjBRVReIyM1AvqpOr/kX6ub882P5a86lpv33h6uussdDc+ZAv35hR+QSQaB1BKo6Q1V7qupBqnprZN0NVSUBVe1f39KAcy56V10FWVlw3XXW7Nq5lOpZ7JyrXevW9oho5kwfg8gZTwTOpaHRoyE3F669FkpLw47Ghc0TgXNpqEkTuOUW+PRTePTRsKNxYfNE4FyaGjECjjkGxoyxnvkufXkicC5NZWTAI4/Arl1w0UVecZzOPBE4l8Z69oQ77rBK4/vuCzsaFxZPBM6ludGjYfBguPpqG8HXpR9PBM6lORF4+GGbs+D882H37rAjcvHmicA5R8eOMGECzJ1rg9O59OKJwDkHwE9/ChdcALfeCh9+GHY0Lp48ETjnvnf33ZCdbY+Itm0LOxoXL54InHPfa9MGJk+GFSvg9NM9GaQLTwTOuT2ccAI8/jh88IG1JvJkkPo8ETjn9jJ8ODz5pNUVDBoEW7eGHZELkicC51yVzjkHpkyxeQsGDoQtW8KOyAXFE4FzrlrDhsHUqZCf78kglXkicM7VaOhQeOop62Nw2mmwcWPYEblY80TgnKvVkCEwbRp88olVJvtopakl0EQgIoNE5AsRWSoi11Xx/e9EZKGIzBeRN0Ska5DxOOfq76yz4OWXYdUq+PGP4csvw47IxUpgiUBEMoF7gcFAb2CkiPSutNnHQJ6qHgY8Dfw1qHiccw130kk2xeX27ZYMPv447IhcLARZIugHLFXVZapaBDwJnF1xA1WdqarfRT5+CHQOMB7nXAwceSS8+y40bw4nnghvvRV2RK6hgkwEnYDVFT6viayrzijgpaq+EJGLRSRfRPILCwtjGKJz4UrWc/vgg+G996BzZ+tn8OyzYUfkGiIhKotF5HwgDxhX1feqOkFV81Q1LysrK77BORegZD63O3eGd96BH/7QBqz7/e99COtkFWQiWAtkV/jcObJuDyJyCvBH4CxV3RVgPM65GGvf3uoMLrkExo2D44+3cYpccgkyEXwE9BCRXBFpAowAplfcQEQOBx7EkkBBgLE45wLSvLlNc/nUU7BokZUQpk0LOypXF4ElAlUtBi4DXgEWAVNVdYGI3CwiZ0U2Gwe0BJ4SkU9EZHo1P+ecS3DnnGOtiHr2tPeXXgo7d4YdlYtGoyB/XFVnADMqrbuhwvtTgty/cy6+unWzFkXXXw933GHDU1xwAYwaBYccEnZ0rjoJUVnsnEsdTZrA3/5mzUpPPNEmu+ndG447Dv71L+uD4BKLJwLnXCBOPBGeftqGoxg3Dtavh4susvmRL7kEPv007AhdGU8EzrlA7bcfXH21VSS/844NYjdpklUqH3uszYi2Y0fYUaY3TwTOubgQscdDjzxipYQ777SRTC+4ADp1giuugFdfhe++q/23XGx5InDOxd2++8KYMVZKmDnThrd+8EGb86BdOxgwAG67DT76CEpKwo429XkicM6FRgT697dpMTdutNFNL7/c3l9/PfTrZ53WzjzTKqDz86G4OOyoU0+gzUedcy5a++xjJYKBA+1zQQG88YaVGN56C154wda3amWPmHr1gjZt9lzatoUePWz4C5GwjiT5eCJwziWk/faDkSNtAfj6a5g1C95+215nzaq+KWqbNtC3ry2HHVb+vnXr+MWfTDwROOeSwoEHwogRtpQpLoatW20u5S1b7JHSF1/A/Pnw2Wfw2GNw//3l2+fklCeHww6zUVTbt7c6i+bN07cU4YnAOZe0GjWyi/i++5avGzCg/L0qrF5dnhjmz7dlxoy9K6GbNi1PCk2bQmnpnosq9Oljk/MMGGBDaVSVOIqLYdky2LbNkk3jxsEceyx5InDOpSwR6NLFlp/8pHz9zp3WYumrr6wUUXkpKoKMjD2XkhL44AMbXA+shDJggFVor1sHixfbsnRp+XDcLVpYX4n+/a2D3VFHWZLZvdseda1ebcvatZYwOnSArKzy1/btbfvMzL2PTdUejZWViLZutePt16/uf0+eCJxzaadZMzj8cFvqQtWSx5tvWiX2q6/Co49ayaRHD6vAHjLEHjk1b27jLr31FowdW77fffe1xKEa/X5FLFE0bmz7AitxlJbuuV3v3rBgQd2OCTwROOdc1ESge3dbLr7YLubr1tnde1WPgIYPt9cNG6xX9dtv2917dvaeS+fO9kipsNCG4li/3t5v2AC7dtl3u3eXL6pW8d26tVWMl73Wd24jTwTOOVdPIvaIqDbt21tJYciQmrfbd18rTcSbdyhzzrk054nAOefSnCcC55xLc4EmAhEZJCJfiMhSEbmuiu+bisiUyPezRSQnyHicc87tLbBEICKZwL3AYKA3MFJEelfabBSwSVW7A3cCtwcVj3POuaoFWSLoByxV1WWqWgQ8CZxdaZuzgUci758GThZJ107ezjkXjiATQSdgdYXPayLrqtxGVYuBLUD7yj8kIheLSL6I5BcWFgYUrnPx5+e2SwRJUVmsqhNUNU9V87Lq22PCuQTk57ZLBEF2KFsLZFf43Dmyrqpt1ohII6ANsKGmH507d+56EVlZzdcdgPX1CzeppMNxhnWMXUPYJ+DnNulxjJCA53aQieAjoIeI5GIX/BHAzyptMx24APgAOAd4U7XmEThUtdrbJhHJV9W8BkWdBNLhONPhGCtL93M7HY4REvM4A0sEqlosIpcBrwCZwERVXSAiNwP5qjodeBj4t4gsBTZiycI551wcBTrWkKrOAGZUWndDhfc7gXODjME551zNkqKyuA4mhB1AnKTDcabDMdZFOvx9pMMxQgIep9TySN4551yKS7USgXPOuTryROCcc2kuZRJBbQPcJSsRmSgiBSLyeYV1+4rIayLyZeS1XZgxNpSIZIvITBFZKCILROTKyPqUOs768PM6eSXTeZ0SiSDKAe6S1SRgUKV11wFvqGoP4I3I52RWDFylqr2BY4BLI/9+qXacdeLnddL/eyfNeZ0SiYDoBrhLSqo6C+tjUVHFwfoeAYbEM6ZYU9V1qjov8n4bsAgbhyqljrMe/LxOYsl0XqdKIohmgLtUsr+qrou8/y+wf5jBxFJkTorDgdmk8HFGyc/rFJHo53WqJIK0FRmSIyXaAItIS2AaMEZVt1b8LpWO09Uulf69k+G8TpVEEM0Ad6nkGxHpCBB5LQg5ngYTkcbYf5bHVPWZyOqUO8468vM6ySXLeZ0qieD7Ae5EpAk2ZtH0kGMKUtlgfURe/xNiLA0WmYzoYWCRqv69wlcpdZz14Od1Ekum8zplehaLyOnAXZQPcHdruBHFhog8AfTHhq79BrgReA6YCnQBVgLDVbVyxVvSEJHjgHeAz4DSyOrrseepKXOc9eHndfL+eyfTeZ0yicA551z9pMqjIeecc/XkicA559KcJwLnnEtzngiccy7NeSJwzrk054kgSYhIiYh8UmGJ2UBVIpJTcRRI5+LJz+3wBTpnsYupHar6w7CDcC4Afm6HzEsESU5EVojIX0XkMxGZIyLdI+tzRORNEZkvIm+ISJfI+v1F5FkR+TSyHBv5qUwReSgybvqrItI8tINyDj+348kTQfJoXqn4fF6F77aoal/gH1gvVIB7gEdU9TDgMWB8ZP144G1V/QFwBLAgsr4HcK+q9gE2A8MCPRrnyvm5HTLvWZwkRORbVW1ZxfoVwABVXRYZ4Oq/qtpeRNYDHVV1d2T9OlXtICKFQGdV3VXhN3KA1yITZSAi1wKNVfWWOByaS3N+bofPSwSpQat5Xxe7KrwvweuPXGLwczsOPBGkhvMqvH4Qef8+NlolwM+xwa/Apsa7BGwqRBFpE68gnasHP7fjwDNj8mguIp9U+PyyqpY1s2snIvOxO5+RkXWXA/8SkWuAQuDCyPorgQkiMgq7O7oEWIdz4fFzO2ReR5DkIs9R81R1fdixOBdLfm7Hjz8acs65NOclAuecS3NeInDOuTTnicA559KcJwLnnEtzngiccy7NeSJwzrk09/8BBuynjCUkNigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer here\n",
    "f, (ax1,ax2) = plt.subplots(1,2,sharey=True)\n",
    "ax1.plot(training_log['epoch'],training_log['accuracy'],color='blue')\n",
    "ax1.plot(training_log['epoch'],training_log['val_accuracy'],color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax2.plot(training_log['epoch'],training_log['loss'],color='blue')\n",
    "ax2.plot(training_log['epoch'],training_log['val_loss'],color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it is overfit. the model is good at training set but poor at validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "If you don't have a GPU (level is higher than GTX 1060) or you are not good at setting lots of things about computer, we recommend you to use the [kaggle kernel](https://www.kaggle.com/kernels) to do deep learning model training. They have already installed all the librarys and provided free GPU for you to use.\n",
    "\n",
    "Note however that you will only be able to run a kernel for 6 hours. After 6 hours of inactivity, your Kaggle kernel will shut down (meaning if your model takes more than 6 hours to train, you can't train it at once).\n",
    "\n",
    "\n",
    "### More Information for your reference\n",
    "\n",
    "* Keras document: https://keras.io/\n",
    "* Keras GitHub example: https://github.com/keras-team/keras/tree/master/examples\n",
    "* CS229: Machine Learning: http://cs229.stanford.edu/syllabus.html\n",
    "* Deep Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n",
    "* If you want to try TensorFlow or PyTorch: https://pytorch.org/tutorials/\n",
    "https://www.tensorflow.org/tutorials/quickstart/beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Word2Vector\n",
    "\n",
    "We will introduce how to use `gensim` to train your word2vec model and how to load a pre-trained model.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Collecting Cython==0.29.23\n",
      "  Using cached Cython-0.29.23-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\envs\\vvv\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.26\n",
      "    Uninstalling Cython-0.29.26:\n",
      "      Successfully uninstalled Cython-0.29.26\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prepare training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>40345</td>\n",
       "      <td>Im kind of confused.  The one thing i do right...</td>\n",
       "      <td>[Im, kind, of, confused, ., The, one, thing, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>40213</td>\n",
       "      <td>I wonder what would happen if I were a father....</td>\n",
       "      <td>[I, wonder, what, would, happen, if, I, were, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>40690</td>\n",
       "      <td>@makai_kishi_ Her eyes stayed glued to the mon...</td>\n",
       "      <td>[@, makai_kishi_, Her, eyes, stayed, glued, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>20392</td>\n",
       "      <td>When my 4yo is gone I blast gothcore music. Sh...</td>\n",
       "      <td>[When, my, 4yo, is, gone, I, blast, gothcore, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>10531</td>\n",
       "      <td>Paul forever. Paul should have won! Paul playe...</td>\n",
       "      <td>[Paul, forever, ., Paul, should, have, won, !,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "3172  40345  Im kind of confused.  The one thing i do right...   \n",
       "3040  40213  I wonder what would happen if I were a father....   \n",
       "3517  40690  @makai_kishi_ Her eyes stayed glued to the mon...   \n",
       "1249  20392  When my 4yo is gone I blast gothcore music. Sh...   \n",
       "531   10531  Paul forever. Paul should have won! Paul playe...   \n",
       "\n",
       "                                         text_tokenized  \n",
       "3172  [Im, kind, of, confused, ., The, one, thing, i...  \n",
       "3040  [I, wonder, what, would, happen, if, I, were, ...  \n",
       "3517  [@, makai_kishi_, Her, eyes, stayed, glued, to...  \n",
       "1249  [When, my, 4yo, is, gone, I, blast, gothcore, ...  \n",
       "531   [Paul, forever, ., Paul, should, have, won, !,...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check library\n",
    "import gensim\n",
    "\n",
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # if you want to see the training messages, you can use it\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Im', 'kind', 'of', 'confused', '.', 'The', 'one', 'thing', 'i', 'do', 'right', 'now', 'has', 'a', 'great', 'future', ',', 'but', 'on', 'the', 'other', 'hand', 'so', 'does', 'the', 'new', 'thing', '.', '#', 'needhelp']),\n",
       "       list(['I', 'wonder', 'what', 'would', 'happen', 'if', 'I', 'were', 'a', 'father', '.', '#', 'weary']),\n",
       "       list(['@', 'makai_kishi_', 'Her', 'eyes', 'stayed', 'glued', 'to', 'the', 'monitor', ',', 'not', 'turning', 'away', 'for', 'anything'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the training corpus\n",
    "training_corpus = train_df['text_tokenized'].values\n",
    "training_corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training our model\n",
    "\n",
    "You can try to train your own model. More details: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>40345</td>\n",
       "      <td>Im kind of confused.  The one thing i do right...</td>\n",
       "      <td>[Im, kind, of, confused, ., The, one, thing, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>40213</td>\n",
       "      <td>I wonder what would happen if I were a father....</td>\n",
       "      <td>[I, wonder, what, would, happen, if, I, were, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>40690</td>\n",
       "      <td>@makai_kishi_ Her eyes stayed glued to the mon...</td>\n",
       "      <td>[@, makai_kishi_, Her, eyes, stayed, glued, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>20392</td>\n",
       "      <td>When my 4yo is gone I blast gothcore music. Sh...</td>\n",
       "      <td>[When, my, 4yo, is, gone, I, blast, gothcore, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>10531</td>\n",
       "      <td>Paul forever. Paul should have won! Paul playe...</td>\n",
       "      <td>[Paul, forever, ., Paul, should, have, won, !,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "3172  40345  Im kind of confused.  The one thing i do right...   \n",
       "3040  40213  I wonder what would happen if I were a father....   \n",
       "3517  40690  @makai_kishi_ Her eyes stayed glued to the mon...   \n",
       "1249  20392  When my 4yo is gone I blast gothcore music. Sh...   \n",
       "531   10531  Paul forever. Paul should have won! Paul playe...   \n",
       "\n",
       "                                         text_tokenized  \n",
       "3172  [Im, kind, of, confused, ., The, one, thing, i...  \n",
       "3040  [I, wonder, what, would, happen, if, I, were, ...  \n",
       "3517  [@, makai_kishi_, Her, eyes, stayed, glued, to...  \n",
       "1249  [When, my, 4yo, is, gone, I, blast, gothcore, ...  \n",
       "531   [Paul, forever, ., Paul, should, have, won, !,...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_iter = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          vector_size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, epochs=training_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/Fca3MCs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Generating word vector (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84742373,  0.44697565, -0.1829838 , -0.02356605,  0.13429809,\n",
       "       -1.2837704 ,  0.08357092,  1.422237  , -0.49264202, -0.24797975,\n",
       "       -0.65766746, -0.6441507 , -0.31047323,  0.03755169, -0.09555705,\n",
       "       -0.34329942,  0.09376024, -0.31731686,  0.65130925, -0.84294546,\n",
       "        0.32835096,  0.6529729 ,  0.75403726, -0.2533539 ,  0.24068569,\n",
       "       -0.11929299,  0.12433934,  0.6376294 , -0.59826815,  0.27843744,\n",
       "        0.6342807 , -0.32833773,  0.39913687, -0.8668227 , -0.37286577,\n",
       "        0.34376264, -0.2587166 , -0.21430382, -0.19223368, -0.43784863,\n",
       "        0.31661198,  0.02520891, -0.05473905,  0.6933606 ,  0.96336466,\n",
       "       -0.15026426, -0.6501665 , -0.19190714,  0.5223494 ,  0.34992   ,\n",
       "       -0.17177412, -0.27397797, -0.25842288,  0.08878174, -0.56359226,\n",
       "       -0.33797833,  0.04392673, -0.06884766, -0.29441115,  0.14215155,\n",
       "        0.13641696, -0.09715936, -0.05577608,  0.16137451, -0.34119934,\n",
       "        0.7108384 ,  0.7170152 ,  0.3806995 , -1.6213263 ,  0.35594153,\n",
       "       -0.41138428,  0.45722786,  0.9103965 , -0.08602984,  0.59101385,\n",
       "       -0.09721006,  0.9711279 ,  0.18481974, -0.5497662 ,  0.26558352,\n",
       "       -0.82949346,  0.11025067, -0.96957934,  0.7332932 , -0.06582779,\n",
       "       -0.97513455,  0.7267579 ,  0.2533735 , -0.2610807 ,  0.543982  ,\n",
       "        1.0087564 , -0.06656644,  0.3108815 ,  0.2306056 ,  0.9554889 ,\n",
       "       -0.0134489 ,  0.51038885,  0.17562298,  0.87127167,  0.34658435],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the corresponding vector of a word\n",
    "word_vec = word2vec_model.wv['happy']\n",
    "word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('See', 0.9232342839241028),\n",
       " ('Be', 0.9212568998336792),\n",
       " ('smile', 0.9210729002952576),\n",
       " ('birthday', 0.9190670847892761),\n",
       " ('right', 0.9174973368644714),\n",
       " ('blessing', 0.9137982726097107),\n",
       " (\"'you\", 0.9135348200798035),\n",
       " ('help', 0.9124181270599365),\n",
       " ('blessed', 0.9081891179084778),\n",
       " ('depress', 0.9078040719032288)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most similar words\n",
    "word = 'happy'\n",
    "topn = 10\n",
    "word2vec_model.wv.most_similar(word, topn=topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Using a pre-trained w2v model\n",
    "\n",
    "Instead of training your own model ,you can use a model that has already been trained. Here, we see 2 ways of doing that:\n",
    "\n",
    "\n",
    "#### (1) Download model by yourself\n",
    "\n",
    "source: [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('glad', 0.7408890724182129),\n",
       " ('pleased', 0.6632170677185059),\n",
       " ('ecstatic', 0.6626912355422974),\n",
       " ('overjoyed', 0.6599286794662476),\n",
       " ('thrilled', 0.6514049172401428),\n",
       " ('satisfied', 0.6437949538230896),\n",
       " ('proud', 0.636042058467865),\n",
       " ('delighted', 0.627237856388092),\n",
       " ('disappointed', 0.6269949674606323),\n",
       " ('excited', 0.6247665286064148)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## Note: this model is very huge, this will take some time ...\n",
    "model_path = \"./GoogleNews-vectors-negative300.bin.gz\"\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print('load ok')\n",
    "\n",
    "w2v_google_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Using gensim api\n",
    "\n",
    "Other pretrained models are available here: https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('birthday', 0.9577818512916565),\n",
       " ('thank', 0.937666654586792),\n",
       " ('welcome', 0.93361496925354),\n",
       " ('love', 0.9176183342933655),\n",
       " ('miss', 0.9164500832557678),\n",
       " ('hello', 0.9158351421356201),\n",
       " ('thanks', 0.915008544921875),\n",
       " ('merry', 0.9053249359130859),\n",
       " ('bless', 0.902732253074646),\n",
       " ('wish', 0.9013164043426514)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "## If you see `SSL: CERTIFICATE_VERIFY_FAILED` error, use this:\n",
    "import ssl\n",
    "import urllib.request\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "glove_twitter_25_model = api.load(\"glove-twitter-25\")\n",
    "print('load ok')\n",
    "\n",
    "glove_twitter_25_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 king + woman - man = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run one of the most famous examples for Word2Vec and compute the similarity between these 3 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11683205,  0.29575214,  0.09792458, ..., -0.17196307,\n",
       "         0.18764077, -0.0728706 ],\n",
       "       [ 0.1323571 ,  0.35957652,  0.08792136, ..., -0.39939037,\n",
       "         0.11268804, -0.14817514],\n",
       "       [ 0.02626031,  1.1900266 , -0.29648528, ..., -1.2096583 ,\n",
       "         0.2657083 , -1.3684824 ],\n",
       "       ...,\n",
       "       [ 0.22472548,  0.62455684,  0.55519503, ..., -0.7213811 ,\n",
       "         0.6686044 ,  0.051084  ],\n",
       "       [ 0.19837703, -0.03672261,  1.2235596 , ..., -0.8976644 ,\n",
       "         1.90395   ,  1.4502223 ],\n",
       "       [ 0.02357206,  0.01033695,  0.02661289, ..., -0.0280924 ,\n",
       "         0.00246542,  0.0210252 ]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "word2vec_model.wv[training_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clustering: k-means\n",
    "\n",
    "Here we introduce how to use `sklearn` to do the basic **unsupervised learning** approach, k-means.    \n",
    "\n",
    "more details: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic concept\n",
    "\n",
    "![Image](https://i.imgur.com/PEdUf54.png)\n",
    "\n",
    "(img source: https://towardsdatascience.com/k-means-clustering-identifying-f-r-i-e-n-d-s-in-the-world-of-strangers-695537505d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target words:  ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\n"
     ]
    }
   ],
   "source": [
    "# clustering target\n",
    "target_list = ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\n",
    "print('target words: ', target_list)\n",
    "\n",
    "# convert to word vector\n",
    "X = [word2vec_model.wv[word] for word in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: happy \t cluster: 1\n",
      "word: fear \t cluster: 1\n",
      "word: angry \t cluster: 1\n",
      "word: car \t cluster: 0\n",
      "word: teacher \t cluster: 0\n",
      "word: computer \t cluster: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# we have to decide how many cluster (k) we want\n",
    "k = 2\n",
    "\n",
    "# k-means model\n",
    "kmeans_model = KMeans(n_clusters=k)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# cluster result\n",
    "cluster_result = kmeans_model.labels_\n",
    "\n",
    "# show\n",
    "for i in range(len(target_list)):\n",
    "    print('word: {} \\t cluster: {}'.format(target_list[i], cluster_result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](pics/pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cluster membership\n",
    "word = 'student'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cluster membership\n",
    "word = 'sad'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. High-dimension Visualization: t-SNE\n",
    "\n",
    "No matter if you use the Bag-of-words, tf-idf, or word2vec, it's very hard to see the embedding result, because the dimension is larger than 3.  \n",
    "\n",
    "In Lab 1, we already talked about PCA. We can use PCA to reduce the dimension of our data, then visualize it. However, if you dig deeper into the result, you'd find it is insufficient...\n",
    "\n",
    "Our aim will be to create a visualization similar to the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](pics/pic7.png)\n",
    "source: https://www.fabian-keller.de/research/high-dimensional-data-visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would like to introduce another visualization method called t-SNE.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Prepare visualizing target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repare data lists like:\n",
    "    - happpy words\n",
    "    - angry words\n",
    "    - data words\n",
    "    - mining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy_words:  ['happy', 'glad', 'pleased', 'ecstatic', 'overjoyed', 'thrilled']\n",
      "angry_words:  ['angry', 'irate', 'enraged', 'indignant', 'incensed', 'annoyed']\n",
      "data_words:  ['data', 'Data', 'datasets', 'dataset', 'databases', 'statistics']\n",
      "mining_words:  ['mining', 'Mining', 'mines', 'coal_mining', 'mine', 'miner']\n",
      "\n",
      "target words: \n",
      "['happy', 'glad', 'pleased', 'ecstatic', 'overjoyed', 'thrilled', 'angry', 'irate', 'enraged', 'indignant', 'incensed', 'annoyed', 'data', 'Data', 'datasets', 'dataset', 'databases', 'statistics', 'mining', 'Mining', 'mines', 'coal_mining', 'mine', 'miner']\n",
      "\n",
      "color list:\n",
      "['b', 'b', 'b', 'b', 'b', 'b', 'g', 'g', 'g', 'g', 'g', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'y', 'y', 'y', 'y', 'y', 'y']\n"
     ]
    }
   ],
   "source": [
    "word_list = ['happy', 'angry', 'data', 'mining']\n",
    "\n",
    "topn = 5\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Plot using t-SNE (2-dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 862.5x862.5 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.key_to_index.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy_words:  ['happy', 'glad', 'pleased', 'ecstatic', 'overjoyed', 'thrilled', 'satisfied', 'proud', 'delighted', 'disappointed', 'excited', 'happier', 'Said_Hirschbeck', 'elated', 'thankful', 'unhappy']\n",
      "angry_words:  ['angry', 'irate', 'enraged', 'indignant', 'incensed', 'annoyed', 'agitated', 'frustrated', 'furious', 'unhappy', 'outraged', 'livid', 'irritated', 'exasperated', 'angrier', 'Angry']\n",
      "data_words:  ['data', 'Data', 'datasets', 'dataset', 'databases', 'statistics', 'information', 'database', 'Data_System_IPEDS', 'data.The', 'OpenSpirit_enabled', 'datafeeds', 'microdata', 'operator_Telecity', 'GT_Datamaker', 'datawarehouse']\n",
      "mining_words:  ['mining', 'Mining', 'mines', 'coal_mining', 'mine', 'miner', 'uranium_mining', 'miners', 'Mountaintop_removal_coal', 'goldmining', 'alluvial_mining', 'Ok_Tedi_copper', 'uranium_mines', 'coal_mines', 'openpit_mining', 'Mines']\n",
      "\n",
      "target words: \n",
      "['happy', 'glad', 'pleased', 'ecstatic', 'overjoyed', 'thrilled', 'satisfied', 'proud', 'delighted', 'disappointed', 'excited', 'happier', 'Said_Hirschbeck', 'elated', 'thankful', 'unhappy', 'angry', 'irate', 'enraged', 'indignant', 'incensed', 'annoyed', 'agitated', 'frustrated', 'furious', 'unhappy', 'outraged', 'livid', 'irritated', 'exasperated', 'angrier', 'Angry', 'data', 'Data', 'datasets', 'dataset', 'databases', 'statistics', 'information', 'database', 'Data_System_IPEDS', 'data.The', 'OpenSpirit_enabled', 'datafeeds', 'microdata', 'operator_Telecity', 'GT_Datamaker', 'datawarehouse', 'mining', 'Mining', 'mines', 'coal_mining', 'mine', 'miner', 'uranium_mining', 'miners', 'Mountaintop_removal_coal', 'goldmining', 'alluvial_mining', 'Ok_Tedi_copper', 'uranium_mines', 'coal_mines', 'openpit_mining', 'Mines']\n",
      "\n",
      "color list:\n",
      "['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'y']\n"
     ]
    }
   ],
   "source": [
    "word_list = ['happy', 'angry', 'sad', 'fear']\n",
    "\n",
    "topn = 15\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 862.5x862.5 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.key_to_index.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "We start by loading the csv files into a single pandas dataframe for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "score_list = []\n",
    "text_list = []\n",
    "id_list = []\n",
    "hashtags_list = []\n",
    "date_list = []\n",
    "with open(\"tweets_DM.json\",\"r\")as f :\n",
    "    info_list = f.readlines()\n",
    "    for data_json in info_list:\n",
    "        data = json.loads(data_json)\n",
    "#         score_list.append(data['_score'])\n",
    "        text_list.append(''.join(data['_source']['tweet']['text']))\n",
    "        id_list.append(data['_source']['tweet']['tweet_id'])\n",
    "#         hashtags_list.append(data['_source']['tweet']['hashtags'])\n",
    "#         date_list.append(data['_crawldate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = {\"tweet_id\":id_list,\"text\":text_list}\n",
    "data_tweet = pd.DataFrame(data_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>Huge RespectðŸ–’ @JohnnyVegasReal talking about l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>@BBCBreaking Such an inspirational talented pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>And still #libtards won't get off the guy's ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>When you sow #seeds of service or hospitality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>@lorettalrose Will you be displaying some &lt;LH&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>Lord, I &lt;LH&gt; in you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "0        0x28cc61           test   \n",
       "1        0x29e452          train   \n",
       "2        0x2b3819          train   \n",
       "3        0x2db41f           test   \n",
       "4        0x2a2acc          train   \n",
       "...           ...            ...   \n",
       "1867530  0x227e25          train   \n",
       "1867531  0x293813          train   \n",
       "1867532  0x1e1a7e          train   \n",
       "1867533  0x2156a5          train   \n",
       "1867534  0x2bb9d2          train   \n",
       "\n",
       "                                                      text  \n",
       "0        @Habbo I've seen two separate colours of the e...  \n",
       "1        Huge RespectðŸ–’ @JohnnyVegasReal talking about l...  \n",
       "2        Yoooo we hit all our monthly goals with the ne...  \n",
       "3        @FoxNews @KellyannePolls No serious self respe...  \n",
       "4        @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n",
       "...                                                    ...  \n",
       "1867530  @BBCBreaking Such an inspirational talented pe...  \n",
       "1867531  And still #libtards won't get off the guy's ba...  \n",
       "1867532  When you sow #seeds of service or hospitality ...  \n",
       "1867533  @lorettalrose Will you be displaying some <LH>...  \n",
       "1867534                               Lord, I <LH> in you.  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_identification.csv\",sep=\",\")\n",
    "df = pd.merge(df, data_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['identification']=='test']\n",
    "df_train = df[df['identification']=='train']\n",
    "df_label = pd.read_csv(\"emotion.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>Huge RespectðŸ–’ @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>@BBCBreaking Such an inspirational talented pe...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>And still #libtards won't get off the guy's ba...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>When you sow #seeds of service or hospitality ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>@lorettalrose Will you be displaying some &lt;LH&gt;...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>Lord, I &lt;LH&gt; in you.</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "0        0x29e452          train   \n",
       "1        0x2b3819          train   \n",
       "2        0x2a2acc          train   \n",
       "3        0x2a8830          train   \n",
       "4        0x20b21d          train   \n",
       "...           ...            ...   \n",
       "1455558  0x227e25          train   \n",
       "1455559  0x293813          train   \n",
       "1455560  0x1e1a7e          train   \n",
       "1455561  0x2156a5          train   \n",
       "1455562  0x2bb9d2          train   \n",
       "\n",
       "                                                      text       emotion  \n",
       "0        Huge RespectðŸ–’ @JohnnyVegasReal talking about l...           joy  \n",
       "1        Yoooo we hit all our monthly goals with the ne...           joy  \n",
       "2        @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...         trust  \n",
       "3        Come join @ambushman27 on #PUBG while he striv...           joy  \n",
       "4        @fanshixieen2014 Blessings!My #strength little...  anticipation  \n",
       "...                                                    ...           ...  \n",
       "1455558  @BBCBreaking Such an inspirational talented pe...       disgust  \n",
       "1455559  And still #libtards won't get off the guy's ba...       sadness  \n",
       "1455560  When you sow #seeds of service or hospitality ...           joy  \n",
       "1455561  @lorettalrose Will you be displaying some <LH>...         trust  \n",
       "1455562                               Lord, I <LH> in you.         trust  \n",
       "\n",
       "[1455563 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.merge(df_train, df_label)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>39867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>248935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>139101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>63999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>516017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>193437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>48729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>205478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id\n",
       "emotion               \n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = df_test.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (1455563, 4)\n",
      "Shape of Testing df:  (411972, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", df_train.shape)\n",
    "print(\"Shape of Testing df: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return tf.keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 554582     disgust\n",
      "1035980        joy\n",
      "1384065       fear\n",
      "64845        trust\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1455563,)\n"
     ]
    }
   ],
   "source": [
    "# replace the positive with 1, replace the negative with 0\n",
    "y = df_train['emotion']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y[0:4])\n",
    "print('\\ny_train.shape: ', y.shape)\n",
    "y = label_encode(label_encoder, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we load all dataset what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess\n",
    "this part is method 1, use seq2seq model with attention. but the result is tragedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159364</th>\n",
       "      <td>0x246dcf</td>\n",
       "      <td>test</td>\n",
       "      <td>Does using \"just\" in email make you sound less...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233130</th>\n",
       "      <td>0x1f8151</td>\n",
       "      <td>test</td>\n",
       "      <td>When one of your exes is a new hire at your wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311720</th>\n",
       "      <td>0x31c2fc</td>\n",
       "      <td>test</td>\n",
       "      <td>Just came off a LIVE BROADCAST on WEALTH CREAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428024</th>\n",
       "      <td>0x2bead5</td>\n",
       "      <td>test</td>\n",
       "      <td>@NerdyWonka Probably the children of all the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424428</th>\n",
       "      <td>0x29d045</td>\n",
       "      <td>test</td>\n",
       "      <td>Am I on something or were there actually cloud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233327</th>\n",
       "      <td>0x2c89e1</td>\n",
       "      <td>test</td>\n",
       "      <td>Feels like I should have been further but my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776233</th>\n",
       "      <td>0x359ea7</td>\n",
       "      <td>test</td>\n",
       "      <td>@andybevanitv Wow. Just wow. We can do this. &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222789</th>\n",
       "      <td>0x2952f5</td>\n",
       "      <td>test</td>\n",
       "      <td>The car in front of me at On The Grind just pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704139</th>\n",
       "      <td>0x21bec9</td>\n",
       "      <td>test</td>\n",
       "      <td>I don't understand why people still try to giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948549</th>\n",
       "      <td>0x376e20</td>\n",
       "      <td>test</td>\n",
       "      <td>@GOtransit As a friend once said,â€itâ€™s just no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "159364   0x246dcf           test   \n",
       "233130   0x1f8151           test   \n",
       "1311720  0x31c2fc           test   \n",
       "428024   0x2bead5           test   \n",
       "424428   0x29d045           test   \n",
       "...           ...            ...   \n",
       "1233327  0x2c89e1           test   \n",
       "776233   0x359ea7           test   \n",
       "1222789  0x2952f5           test   \n",
       "1704139  0x21bec9           test   \n",
       "948549   0x376e20           test   \n",
       "\n",
       "                                                      text  \n",
       "159364   Does using \"just\" in email make you sound less...  \n",
       "233130   When one of your exes is a new hire at your wo...  \n",
       "1311720  Just came off a LIVE BROADCAST on WEALTH CREAT...  \n",
       "428024   @NerdyWonka Probably the children of all the b...  \n",
       "424428   Am I on something or were there actually cloud...  \n",
       "...                                                    ...  \n",
       "1233327  Feels like I should have been further but my f...  \n",
       "776233   @andybevanitv Wow. Just wow. We can do this. <LH>  \n",
       "1222789  The car in front of me at On The Grind just pa...  \n",
       "1704139  I don't understand why people still try to giv...  \n",
       "948549   @GOtransit As a friend once said,â€itâ€™s just no...  \n",
       "\n",
       "[411972 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(df_train['text'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "sentences = list(df_test['text'])\n",
    "for sen in sentences:\n",
    "    test.append(preprocess_text(sen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training data: 1164450\n",
      "# test data: 291113\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset and test dataset\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_vali)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_vali = tokenizer.texts_to_sequences(X_vali)\n",
    "max_len = 100\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_vali = tf.keras.preprocessing.sequence.pad_sequences(X_vali, padding='post', maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.texts_to_sequences(test)\n",
    "test =  tf.keras.preprocessing.sequence.pad_sequences(test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 100]), TensorShape([16, 8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "vali_dataset = tf.data.Dataset.from_tensor_slices((X_vali, y_vali))\n",
    "vali_dataset = vali_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize the raw data and seperate the training set to train and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='Softmax',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data with shape == (batch_sizeï¼Œmax_length)  -> (128, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (16, 100, 256)\n",
      "Encoder Hidden state shape: (batch size, units) (16, 256)\n",
      "tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(256,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sample input\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LuongAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        # TODO: Complete the function.\n",
    "\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # TODO: Implement the Luong attention.\n",
    "        #query = sample_hidden, values = sample_output\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "#         score = self.V(tf.matmul(self.attn(query).transpose(1, 0), values.permute(1, 2, 0)))\n",
    "        score = tf.matmul(hidden_with_time_axis,values,transpose_b=True)\n",
    "\n",
    "#         score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(-1, attention_weights.shape[2], 1))\n",
    "#         attention_weights = tf.reshape(attention_weights, shape=(BATCH_SIZE, attention_weights.shape[2], 1))\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "#         St = self.attn(output).transpose(1, 0)\n",
    "#         St = St.unsqueeze(-1)\n",
    "#         attn = tf.linalg.matmul(query, values, transpose_a=true)\n",
    "#         return tf.nn.softmax(attn, axis=None).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # pass through four fully connected layers, the model will return \n",
    "        # the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(8)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 8)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_object = tf.keras.losses.categorical_crossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = tf.keras.losses.categorical_crossentropy(real, pred,from_logits=True)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.1624,time:1.567352533340454sec\n",
      "Epoch 1 Batch 100 Loss 1.1138,time:12.170740604400635sec\n",
      "Epoch 1 Batch 200 Loss 2.1737,time:22.81913924217224sec\n",
      "Epoch 1 Batch 300 Loss 1.2996,time:33.87752842903137sec\n",
      "Epoch 1 Batch 400 Loss 0.8755,time:44.39189648628235sec\n",
      "Epoch 1 Batch 500 Loss 1.4679,time:55.10117030143738sec\n",
      "Epoch 1 Batch 600 Loss 0.8723,time:65.89453434944153sec\n",
      "Epoch 1 Batch 700 Loss 1.4236,time:76.26335859298706sec\n",
      "Epoch 1 Batch 800 Loss 1.2752,time:86.61469006538391sec\n",
      "Epoch 1 Batch 900 Loss 1.1099,time:97.58563280105591sec\n",
      "Epoch 1 Batch 1000 Loss 1.1736,time:108.80485200881958sec\n",
      "Epoch 1 Batch 1100 Loss 1.5126,time:119.74131608009338sec\n",
      "Epoch 1 Batch 1200 Loss 1.0502,time:130.50319170951843sec\n",
      "Epoch 1 Batch 1300 Loss 1.7299,time:141.2156035900116sec\n",
      "Epoch 1 Batch 1400 Loss 1.1362,time:151.6715383529663sec\n",
      "Epoch 1 Batch 1500 Loss 1.6673,time:162.1589000225067sec\n",
      "Epoch 1 Batch 1600 Loss 0.9458,time:172.35619640350342sec\n",
      "Epoch 1 Batch 1700 Loss 1.5996,time:182.5878541469574sec\n",
      "Epoch 1 Batch 1800 Loss 1.5870,time:193.1704878807068sec\n",
      "Epoch 1 Batch 1900 Loss 1.2811,time:203.84271049499512sec\n",
      "Epoch 1 Batch 2000 Loss 1.4396,time:214.5531222820282sec\n",
      "Epoch 1 Batch 2100 Loss 1.6461,time:225.1575105190277sec\n",
      "Epoch 1 Batch 2200 Loss 1.1765,time:236.44416332244873sec\n",
      "Epoch 1 Batch 2300 Loss 1.3134,time:247.79071807861328sec\n",
      "Epoch 1 Batch 2400 Loss 1.6768,time:258.6411621570587sec\n",
      "Epoch 1 Batch 2500 Loss 1.1153,time:269.13284063339233sec\n",
      "Epoch 1 Batch 2600 Loss 1.0692,time:280.1663236618042sec\n",
      "Epoch 1 Batch 2700 Loss 1.1364,time:291.07278084754944sec\n",
      "Epoch 1 Batch 2800 Loss 0.9571,time:301.81539034843445sec\n",
      "Epoch 1 Batch 2900 Loss 1.1970,time:312.8942623138428sec\n",
      "Epoch 1 Batch 3000 Loss 1.1500,time:323.6595268249512sec\n",
      "Epoch 1 Batch 3100 Loss 1.2816,time:334.2229058742523sec\n",
      "Epoch 1 Batch 3200 Loss 1.2203,time:344.646253824234sec\n",
      "Epoch 1 Batch 3300 Loss 1.2895,time:355.35666584968567sec\n",
      "Epoch 1 Batch 3400 Loss 1.0825,time:365.9517719745636sec\n",
      "Epoch 1 Batch 3500 Loss 1.7234,time:376.61744594573975sec\n",
      "Epoch 1 Batch 3600 Loss 1.1117,time:387.8859841823578sec\n",
      "Epoch 1 Batch 3700 Loss 1.1079,time:398.5073754787445sec\n",
      "Epoch 1 Batch 3800 Loss 1.6869,time:409.41883277893066sec\n",
      "Epoch 1 Batch 3900 Loss 1.8056,time:420.22187972068787sec\n",
      "Epoch 1 Batch 4000 Loss 1.4768,time:431.15440464019775sec\n",
      "Epoch 1 Batch 4100 Loss 1.4075,time:441.513738155365sec\n",
      "Epoch 1 Batch 4200 Loss 1.2703,time:452.0361080169678sec\n",
      "Epoch 1 Batch 4300 Loss 1.0699,time:462.98157262802124sec\n",
      "Epoch 1 Batch 4400 Loss 1.4108,time:473.78895902633667sec\n",
      "Epoch 1 Batch 4500 Loss 1.7577,time:484.4493598937988sec\n",
      "Epoch 1 Batch 4600 Loss 1.1717,time:495.1667740345001sec\n",
      "Epoch 1 Batch 4700 Loss 1.1173,time:505.4000782966614sec\n",
      "Epoch 1 Batch 4800 Loss 1.3617,time:515.8569066524506sec\n",
      "Epoch 1 Batch 4900 Loss 0.8301,time:526.4738032817841sec\n",
      "Epoch 1 Batch 5000 Loss 0.9891,time:537.0858774185181sec\n",
      "Epoch 1 Batch 5100 Loss 0.8223,time:548.0603477954865sec\n",
      "Epoch 1 Batch 5200 Loss 1.3395,time:559.3338866233826sec\n",
      "Epoch 1 Batch 5300 Loss 1.1257,time:569.9982879161835sec\n",
      "Epoch 1 Batch 5400 Loss 1.4132,time:580.3574624061584sec\n",
      "Epoch 1 Batch 5500 Loss 1.1640,time:591.116945028305sec\n",
      "Epoch 1 Batch 5600 Loss 1.5555,time:601.4019944667816sec\n",
      "Epoch 1 Batch 5700 Loss 1.4030,time:611.8123388290405sec\n",
      "Epoch 1 Batch 5800 Loss 1.2455,time:622.1136593818665sec\n",
      "Epoch 1 Batch 5900 Loss 1.1493,time:632.5750141143799sec\n",
      "Epoch 1 Batch 6000 Loss 1.2753,time:643.3866012096405sec\n",
      "Epoch 1 Batch 6100 Loss 1.0120,time:653.7593233585358sec\n",
      "Epoch 1 Batch 6200 Loss 1.1947,time:663.9476175308228sec\n",
      "Epoch 1 Batch 6300 Loss 1.7143,time:674.2729434967041sec\n",
      "Epoch 1 Batch 6400 Loss 1.2890,time:684.7703070640564sec\n",
      "Epoch 1 Batch 6500 Loss 1.2440,time:695.6082561016083sec\n",
      "Epoch 1 Batch 6600 Loss 1.1991,time:706.5750601291656sec\n",
      "Epoch 1 Batch 6700 Loss 1.1313,time:717.3904964923859sec\n",
      "Epoch 1 Batch 6800 Loss 1.0682,time:728.2649445533752sec\n",
      "Epoch 1 Batch 6900 Loss 1.4189,time:738.7243003845215sec\n",
      "Epoch 1 Batch 7000 Loss 1.1861,time:749.083845615387sec\n",
      "Epoch 1 Batch 7100 Loss 1.0613,time:759.7387890815735sec\n",
      "Epoch 1 Batch 7200 Loss 1.7492,time:770.128128528595sec\n",
      "Epoch 1 Batch 7300 Loss 1.3318,time:780.7285161018372sec\n",
      "Epoch 1 Batch 7400 Loss 1.4115,time:791.4839375019073sec\n",
      "Epoch 1 Batch 7500 Loss 1.5708,time:801.613410949707sec\n",
      "Epoch 1 Batch 7600 Loss 1.4397,time:812.067953824997sec\n",
      "Epoch 1 Batch 7700 Loss 1.0683,time:822.6433348655701sec\n",
      "Epoch 1 Batch 7800 Loss 1.2920,time:833.0026679039001sec\n",
      "Epoch 1 Batch 7900 Loss 1.2027,time:844.1188912391663sec\n",
      "Epoch 1 Batch 8000 Loss 1.1326,time:854.8853161334991sec\n",
      "Epoch 1 Batch 8100 Loss 0.9555,time:865.5067083835602sec\n",
      "Epoch 1 Batch 8200 Loss 1.5926,time:876.0470824241638sec\n",
      "Epoch 1 Batch 8300 Loss 1.2001,time:887.1890544891357sec\n",
      "Epoch 1 Batch 8400 Loss 1.3918,time:898.606374502182sec\n",
      "Epoch 1 Batch 8500 Loss 1.6560,time:909.9512131214142sec\n",
      "Epoch 1 Batch 8600 Loss 1.7485,time:921.1737399101257sec\n",
      "Epoch 1 Batch 8700 Loss 1.3786,time:932.4702844619751sec\n",
      "Epoch 1 Batch 8800 Loss 1.2864,time:943.6019880771637sec\n",
      "Epoch 1 Batch 8900 Loss 1.3604,time:954.6780326366425sec\n",
      "Epoch 1 Batch 9000 Loss 1.8083,time:965.7775328159332sec\n",
      "Epoch 1 Batch 9100 Loss 1.8365,time:976.8440248966217sec\n",
      "Epoch 1 Batch 9200 Loss 1.3127,time:988.1605725288391sec\n",
      "Epoch 1 Batch 9300 Loss 0.9846,time:999.3572409152985sec\n",
      "Epoch 1 Batch 9400 Loss 1.0251,time:1010.3670434951782sec\n",
      "Epoch 1 Batch 9500 Loss 1.3376,time:1021.4535396099091sec\n",
      "Epoch 1 Batch 9600 Loss 1.0628,time:1032.5930480957031sec\n",
      "Epoch 1 Batch 9700 Loss 1.3427,time:1043.845582485199sec\n",
      "Epoch 1 Batch 9800 Loss 1.3214,time:1055.3002982139587sec\n",
      "Epoch 1 Batch 9900 Loss 1.1754,time:1066.4845740795135sec\n",
      "Epoch 1 Batch 10000 Loss 1.0051,time:1077.6690928936005sec\n",
      "Epoch 1 Batch 10100 Loss 1.2395,time:1089.0546565055847sec\n",
      "Epoch 1 Batch 10200 Loss 1.1636,time:1101.0148575305939sec\n",
      "Epoch 1 Batch 10300 Loss 1.3547,time:1112.602013349533sec\n",
      "Epoch 1 Batch 10400 Loss 1.7369,time:1123.8950498104095sec\n",
      "Epoch 1 Batch 10500 Loss 1.7078,time:1135.6456954479218sec\n",
      "Epoch 1 Batch 10600 Loss 1.3253,time:1147.1692905426025sec\n",
      "Epoch 1 Batch 10700 Loss 0.8973,time:1158.785397052765sec\n",
      "Epoch 1 Batch 10800 Loss 0.8734,time:1170.0037364959717sec\n",
      "Epoch 1 Batch 10900 Loss 1.2180,time:1181.5399606227875sec\n",
      "Epoch 1 Batch 11000 Loss 1.5199,time:1192.551440000534sec\n",
      "Epoch 1 Batch 11100 Loss 1.1501,time:1203.0538053512573sec\n",
      "Epoch 1 Batch 11200 Loss 0.9497,time:1214.282842874527sec\n",
      "Epoch 1 Batch 11300 Loss 1.0395,time:1225.5580649375916sec\n",
      "Epoch 1 Batch 11400 Loss 1.4567,time:1236.6655662059784sec\n",
      "Epoch 1 Batch 11500 Loss 1.4564,time:1247.4229888916016sec\n",
      "Epoch 1 Batch 11600 Loss 1.0109,time:1258.5594964027405sec\n",
      "Epoch 1 Batch 11700 Loss 1.4077,time:1270.105224609375sec\n",
      "Epoch 1 Batch 11800 Loss 1.1176,time:1282.1552119255066sec\n",
      "Epoch 1 Batch 11900 Loss 1.0809,time:1293.5647819042206sec\n",
      "Epoch 1 Batch 12000 Loss 1.2506,time:1304.449232339859sec\n",
      "Epoch 1 Batch 12100 Loss 1.0340,time:1315.0806274414062sec\n",
      "Epoch 1 Batch 12200 Loss 1.8343,time:1325.5020179748535sec\n",
      "Epoch 1 Batch 12300 Loss 1.7649,time:1335.9994032382965sec\n",
      "Epoch 1 Batch 12400 Loss 1.5385,time:1346.4827637672424sec\n",
      "Epoch 1 Batch 12500 Loss 1.3259,time:1357.0141348838806sec\n",
      "Epoch 1 Batch 12600 Loss 1.1250,time:1368.9838118553162sec\n",
      "Epoch 1 Batch 12700 Loss 1.4366,time:1380.7498416900635sec\n",
      "Epoch 1 Batch 12800 Loss 1.7155,time:1392.3754596710205sec\n",
      "Epoch 1 Batch 12900 Loss 1.3472,time:1403.2329049110413sec\n",
      "Epoch 1 Batch 13000 Loss 1.2794,time:1414.06134390831sec\n",
      "Epoch 1 Batch 13100 Loss 1.6887,time:1424.9607849121094sec\n",
      "Epoch 1 Batch 13200 Loss 1.1591,time:1435.3174641132355sec\n",
      "Epoch 1 Batch 13300 Loss 1.4322,time:1445.920851945877sec\n",
      "Epoch 1 Batch 13400 Loss 1.3492,time:1456.4622259140015sec\n",
      "Epoch 1 Batch 13500 Loss 1.1437,time:1466.9195806980133sec\n",
      "Epoch 1 Batch 13600 Loss 1.1927,time:1477.409439086914sec\n",
      "Epoch 1 Batch 13700 Loss 1.5419,time:1487.8763010501862sec\n",
      "Epoch 1 Batch 13800 Loss 1.1647,time:1498.2416355609894sec\n",
      "Epoch 1 Batch 13900 Loss 1.2429,time:1508.6749849319458sec\n",
      "Epoch 1 Batch 14000 Loss 1.3752,time:1519.1643471717834sec\n",
      "Epoch 1 Batch 14100 Loss 0.6768,time:1529.5698263645172sec\n",
      "Epoch 1 Batch 14200 Loss 1.5555,time:1540.184599161148sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 14300 Loss 1.1206,time:1550.9500243663788sec\n",
      "Epoch 1 Batch 14400 Loss 1.5392,time:1561.6544342041016sec\n",
      "Epoch 1 Batch 14500 Loss 1.5217,time:1572.3578443527222sec\n",
      "Epoch 1 Batch 14600 Loss 1.0295,time:1582.8319880962372sec\n",
      "Epoch 1 Batch 14700 Loss 1.6957,time:1593.3608388900757sec\n",
      "Epoch 1 Batch 14800 Loss 1.7902,time:1603.802190542221sec\n",
      "Epoch 1 Batch 14900 Loss 1.8243,time:1614.3185596466064sec\n",
      "Epoch 1 Batch 15000 Loss 0.9313,time:1624.86647439003sec\n",
      "Epoch 1 Batch 15100 Loss 1.6730,time:1635.4086787700653sec\n",
      "Epoch 1 Batch 15200 Loss 1.2081,time:1645.8400275707245sec\n",
      "Epoch 1 Batch 15300 Loss 0.9806,time:1656.3493943214417sec\n",
      "Epoch 1 Batch 15400 Loss 1.5011,time:1666.6657176017761sec\n",
      "Epoch 1 Batch 15500 Loss 1.0329,time:1677.1785254478455sec\n",
      "Epoch 1 Batch 15600 Loss 1.1493,time:1687.572193622589sec\n",
      "Epoch 1 Batch 15700 Loss 1.2198,time:1698.0645565986633sec\n",
      "Epoch 1 Batch 15800 Loss 1.1940,time:1708.5269122123718sec\n",
      "Epoch 1 Batch 15900 Loss 0.9976,time:1718.912250995636sec\n",
      "Epoch 1 Batch 16000 Loss 1.3417,time:1729.275997877121sec\n",
      "Epoch 1 Batch 16100 Loss 1.6546,time:1739.6107580661774sec\n",
      "Epoch 1 Batch 16200 Loss 1.2526,time:1749.831059217453sec\n",
      "Epoch 1 Batch 16300 Loss 1.0173,time:1760.0803673267365sec\n",
      "Epoch 1 Batch 16400 Loss 1.3712,time:1770.3991985321045sec\n",
      "Epoch 1 Batch 16500 Loss 1.8167,time:1780.7014784812927sec\n",
      "Epoch 1 Batch 16600 Loss 1.2668,time:1790.984833240509sec\n",
      "Epoch 1 Batch 16700 Loss 1.2513,time:1801.2721498012543sec\n",
      "Epoch 1 Batch 16800 Loss 1.2369,time:1811.443440437317sec\n",
      "Epoch 1 Batch 16900 Loss 1.4111,time:1821.6897480487823sec\n",
      "Epoch 1 Batch 17000 Loss 1.3663,time:1832.0504894256592sec\n",
      "Epoch 1 Batch 17100 Loss 1.5322,time:1842.3418066501617sec\n",
      "Epoch 1 Batch 17200 Loss 1.3696,time:1852.6271224021912sec\n",
      "Epoch 1 Batch 17300 Loss 1.6362,time:1862.9044365882874sec\n",
      "Epoch 1 Batch 17400 Loss 1.5035,time:1873.1677284240723sec\n",
      "Epoch 1 Batch 17500 Loss 1.1880,time:1883.4286658763885sec\n",
      "Epoch 1 Batch 17600 Loss 1.3749,time:1893.6769728660583sec\n",
      "Epoch 1 Batch 17700 Loss 0.8373,time:1904.1613337993622sec\n",
      "Epoch 1 Batch 17800 Loss 1.1070,time:1914.4846591949463sec\n",
      "Epoch 1 Batch 17900 Loss 1.4339,time:1924.7660784721375sec\n",
      "Epoch 1 Batch 18000 Loss 1.2198,time:1934.9071247577667sec\n",
      "Epoch 1 Batch 18100 Loss 1.5699,time:1945.2134461402893sec\n",
      "Epoch 1 Batch 18200 Loss 1.6840,time:1955.3307242393494sec\n",
      "Epoch 1 Batch 18300 Loss 1.4918,time:1965.7180633544922sec\n",
      "Epoch 1 Batch 18400 Loss 1.3349,time:1976.2943425178528sec\n",
      "Epoch 1 Batch 18500 Loss 1.1267,time:1986.6341376304626sec\n",
      "Epoch 1 Batch 18600 Loss 1.4212,time:1996.9694652557373sec\n",
      "Epoch 1 Batch 18700 Loss 1.5795,time:2007.3708081245422sec\n",
      "Epoch 1 Batch 18800 Loss 1.5566,time:2017.7451438903809sec\n",
      "Epoch 1 Batch 18900 Loss 1.1466,time:2028.3742110729218sec\n",
      "Epoch 1 Batch 19000 Loss 1.1286,time:2038.752837896347sec\n",
      "Epoch 1 Batch 19100 Loss 1.0926,time:2048.980141401291sec\n",
      "Epoch 1 Batch 19200 Loss 1.7435,time:2059.1944420337677sec\n",
      "Epoch 1 Batch 19300 Loss 1.4711,time:2069.3027181625366sec\n",
      "Epoch 1 Batch 19400 Loss 0.9324,time:2079.5254998207092sec\n",
      "Epoch 1 Batch 19500 Loss 1.7337,time:2089.653911113739sec\n",
      "Epoch 1 Batch 19600 Loss 1.7979,time:2099.7221777439117sec\n",
      "Epoch 1 Batch 19700 Loss 1.6994,time:2109.8854670524597sec\n",
      "Epoch 1 Batch 19800 Loss 1.1035,time:2120.02174949646sec\n",
      "Epoch 1 Batch 19900 Loss 0.9865,time:2130.2463343143463sec\n",
      "Epoch 1 Batch 20000 Loss 1.2396,time:2140.643221616745sec\n",
      "Epoch 1 Batch 20100 Loss 1.2080,time:2150.89052939415sec\n",
      "Epoch 1 Batch 20200 Loss 1.4946,time:2161.091826438904sec\n",
      "Epoch 1 Batch 20300 Loss 1.1662,time:2171.3851447105408sec\n",
      "Epoch 1 Batch 20400 Loss 1.3993,time:2182.0916862487793sec\n",
      "Epoch 1 Batch 20500 Loss 0.8688,time:2192.374579191208sec\n",
      "Epoch 1 Batch 20600 Loss 1.6409,time:2202.5498712062836sec\n",
      "Epoch 1 Batch 20700 Loss 1.1845,time:2212.6711513996124sec\n",
      "Epoch 1 Batch 20800 Loss 1.1093,time:2222.8574438095093sec\n",
      "Epoch 1 Batch 20900 Loss 1.6744,time:2233.0797457695007sec\n",
      "Epoch 1 Batch 21000 Loss 1.0782,time:2243.179622888565sec\n",
      "Epoch 1 Batch 21100 Loss 1.4332,time:2253.200112104416sec\n",
      "Epoch 1 Batch 21200 Loss 1.7453,time:2263.4534220695496sec\n",
      "Epoch 1 Batch 21300 Loss 1.2935,time:2273.7167332172394sec\n",
      "Epoch 1 Batch 21400 Loss 1.2529,time:2284.0210540294647sec\n",
      "Epoch 1 Batch 21500 Loss 0.9648,time:2294.3305163383484sec\n",
      "Epoch 1 Batch 21600 Loss 1.1232,time:2304.5746717453003sec\n",
      "Epoch 1 Batch 21700 Loss 1.7800,time:2314.7779700756073sec\n",
      "Epoch 1 Batch 21800 Loss 1.3853,time:2324.921255350113sec\n",
      "Epoch 1 Batch 21900 Loss 1.3273,time:2335.204569578171sec\n",
      "Epoch 1 Batch 22000 Loss 1.2090,time:2345.3209874629974sec\n",
      "Epoch 1 Batch 22100 Loss 1.3949,time:2355.4386451244354sec\n",
      "Epoch 1 Batch 22200 Loss 1.1855,time:2365.5639250278473sec\n",
      "Epoch 1 Batch 22300 Loss 1.5264,time:2375.853241920471sec\n",
      "Epoch 1 Batch 22400 Loss 1.2640,time:2386.314597606659sec\n",
      "Epoch 1 Batch 22500 Loss 1.0378,time:2396.5261890888214sec\n",
      "Epoch 1 Batch 22600 Loss 1.3056,time:2406.6382007598877sec\n",
      "Epoch 1 Batch 22700 Loss 1.2871,time:2416.718471288681sec\n",
      "Epoch 1 Batch 22800 Loss 1.6670,time:2427.200831890106sec\n",
      "Epoch 1 Batch 22900 Loss 1.3856,time:2437.7362043857574sec\n",
      "Epoch 1 Batch 23000 Loss 1.7122,time:2448.0105373859406sec\n",
      "Epoch 1 Batch 23100 Loss 1.1794,time:2458.2639145851135sec\n",
      "Epoch 1 Batch 23200 Loss 1.3050,time:2468.5172231197357sec\n",
      "Epoch 1 Batch 23300 Loss 1.3449,time:2478.7305233478546sec\n",
      "Epoch 1 Batch 23400 Loss 1.4871,time:2489.0612182617188sec\n",
      "Epoch 1 Batch 23500 Loss 1.2760,time:2499.5709614753723sec\n",
      "Epoch 1 Batch 23600 Loss 1.1864,time:2509.966302394867sec\n",
      "Epoch 1 Batch 23700 Loss 0.9673,time:2520.3746461868286sec\n",
      "Epoch 1 Batch 23800 Loss 0.8326,time:2530.7279772758484sec\n",
      "Epoch 1 Batch 23900 Loss 1.2033,time:2540.9464008808136sec\n",
      "Epoch 1 Batch 24000 Loss 1.5838,time:2551.0804176330566sec\n",
      "Epoch 1 Batch 24100 Loss 1.2253,time:2561.188775062561sec\n",
      "Epoch 1 Batch 24200 Loss 1.1126,time:2571.4000747203827sec\n",
      "Epoch 1 Batch 24300 Loss 1.3596,time:2581.6253776550293sec\n",
      "Epoch 1 Batch 24400 Loss 1.3465,time:2591.811671257019sec\n",
      "Epoch 1 Batch 24500 Loss 1.2324,time:2601.971000432968sec\n",
      "Epoch 1 Batch 24600 Loss 0.9232,time:2612.146291255951sec\n",
      "Epoch 1 Batch 24700 Loss 1.2944,time:2622.46861577034sec\n",
      "Epoch 1 Batch 24800 Loss 1.9017,time:2632.852954387665sec\n",
      "Epoch 1 Batch 24900 Loss 1.3075,time:2643.096561193466sec\n",
      "Epoch 1 Batch 25000 Loss 1.3706,time:2653.3927874565125sec\n",
      "Epoch 1 Batch 25100 Loss 1.5039,time:2663.5403678417206sec\n",
      "Epoch 1 Batch 25200 Loss 1.4643,time:2673.9237065315247sec\n",
      "Epoch 1 Batch 25300 Loss 1.2069,time:2684.2800390720367sec\n",
      "Epoch 1 Batch 25400 Loss 1.2267,time:2694.390828847885sec\n",
      "Epoch 1 Batch 25500 Loss 1.1370,time:2704.6025519371033sec\n",
      "Epoch 1 Batch 25600 Loss 1.4948,time:2714.6888234615326sec\n",
      "Epoch 1 Batch 25700 Loss 0.8702,time:2724.7590913772583sec\n",
      "Epoch 1 Batch 25800 Loss 1.4940,time:2735.1584327220917sec\n",
      "Epoch 1 Batch 25900 Loss 1.0663,time:2745.43857216835sec\n",
      "Epoch 1 Batch 26000 Loss 1.3997,time:2755.6330029964447sec\n",
      "Epoch 1 Batch 26100 Loss 1.6465,time:2765.877310037613sec\n",
      "Epoch 1 Batch 26200 Loss 1.2889,time:2776.1546669006348sec\n",
      "Epoch 1 Batch 26300 Loss 1.6573,time:2786.512957572937sec\n",
      "Epoch 1 Batch 26400 Loss 1.4128,time:2796.812665462494sec\n",
      "Epoch 1 Batch 26500 Loss 1.7878,time:2806.91801071167sec\n",
      "Epoch 1 Batch 26600 Loss 1.9994,time:2817.052292585373sec\n",
      "Epoch 1 Batch 26700 Loss 1.4748,time:2827.1725718975067sec\n",
      "Epoch 1 Batch 26800 Loss 0.9619,time:2837.239839553833sec\n",
      "Epoch 1 Batch 26900 Loss 1.3595,time:2847.704626560211sec\n",
      "Epoch 1 Batch 27000 Loss 1.1840,time:2858.26185631752sec\n",
      "Epoch 1 Batch 27100 Loss 1.1376,time:2868.485419988632sec\n",
      "Epoch 1 Batch 27200 Loss 1.1063,time:2878.636706352234sec\n",
      "Epoch 1 Batch 27300 Loss 1.1250,time:2888.832002401352sec\n",
      "Epoch 1 Batch 27400 Loss 1.1705,time:2898.9394001960754sec\n",
      "Epoch 1 Batch 27500 Loss 0.9859,time:2909.1050095558167sec\n",
      "Epoch 1 Batch 27600 Loss 1.4333,time:2919.224289417267sec\n",
      "Epoch 1 Batch 27700 Loss 1.4661,time:2929.377575159073sec\n",
      "Epoch 1 Batch 27800 Loss 1.0735,time:2939.561868429184sec\n",
      "Epoch 1 Batch 27900 Loss 1.3787,time:2949.710242986679sec\n",
      "Epoch 1 Batch 28000 Loss 1.4184,time:2960.0505604743958sec\n",
      "Epoch 1 Batch 28100 Loss 1.1505,time:2970.410893201828sec\n",
      "Epoch 1 Batch 28200 Loss 1.1553,time:2980.5971879959106sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 28300 Loss 1.1793,time:2990.729470014572sec\n",
      "Epoch 1 Batch 28400 Loss 1.7200,time:3000.927271604538sec\n",
      "Epoch 1 Batch 28500 Loss 1.0699,time:3011.0541486740112sec\n",
      "Epoch 1 Batch 28600 Loss 1.1972,time:3021.2425360679626sec\n",
      "Epoch 1 Batch 28700 Loss 1.0817,time:3031.4798412323sec\n",
      "Epoch 1 Batch 28800 Loss 1.2445,time:3041.8701820373535sec\n",
      "Epoch 1 Batch 28900 Loss 1.5318,time:3052.1124877929688sec\n",
      "Epoch 1 Batch 29000 Loss 1.0056,time:3062.335461616516sec\n",
      "Epoch 1 Batch 29100 Loss 1.4547,time:3072.57546043396sec\n",
      "Epoch 1 Batch 29200 Loss 0.9412,time:3082.8550510406494sec\n",
      "Epoch 1 Batch 29300 Loss 1.2599,time:3093.2593936920166sec\n",
      "Epoch 1 Batch 29400 Loss 1.3362,time:3103.6397314071655sec\n",
      "Epoch 1 Batch 29500 Loss 1.1242,time:3113.6989970207214sec\n",
      "Epoch 1 Batch 29600 Loss 1.0785,time:3123.817387342453sec\n",
      "Epoch 1 Batch 29700 Loss 1.5259,time:3133.924292564392sec\n",
      "Epoch 1 Batch 29800 Loss 1.4709,time:3144.054583311081sec\n",
      "Epoch 1 Batch 29900 Loss 1.0763,time:3154.1298520565033sec\n",
      "Epoch 1 Batch 30000 Loss 1.1090,time:3164.6292159557343sec\n",
      "Epoch 1 Batch 30100 Loss 2.1308,time:3174.8735234737396sec\n",
      "Epoch 1 Batch 30200 Loss 1.7873,time:3185.012904405594sec\n",
      "Epoch 1 Batch 30300 Loss 1.4607,time:3195.2172026634216sec\n",
      "Epoch 1 Batch 30400 Loss 0.7239,time:3205.3694891929626sec\n",
      "Epoch 1 Batch 30500 Loss 1.1367,time:3215.4757652282715sec\n",
      "Epoch 1 Batch 30600 Loss 1.0897,time:3225.6345574855804sec\n",
      "Epoch 1 Batch 30700 Loss 1.4674,time:3235.8523976802826sec\n",
      "Epoch 1 Batch 30800 Loss 1.3539,time:3246.107707262039sec\n",
      "Epoch 1 Batch 30900 Loss 1.1814,time:3256.3270087242126sec\n",
      "Epoch 1 Batch 31000 Loss 1.1548,time:3266.61532497406sec\n",
      "Epoch 1 Batch 31100 Loss 1.3660,time:3276.8339920043945sec\n",
      "Epoch 1 Batch 31200 Loss 1.5445,time:3286.9939844608307sec\n",
      "Epoch 1 Batch 31300 Loss 1.2862,time:3297.1954033374786sec\n",
      "Epoch 1 Batch 31400 Loss 1.3537,time:3307.314682006836sec\n",
      "Epoch 1 Batch 31500 Loss 1.0806,time:3317.6540112495422sec\n",
      "Epoch 1 Batch 31600 Loss 1.4117,time:3327.954329967499sec\n",
      "Epoch 1 Batch 31700 Loss 0.9218,time:3338.1560847759247sec\n",
      "Epoch 1 Batch 31800 Loss 0.9823,time:3348.4114871025085sec\n",
      "Epoch 1 Batch 31900 Loss 1.1595,time:3358.74281334877sec\n",
      "Epoch 1 Batch 32000 Loss 1.3849,time:3368.939109325409sec\n",
      "Epoch 1 Batch 32100 Loss 1.3924,time:3379.1964194774628sec\n",
      "Epoch 1 Batch 32200 Loss 0.8407,time:3389.4017481803894sec\n",
      "Epoch 1 Batch 32300 Loss 1.4120,time:3399.6360952854156sec\n",
      "Epoch 1 Batch 32400 Loss 1.0170,time:3410.0184333324432sec\n",
      "Epoch 1 Batch 32500 Loss 0.6823,time:3420.3137521743774sec\n",
      "Epoch 1 Batch 32600 Loss 1.7645,time:3430.4025299549103sec\n",
      "Epoch 1 Batch 32700 Loss 1.6073,time:3440.582213640213sec\n",
      "Epoch 1 Batch 32800 Loss 1.1329,time:3450.6464796066284sec\n",
      "Epoch 1 Batch 32900 Loss 1.3732,time:3460.8457770347595sec\n",
      "Epoch 1 Batch 33000 Loss 0.9458,time:3471.168101787567sec\n",
      "Epoch 1 Batch 33100 Loss 1.6479,time:3481.5539813041687sec\n",
      "Epoch 1 Batch 33200 Loss 1.4232,time:3491.8798444271088sec\n",
      "Epoch 1 Batch 33300 Loss 1.0784,time:3502.236175775528sec\n",
      "Epoch 1 Batch 33400 Loss 1.5475,time:3512.6285161972046sec\n",
      "Epoch 1 Batch 33500 Loss 1.2548,time:3523.136883020401sec\n",
      "Epoch 1 Batch 33600 Loss 1.3963,time:3533.529547691345sec\n",
      "Epoch 1 Batch 33700 Loss 1.2197,time:3543.749460220337sec\n",
      "Epoch 1 Batch 33800 Loss 1.1817,time:3553.993805170059sec\n",
      "Epoch 1 Batch 33900 Loss 1.3051,time:3564.3221011161804sec\n",
      "Epoch 1 Batch 34000 Loss 1.1243,time:3574.574411392212sec\n",
      "Epoch 1 Batch 34100 Loss 1.6015,time:3584.716694355011sec\n",
      "Epoch 1 Batch 34200 Loss 1.4984,time:3594.9250247478485sec\n",
      "Epoch 1 Batch 34300 Loss 1.4047,time:3605.098341703415sec\n",
      "Epoch 1 Batch 34400 Loss 0.8376,time:3615.2356247901917sec\n",
      "Epoch 1 Batch 34500 Loss 1.2469,time:3625.3739080429077sec\n",
      "Epoch 1 Batch 34600 Loss 1.4570,time:3635.4781832695007sec\n",
      "Epoch 1 Batch 34700 Loss 0.9132,time:3645.783974170685sec\n",
      "Epoch 1 Batch 34800 Loss 1.3776,time:3656.018705844879sec\n",
      "Epoch 1 Batch 34900 Loss 1.2858,time:3666.3400316238403sec\n",
      "Epoch 1 Batch 35000 Loss 0.8724,time:3676.8523981571198sec\n",
      "Epoch 1 Batch 35100 Loss 0.9604,time:3687.1315870285034sec\n",
      "Epoch 1 Batch 35200 Loss 1.1898,time:3697.4966027736664sec\n",
      "Epoch 1 Batch 35300 Loss 0.7716,time:3707.7829196453094sec\n",
      "Epoch 1 Batch 35400 Loss 0.9964,time:3717.8601891994476sec\n",
      "Epoch 1 Batch 35500 Loss 1.2294,time:3728.087492465973sec\n",
      "Epoch 1 Batch 35600 Loss 1.3831,time:3738.739869594574sec\n",
      "Epoch 1 Batch 35700 Loss 1.0020,time:3748.9803943634033sec\n",
      "Epoch 1 Batch 35800 Loss 0.9893,time:3759.1836924552917sec\n",
      "Epoch 1 Batch 35900 Loss 1.8369,time:3769.380989074707sec\n",
      "Epoch 1 Batch 36000 Loss 1.4893,time:3779.5842866897583sec\n",
      "Epoch 1 Batch 36100 Loss 1.0439,time:3789.829927921295sec\n",
      "Epoch 1 Batch 36200 Loss 0.6839,time:3800.0441179275513sec\n",
      "Epoch 1 Batch 36300 Loss 0.9172,time:3810.341436624527sec\n",
      "Epoch 1 Batch 36400 Loss 1.1025,time:3820.5767419338226sec\n",
      "Epoch 1 Batch 36500 Loss 1.3719,time:3830.7690370082855sec\n",
      "Epoch 1 Batch 36600 Loss 1.6229,time:3841.189661026001sec\n",
      "Epoch 1 Batch 36700 Loss 1.3202,time:3851.3724756240845sec\n",
      "Epoch 1 Batch 36800 Loss 1.8100,time:3861.574773311615sec\n",
      "Epoch 1 Batch 36900 Loss 1.5477,time:3871.7760710716248sec\n",
      "Epoch 1 Batch 37000 Loss 1.3504,time:3881.9353590011597sec\n",
      "Epoch 1 Batch 37100 Loss 1.2647,time:3892.1017463207245sec\n",
      "Epoch 1 Batch 37200 Loss 1.2845,time:3902.32194852829sec\n",
      "Epoch 1 Batch 37300 Loss 0.9991,time:3912.459417819977sec\n",
      "Epoch 1 Batch 37400 Loss 1.2241,time:3922.6927230358124sec\n",
      "Epoch 1 Batch 37500 Loss 0.9542,time:3932.8590126037598sec\n",
      "Epoch 1 Batch 37600 Loss 1.3655,time:3943.0423057079315sec\n",
      "Epoch 1 Batch 37700 Loss 1.0728,time:3953.4788591861725sec\n",
      "Epoch 1 Batch 37800 Loss 1.2736,time:3964.0392370224sec\n",
      "Epoch 1 Batch 37900 Loss 0.8557,time:3974.547604084015sec\n",
      "Epoch 1 Batch 38000 Loss 1.1021,time:3984.988955974579sec\n",
      "Epoch 1 Batch 38100 Loss 1.0301,time:3995.5307013988495sec\n",
      "Epoch 1 Batch 38200 Loss 1.4548,time:4005.9389917850494sec\n",
      "Epoch 1 Batch 38300 Loss 1.2314,time:4016.2683188915253sec\n",
      "Epoch 1 Batch 38400 Loss 1.4047,time:4026.5176270008087sec\n",
      "Epoch 1 Batch 38500 Loss 1.2297,time:4036.7639343738556sec\n",
      "Epoch 1 Batch 38600 Loss 1.9485,time:4046.979747056961sec\n",
      "Epoch 1 Batch 38700 Loss 2.0683,time:4057.2244503498077sec\n",
      "Epoch 1 Batch 38800 Loss 1.6081,time:4067.459756374359sec\n",
      "Epoch 1 Batch 38900 Loss 1.0126,time:4077.6140429973602sec\n",
      "Epoch 1 Batch 39000 Loss 1.6832,time:4087.8103392124176sec\n",
      "Epoch 1 Batch 39100 Loss 1.0299,time:4097.945966243744sec\n",
      "Epoch 1 Batch 39200 Loss 1.7562,time:4108.210811853409sec\n",
      "Epoch 1 Batch 39300 Loss 1.4825,time:4118.522181749344sec\n",
      "Epoch 1 Batch 39400 Loss 0.9229,time:4128.825501680374sec\n",
      "Epoch 1 Batch 39500 Loss 1.4454,time:4139.05880689621sec\n",
      "Epoch 1 Batch 39600 Loss 1.2731,time:4149.26810503006sec\n",
      "Epoch 1 Batch 39700 Loss 1.0095,time:4159.667033195496sec\n",
      "Epoch 1 Batch 39800 Loss 1.4917,time:4169.847739696503sec\n",
      "Epoch 1 Batch 39900 Loss 1.0344,time:4180.060039758682sec\n",
      "Epoch 1 Batch 40000 Loss 1.5217,time:4190.5684061050415sec\n",
      "Epoch 1 Batch 40100 Loss 0.8142,time:4200.768702745438sec\n",
      "Epoch 1 Batch 40200 Loss 0.9859,time:4211.026195526123sec\n",
      "Epoch 1 Batch 40300 Loss 1.6906,time:4221.219089508057sec\n",
      "Epoch 1 Batch 40400 Loss 1.5082,time:4231.344503164291sec\n",
      "Epoch 1 Batch 40500 Loss 1.2208,time:4241.480786561966sec\n",
      "Epoch 1 Batch 40600 Loss 1.0150,time:4251.7901084423065sec\n",
      "Epoch 1 Batch 40700 Loss 1.4304,time:4262.185449123383sec\n",
      "Epoch 1 Batch 40800 Loss 1.2011,time:4272.727733135223sec\n",
      "Epoch 1 Batch 40900 Loss 1.7531,time:4283.415146589279sec\n",
      "Epoch 1 Batch 41000 Loss 1.4356,time:4293.834492444992sec\n",
      "Epoch 1 Batch 41100 Loss 1.6772,time:4304.198826551437sec\n",
      "Epoch 1 Batch 41200 Loss 1.2904,time:4314.385120868683sec\n",
      "Epoch 1 Batch 41300 Loss 1.3675,time:4324.636096000671sec\n",
      "Epoch 1 Batch 41400 Loss 1.0899,time:4334.813112735748sec\n",
      "Epoch 1 Batch 41500 Loss 1.2132,time:4344.88338136673sec\n",
      "Epoch 1 Batch 41600 Loss 1.2900,time:4355.066673517227sec\n",
      "Epoch 1 Batch 41700 Loss 0.9878,time:4365.261966705322sec\n",
      "Epoch 1 Batch 41800 Loss 1.5727,time:4375.298235416412sec\n",
      "Epoch 1 Batch 41900 Loss 1.0659,time:4385.414516687393sec\n",
      "Epoch 1 Batch 42000 Loss 0.9787,time:4395.709835529327sec\n",
      "Epoch 1 Batch 42100 Loss 1.4410,time:4405.926136493683sec\n",
      "Epoch 1 Batch 42200 Loss 1.5811,time:4416.236498594284sec\n",
      "Epoch 1 Batch 42300 Loss 1.7203,time:4426.579150915146sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 42400 Loss 1.6111,time:4436.948833703995sec\n",
      "Epoch 1 Batch 42500 Loss 0.8247,time:4447.264156579971sec\n",
      "Epoch 1 Batch 42600 Loss 1.1048,time:4457.678501844406sec\n",
      "Epoch 1 Batch 42700 Loss 2.2282,time:4468.361409187317sec\n",
      "Epoch 1 Batch 42800 Loss 1.0308,time:4478.655166864395sec\n",
      "Epoch 1 Batch 42900 Loss 1.5491,time:4488.922478675842sec\n",
      "Epoch 1 Batch 43000 Loss 1.2887,time:4499.259806871414sec\n",
      "Epoch 1 Batch 43100 Loss 1.1275,time:4509.452101945877sec\n",
      "Epoch 1 Batch 43200 Loss 1.2894,time:4519.730899572372sec\n",
      "Epoch 1 Batch 43300 Loss 1.0128,time:4530.133008241653sec\n",
      "Epoch 1 Batch 43400 Loss 0.8264,time:4540.674382925034sec\n",
      "Epoch 1 Batch 43500 Loss 0.9426,time:4551.008709669113sec\n",
      "Epoch 1 Batch 43600 Loss 1.3500,time:4561.234012126923sec\n",
      "Epoch 1 Batch 43700 Loss 1.5880,time:4571.538623571396sec\n",
      "Epoch 1 Batch 43800 Loss 1.0960,time:4581.756565570831sec\n",
      "Epoch 1 Batch 43900 Loss 1.3985,time:4591.974866867065sec\n",
      "Epoch 1 Batch 44000 Loss 1.1211,time:4602.241178750992sec\n",
      "Epoch 1 Batch 44100 Loss 0.9737,time:4612.595510482788sec\n",
      "Epoch 1 Batch 44200 Loss 1.1349,time:4622.813896656036sec\n",
      "Epoch 1 Batch 44300 Loss 1.3434,time:4632.97722530365sec\n",
      "Epoch 1 Batch 44400 Loss 1.0241,time:4643.376567602158sec\n",
      "Epoch 1 Batch 44500 Loss 0.7942,time:4653.754904985428sec\n",
      "Epoch 1 Batch 44600 Loss 1.2342,time:4663.9802079200745sec\n",
      "Epoch 1 Batch 44700 Loss 0.9383,time:4674.158841133118sec\n",
      "Epoch 1 Batch 44800 Loss 1.5368,time:4684.307620763779sec\n",
      "Epoch 1 Batch 44900 Loss 1.3043,time:4694.423899173737sec\n",
      "Epoch 1 Batch 45000 Loss 1.2629,time:4704.6582045555115sec\n",
      "Epoch 1 Batch 45100 Loss 1.3418,time:4715.070055246353sec\n",
      "Epoch 1 Batch 45200 Loss 1.6287,time:4725.258343696594sec\n",
      "Epoch 1 Batch 45300 Loss 1.2589,time:4735.525963068008sec\n",
      "Epoch 1 Batch 45400 Loss 1.3959,time:4745.702254772186sec\n",
      "Epoch 1 Batch 45500 Loss 1.0569,time:4755.89354968071sec\n",
      "Epoch 1 Batch 45600 Loss 1.1618,time:4766.0648403167725sec\n",
      "Epoch 1 Batch 45700 Loss 1.4312,time:4776.40895652771sec\n",
      "Epoch 1 Batch 45800 Loss 1.6590,time:4786.7492852211sec\n",
      "Epoch 1 Batch 45900 Loss 1.5273,time:4796.991591930389sec\n",
      "Epoch 1 Batch 46000 Loss 1.2759,time:4807.3401737213135sec\n",
      "Epoch 1 Batch 46100 Loss 1.2176,time:4817.931272745132sec\n",
      "Epoch 1 Batch 46200 Loss 1.5739,time:4828.19259595871sec\n",
      "Epoch 1 Batch 46300 Loss 1.7259,time:4838.52392244339sec\n",
      "Epoch 1 Batch 46400 Loss 1.3213,time:4848.735221862793sec\n",
      "Epoch 1 Batch 46500 Loss 1.2293,time:4859.151567697525sec\n",
      "Epoch 1 Batch 46600 Loss 0.9729,time:4869.624811887741sec\n",
      "Epoch 1 Batch 46700 Loss 1.0203,time:4879.937133550644sec\n",
      "Epoch 1 Batch 46800 Loss 1.3207,time:4890.335475206375sec\n",
      "Epoch 1 Batch 46900 Loss 1.4392,time:4900.711812257767sec\n",
      "Epoch 1 Batch 47000 Loss 1.5493,time:4911.22466635704sec\n",
      "Epoch 1 Batch 47100 Loss 1.6000,time:4921.630311489105sec\n",
      "Epoch 1 Batch 47200 Loss 1.7058,time:4931.917760133743sec\n",
      "Epoch 1 Batch 47300 Loss 1.3524,time:4942.13206076622sec\n",
      "Epoch 1 Batch 47400 Loss 0.9770,time:4952.560409069061sec\n",
      "Epoch 1 Batch 47500 Loss 1.5106,time:4962.860728740692sec\n",
      "Epoch 1 Batch 47600 Loss 1.0421,time:4973.063585758209sec\n",
      "Epoch 1 Batch 47700 Loss 0.9420,time:4983.257213354111sec\n",
      "Epoch 1 Batch 47800 Loss 1.0235,time:4993.498519420624sec\n",
      "Epoch 1 Batch 47900 Loss 1.1557,time:5004.020889043808sec\n",
      "Epoch 1 Batch 48000 Loss 1.4993,time:5014.377222299576sec\n",
      "Epoch 1 Batch 48100 Loss 1.4108,time:5024.683296918869sec\n",
      "Epoch 1 Batch 48200 Loss 1.5353,time:5035.135621786118sec\n",
      "Epoch 1 Batch 48300 Loss 1.5310,time:5045.4199368953705sec\n",
      "Epoch 1 Batch 48400 Loss 0.7258,time:5055.821279525757sec\n",
      "Epoch 1 Batch 48500 Loss 1.0636,time:5066.169609546661sec\n",
      "Epoch 1 Batch 48600 Loss 1.3255,time:5076.577209472656sec\n",
      "Epoch 1 Batch 48700 Loss 1.1566,time:5086.931615114212sec\n",
      "Epoch 1 Batch 48800 Loss 1.7084,time:5097.202929496765sec\n",
      "Epoch 1 Batch 48900 Loss 1.1915,time:5107.372218847275sec\n",
      "Epoch 1 Batch 49000 Loss 1.1539,time:5117.516503334045sec\n",
      "Epoch 1 Batch 49100 Loss 0.7838,time:5127.656686306sec\n",
      "Epoch 1 Batch 49200 Loss 0.8107,time:5137.833358764648sec\n",
      "Epoch 1 Batch 49300 Loss 1.2273,time:5148.137679100037sec\n",
      "Epoch 1 Batch 49400 Loss 1.4485,time:5158.408992290497sec\n",
      "Epoch 1 Batch 49500 Loss 1.6101,time:5168.789792299271sec\n",
      "Epoch 1 Batch 49600 Loss 1.2340,time:5179.217762708664sec\n",
      "Epoch 1 Batch 49700 Loss 1.2539,time:5189.454313516617sec\n",
      "Epoch 1 Batch 49800 Loss 1.8170,time:5199.816647052765sec\n",
      "Epoch 1 Batch 49900 Loss 1.9912,time:5210.140971899033sec\n",
      "Epoch 1 Batch 50000 Loss 1.5251,time:5220.781368017197sec\n",
      "Epoch 1 Batch 50100 Loss 1.4527,time:5231.2547216415405sec\n",
      "Epoch 1 Batch 50200 Loss 1.4321,time:5241.557290554047sec\n",
      "Epoch 1 Batch 50300 Loss 1.3228,time:5251.987639427185sec\n",
      "Epoch 1 Batch 50400 Loss 1.2544,time:5262.299961805344sec\n",
      "Epoch 1 Batch 50500 Loss 1.5593,time:5272.651292800903sec\n",
      "Epoch 1 Batch 50600 Loss 1.3573,time:5282.939805269241sec\n",
      "Epoch 1 Batch 50700 Loss 0.8979,time:5293.190274715424sec\n",
      "Epoch 1 Batch 50800 Loss 1.0249,time:5303.4465844631195sec\n",
      "Epoch 1 Batch 50900 Loss 1.2050,time:5313.763837099075sec\n",
      "Epoch 1 Batch 51000 Loss 1.3180,time:5324.09139251709sec\n",
      "Epoch 1 Batch 51100 Loss 1.5082,time:5334.328132867813sec\n",
      "Epoch 1 Batch 51200 Loss 1.0762,time:5344.467417478561sec\n",
      "Epoch 1 Batch 51300 Loss 1.0109,time:5354.753732442856sec\n",
      "Epoch 1 Batch 51400 Loss 1.4163,time:5364.874011516571sec\n",
      "Epoch 1 Batch 51500 Loss 1.5525,time:5375.060815572739sec\n",
      "Epoch 1 Batch 51600 Loss 1.3627,time:5385.282495260239sec\n",
      "Epoch 1 Batch 51700 Loss 1.3635,time:5395.385771036148sec\n",
      "Epoch 1 Batch 51800 Loss 1.0802,time:5405.735101938248sec\n",
      "Epoch 1 Batch 51900 Loss 1.3058,time:5416.051424741745sec\n",
      "Epoch 1 Batch 52000 Loss 1.7059,time:5426.413843631744sec\n",
      "Epoch 1 Batch 52100 Loss 1.3081,time:5437.062785387039sec\n",
      "Epoch 1 Batch 52200 Loss 1.0185,time:5447.505136728287sec\n",
      "Epoch 1 Batch 52300 Loss 1.0936,time:5457.875472307205sec\n",
      "Epoch 1 Batch 52400 Loss 1.5048,time:5468.170791149139sec\n",
      "Epoch 1 Batch 52500 Loss 1.3259,time:5478.5424201488495sec\n",
      "Epoch 1 Batch 52600 Loss 1.6688,time:5488.759310722351sec\n",
      "Epoch 1 Batch 52700 Loss 1.4646,time:5498.977611541748sec\n",
      "Epoch 1 Batch 52800 Loss 1.7020,time:5509.210916519165sec\n",
      "Epoch 1 Batch 52900 Loss 1.3425,time:5519.592254161835sec\n",
      "Epoch 1 Batch 53000 Loss 1.3269,time:5529.962087392807sec\n",
      "Epoch 1 Batch 53100 Loss 0.8998,time:5540.326069355011sec\n",
      "Epoch 1 Batch 53200 Loss 1.4144,time:5550.661396741867sec\n",
      "Epoch 1 Batch 53300 Loss 1.6175,time:5560.90970492363sec\n",
      "Epoch 1 Batch 53400 Loss 1.3170,time:5571.192020654678sec\n",
      "Epoch 1 Batch 53500 Loss 1.2981,time:5581.435972690582sec\n",
      "Epoch 1 Batch 53600 Loss 0.9690,time:5591.670753240585sec\n",
      "Epoch 1 Batch 53700 Loss 1.1804,time:5602.126106739044sec\n",
      "Epoch 1 Batch 53800 Loss 0.9676,time:5612.7484998703sec\n",
      "Epoch 1 Batch 53900 Loss 1.5279,time:5623.023813009262sec\n",
      "Epoch 1 Batch 54000 Loss 1.2962,time:5633.33381485939sec\n",
      "Epoch 1 Batch 54100 Loss 1.3587,time:5643.525262117386sec\n",
      "Epoch 1 Batch 54200 Loss 1.4474,time:5653.760566234589sec\n",
      "Epoch 1 Batch 54300 Loss 1.7825,time:5663.984868764877sec\n",
      "Epoch 1 Batch 54400 Loss 2.0101,time:5674.172162771225sec\n",
      "Epoch 1 Batch 54500 Loss 1.3369,time:5684.309455871582sec\n",
      "Epoch 1 Batch 54600 Loss 1.3719,time:5694.369415283203sec\n",
      "Epoch 1 Batch 54700 Loss 0.9659,time:5704.549707174301sec\n",
      "Epoch 1 Batch 54800 Loss 1.2096,time:5714.955051183701sec\n",
      "Epoch 1 Batch 54900 Loss 1.3672,time:5725.317384719849sec\n",
      "Epoch 1 Batch 55000 Loss 1.6151,time:5735.862447977066sec\n",
      "Epoch 1 Batch 55100 Loss 1.2909,time:5746.100082159042sec\n",
      "Epoch 1 Batch 55200 Loss 1.3325,time:5756.273373603821sec\n",
      "Epoch 1 Batch 55300 Loss 1.3106,time:5766.61270236969sec\n",
      "Epoch 1 Batch 55400 Loss 0.8416,time:5777.237600803375sec\n",
      "Epoch 1 Batch 55500 Loss 1.3697,time:5787.737694501877sec\n",
      "Epoch 1 Batch 55600 Loss 1.1720,time:5798.151039361954sec\n",
      "Epoch 1 Batch 55700 Loss 1.4597,time:5808.51437330246sec\n",
      "Epoch 1 Batch 55800 Loss 1.2607,time:5818.8276953697205sec\n",
      "Epoch 1 Batch 55900 Loss 1.4040,time:5829.145115375519sec\n",
      "Epoch 1 Batch 56000 Loss 1.1916,time:5839.58135843277sec\n",
      "Epoch 1 Batch 56100 Loss 1.3086,time:5849.7856566905975sec\n",
      "Epoch 1 Batch 56200 Loss 1.1065,time:5860.040966033936sec\n",
      "Epoch 1 Batch 56300 Loss 0.8707,time:5870.256266593933sec\n",
      "Epoch 1 Batch 56400 Loss 1.2717,time:5880.474071502686sec\n",
      "Epoch 1 Batch 56500 Loss 1.1589,time:5890.661389827728sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 56600 Loss 1.2317,time:5900.831790924072sec\n",
      "Epoch 1 Batch 56700 Loss 1.6064,time:5911.328152656555sec\n",
      "Epoch 1 Batch 56800 Loss 1.2996,time:5921.484440326691sec\n",
      "Epoch 1 Batch 56900 Loss 1.3441,time:5931.617722272873sec\n",
      "Epoch 1 Batch 57000 Loss 0.8488,time:5941.857756137848sec\n",
      "Epoch 1 Batch 57100 Loss 1.2783,time:5952.121656179428sec\n",
      "Epoch 1 Batch 57200 Loss 0.9350,time:5962.358961105347sec\n",
      "Epoch 1 Batch 57300 Loss 1.5549,time:5972.531252145767sec\n",
      "Epoch 1 Batch 57400 Loss 1.1504,time:5982.759556770325sec\n",
      "Epoch 1 Batch 57500 Loss 1.5639,time:5993.069977760315sec\n",
      "Epoch 1 Batch 57600 Loss 1.3185,time:6003.381099224091sec\n",
      "Epoch 1 Batch 57700 Loss 2.0106,time:6013.561365604401sec\n",
      "Epoch 1 Batch 57800 Loss 1.7476,time:6023.812674283981sec\n",
      "Epoch 1 Batch 57900 Loss 1.1703,time:6034.406059741974sec\n",
      "Epoch 1 Batch 58000 Loss 1.7724,time:6044.852539300919sec\n",
      "Epoch 1 Batch 58100 Loss 1.2070,time:6055.350308895111sec\n",
      "Epoch 1 Batch 58200 Loss 1.4434,time:6065.687637329102sec\n",
      "Epoch 1 Batch 58300 Loss 1.2584,time:6075.8749322891235sec\n",
      "Epoch 1 Batch 58400 Loss 1.0540,time:6086.149245500565sec\n",
      "Epoch 1 Batch 58500 Loss 1.3538,time:6096.559631586075sec\n",
      "Epoch 1 Batch 58600 Loss 1.2472,time:6106.874747514725sec\n",
      "Epoch 1 Batch 58700 Loss 1.5392,time:6117.006029367447sec\n",
      "Epoch 1 Batch 58800 Loss 1.0431,time:6127.271340608597sec\n",
      "Epoch 1 Batch 58900 Loss 1.3469,time:6137.548655986786sec\n",
      "Epoch 1 Batch 59000 Loss 1.0903,time:6147.744227170944sec\n",
      "Epoch 1 Batch 59100 Loss 1.5267,time:6157.913251161575sec\n",
      "Epoch 1 Batch 59200 Loss 1.2852,time:6167.997640371323sec\n",
      "Epoch 1 Batch 59300 Loss 1.3530,time:6178.293958187103sec\n",
      "Epoch 1 Batch 59400 Loss 1.2935,time:6188.503258466721sec\n",
      "Epoch 1 Batch 59500 Loss 0.9642,time:6198.827583551407sec\n",
      "Epoch 1 Batch 59600 Loss 1.0198,time:6209.104570865631sec\n",
      "Epoch 1 Batch 59700 Loss 1.1343,time:6219.453901052475sec\n",
      "Epoch 1 Batch 59800 Loss 1.0870,time:6229.896252632141sec\n",
      "Epoch 1 Batch 59900 Loss 0.6025,time:6240.369611263275sec\n",
      "Epoch 1 Batch 60000 Loss 0.8466,time:6250.79195857048sec\n",
      "Epoch 1 Batch 60100 Loss 1.4942,time:6261.085995912552sec\n",
      "Epoch 1 Batch 60200 Loss 1.1803,time:6271.4653334617615sec\n",
      "Epoch 1 Batch 60300 Loss 1.4888,time:6281.854672908783sec\n",
      "Epoch 1 Batch 60400 Loss 1.3455,time:6292.622098207474sec\n",
      "Epoch 1 Batch 60500 Loss 1.2537,time:6303.032442331314sec\n",
      "Epoch 1 Batch 60600 Loss 1.5929,time:6313.255893468857sec\n",
      "Epoch 1 Batch 60700 Loss 1.3747,time:6323.435139417648sec\n",
      "Epoch 1 Batch 60800 Loss 1.5855,time:6333.725456953049sec\n",
      "Epoch 1 Batch 60900 Loss 1.1998,time:6343.931755542755sec\n",
      "Epoch 1 Batch 61000 Loss 1.9329,time:6354.166059494019sec\n",
      "Epoch 1 Batch 61100 Loss 1.1952,time:6364.295923233032sec\n",
      "Epoch 1 Batch 61200 Loss 1.1689,time:6374.698554515839sec\n",
      "Epoch 1 Batch 61300 Loss 1.1549,time:6385.081893205643sec\n",
      "Epoch 1 Batch 61400 Loss 1.0343,time:6395.314197063446sec\n",
      "Epoch 1 Batch 61500 Loss 0.5970,time:6405.575508356094sec\n",
      "Epoch 1 Batch 61600 Loss 1.1508,time:6415.989597320557sec\n",
      "Epoch 1 Batch 61700 Loss 1.1561,time:6426.286916255951sec\n",
      "Epoch 1 Batch 61800 Loss 1.6304,time:6436.479211091995sec\n",
      "Epoch 1 Batch 61900 Loss 1.8483,time:6446.601490736008sec\n",
      "Epoch 1 Batch 62000 Loss 0.9365,time:6456.798787117004sec\n",
      "Epoch 1 Batch 62100 Loss 1.2437,time:6467.172163248062sec\n",
      "Epoch 1 Batch 62200 Loss 1.3054,time:6477.499157905579sec\n",
      "Epoch 1 Batch 62300 Loss 1.5175,time:6487.741465091705sec\n",
      "Epoch 1 Batch 62400 Loss 1.0868,time:6498.107798814774sec\n",
      "Epoch 1 Batch 62500 Loss 1.2615,time:6508.428122997284sec\n",
      "Epoch 1 Batch 62600 Loss 1.0809,time:6518.541477918625sec\n",
      "Epoch 1 Batch 62700 Loss 0.9781,time:6528.956823348999sec\n",
      "Epoch 1 Batch 62800 Loss 1.3928,time:6539.477192163467sec\n",
      "Epoch 1 Batch 62900 Loss 1.0731,time:6550.10058426857sec\n",
      "Epoch 1 Batch 63000 Loss 1.9525,time:6560.434285879135sec\n",
      "Epoch 1 Batch 63100 Loss 1.2170,time:6570.6764097213745sec\n",
      "Epoch 1 Batch 63200 Loss 1.3280,time:6581.024077177048sec\n",
      "Epoch 1 Batch 63300 Loss 1.1846,time:6591.209371089935sec\n",
      "Epoch 1 Batch 63400 Loss 1.5746,time:6601.413668394089sec\n",
      "Epoch 1 Batch 63500 Loss 1.2137,time:6611.713960409164sec\n",
      "Epoch 1 Batch 63600 Loss 1.7178,time:6621.970296621323sec\n",
      "Epoch 1 Batch 63700 Loss 1.2654,time:6632.227606534958sec\n",
      "Epoch 1 Batch 63800 Loss 1.3824,time:6642.541929721832sec\n",
      "Epoch 1 Batch 63900 Loss 1.2541,time:6652.738225221634sec\n",
      "Epoch 1 Batch 64000 Loss 1.0437,time:6663.169898986816sec\n",
      "Epoch 1 Batch 64100 Loss 1.1494,time:6673.518108844757sec\n",
      "Epoch 1 Batch 64200 Loss 1.5380,time:6683.803600311279sec\n",
      "Epoch 1 Batch 64300 Loss 1.6373,time:6694.0699117183685sec\n",
      "Epoch 1 Batch 64400 Loss 1.2072,time:6704.403239250183sec\n",
      "Epoch 1 Batch 64500 Loss 1.0458,time:6714.822097539902sec\n",
      "Epoch 1 Batch 64600 Loss 1.2489,time:6725.297987937927sec\n",
      "Epoch 1 Batch 64700 Loss 1.5827,time:6736.077657699585sec\n",
      "Epoch 1 Batch 64800 Loss 1.0294,time:6746.560017824173sec\n",
      "Epoch 1 Batch 64900 Loss 1.4969,time:6756.807325839996sec\n",
      "Epoch 1 Batch 65000 Loss 1.4970,time:6766.953610658646sec\n",
      "Epoch 1 Batch 65100 Loss 1.3123,time:6777.223166942596sec\n",
      "Epoch 1 Batch 65200 Loss 1.0118,time:6787.49948143959sec\n",
      "Epoch 1 Batch 65300 Loss 1.2754,time:6797.751790046692sec\n",
      "Epoch 1 Batch 65400 Loss 1.4011,time:6808.082116365433sec\n",
      "Epoch 1 Batch 65500 Loss 1.2346,time:6818.3696620464325sec\n",
      "Epoch 1 Batch 65600 Loss 1.1460,time:6828.582972288132sec\n",
      "Epoch 1 Batch 65700 Loss 0.9393,time:6838.711253166199sec\n",
      "Epoch 1 Batch 65800 Loss 0.9517,time:6848.987567424774sec\n",
      "Epoch 1 Batch 65900 Loss 1.3227,time:6859.228873491287sec\n",
      "Epoch 1 Batch 66000 Loss 1.1862,time:6869.9311373233795sec\n",
      "Epoch 1 Batch 66100 Loss 1.1665,time:6880.3034727573395sec\n",
      "Epoch 1 Batch 66200 Loss 1.2386,time:6890.540778636932sec\n",
      "Epoch 1 Batch 66300 Loss 2.2846,time:6900.849100112915sec\n",
      "Epoch 1 Batch 66400 Loss 1.5275,time:6911.182945489883sec\n",
      "Epoch 1 Batch 66500 Loss 1.4250,time:6921.364080190659sec\n",
      "Epoch 1 Batch 66600 Loss 1.3377,time:6931.546373605728sec\n",
      "Epoch 1 Batch 66700 Loss 1.3323,time:6941.706660985947sec\n",
      "Epoch 1 Batch 66800 Loss 1.2797,time:6951.825940132141sec\n",
      "Epoch 1 Batch 66900 Loss 1.7808,time:6962.005858421326sec\n",
      "Epoch 1 Batch 67000 Loss 0.9675,time:6972.4159779548645sec\n",
      "Epoch 1 Batch 67100 Loss 1.1327,time:6982.946348905563sec\n",
      "Epoch 1 Batch 67200 Loss 1.3496,time:6993.294679641724sec\n",
      "Epoch 1 Batch 67300 Loss 1.4899,time:7003.661014080048sec\n",
      "Epoch 1 Batch 67400 Loss 0.8525,time:7014.090362071991sec\n",
      "Epoch 1 Batch 67500 Loss 1.4114,time:7024.409768104553sec\n",
      "Epoch 1 Batch 67600 Loss 0.9631,time:7034.751329660416sec\n",
      "Epoch 1 Batch 67700 Loss 1.7061,time:7045.363719701767sec\n",
      "Epoch 1 Batch 67800 Loss 0.9409,time:7055.727053642273sec\n",
      "Epoch 1 Batch 67900 Loss 1.3499,time:7066.319438695908sec\n",
      "Epoch 1 Batch 68000 Loss 0.7980,time:7076.485831975937sec\n",
      "Epoch 1 Batch 68100 Loss 1.2094,time:7086.712249517441sec\n",
      "Epoch 1 Batch 68200 Loss 1.2487,time:7096.851533174515sec\n",
      "Epoch 1 Batch 68300 Loss 1.1384,time:7107.044828653336sec\n",
      "Epoch 1 Batch 68400 Loss 1.4736,time:7117.26513004303sec\n",
      "Epoch 1 Batch 68500 Loss 0.9456,time:7127.381348371506sec\n",
      "Epoch 1 Batch 68600 Loss 1.0956,time:7137.623440980911sec\n",
      "Epoch 1 Batch 68700 Loss 1.4457,time:7147.8556072711945sec\n",
      "Epoch 1 Batch 68800 Loss 1.3835,time:7158.140923500061sec\n",
      "Epoch 1 Batch 68900 Loss 1.0823,time:7168.411236047745sec\n",
      "Epoch 1 Batch 69000 Loss 0.9913,time:7178.691551208496sec\n",
      "Epoch 1 Batch 69100 Loss 1.1606,time:7188.999656915665sec\n",
      "Epoch 1 Batch 69200 Loss 1.5477,time:7199.151060819626sec\n",
      "Epoch 1 Batch 69300 Loss 1.5467,time:7209.261336803436sec\n",
      "Epoch 1 Batch 69400 Loss 1.4532,time:7219.380616188049sec\n",
      "Epoch 1 Batch 69500 Loss 1.2764,time:7229.898984909058sec\n",
      "Epoch 1 Batch 69600 Loss 0.8900,time:7240.087456226349sec\n",
      "Epoch 1 Batch 69700 Loss 1.3372,time:7250.408780574799sec\n",
      "Epoch 1 Batch 69800 Loss 1.5405,time:7260.591074228287sec\n",
      "Epoch 1 Batch 69900 Loss 1.9063,time:7270.765365123749sec\n",
      "Epoch 1 Batch 70000 Loss 1.2220,time:7281.209224224091sec\n",
      "Epoch 1 Batch 70100 Loss 1.2114,time:7291.741548538208sec\n",
      "Epoch 1 Batch 70200 Loss 0.8505,time:7302.192902565002sec\n",
      "Epoch 1 Batch 70300 Loss 1.3209,time:7312.595245599747sec\n",
      "Epoch 1 Batch 70400 Loss 1.3382,time:7322.858556509018sec\n",
      "Epoch 1 Batch 70500 Loss 1.4383,time:7333.12828874588sec\n",
      "Epoch 1 Batch 70600 Loss 0.6810,time:7343.310972929001sec\n",
      "Epoch 1 Batch 70700 Loss 1.3212,time:7353.411247730255sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 70800 Loss 1.4416,time:7363.664556264877sec\n",
      "Epoch 1 Batch 70900 Loss 1.6147,time:7373.811841964722sec\n",
      "Epoch 1 Batch 71000 Loss 1.4529,time:7383.998487472534sec\n",
      "Epoch 1 Batch 71100 Loss 0.8558,time:7394.216017007828sec\n",
      "Epoch 1 Batch 71200 Loss 0.9874,time:7404.370303869247sec\n",
      "Epoch 1 Batch 71300 Loss 1.2154,time:7414.543594837189sec\n",
      "Epoch 1 Batch 71400 Loss 1.1336,time:7424.766896486282sec\n",
      "Epoch 1 Batch 71500 Loss 1.7041,time:7434.960700273514sec\n",
      "Epoch 1 Batch 71600 Loss 1.2234,time:7445.12654209137sec\n",
      "Epoch 1 Batch 71700 Loss 1.3287,time:7455.340841770172sec\n",
      "Epoch 1 Batch 71800 Loss 1.1901,time:7465.8902180194855sec\n",
      "Epoch 1 Batch 71900 Loss 0.8642,time:7476.309566497803sec\n",
      "Epoch 1 Batch 72000 Loss 1.4175,time:7486.537227153778sec\n",
      "Epoch 1 Batch 72100 Loss 1.2938,time:7496.641484260559sec\n",
      "Epoch 1 Batch 72200 Loss 1.1887,time:7506.694852113724sec\n",
      "Epoch 1 Batch 72300 Loss 1.0379,time:7516.814121723175sec\n",
      "Epoch 1 Batch 72400 Loss 1.0574,time:7527.078433275223sec\n",
      "Epoch 1 Batch 72500 Loss 1.0009,time:7537.441843509674sec\n",
      "Epoch 1 Batch 72600 Loss 1.4950,time:7547.751130104065sec\n",
      "Epoch 1 Batch 72700 Loss 1.4521,time:7558.0474491119385sec\n",
      "Epoch 1 Loss 1.3104\n",
      "Time taken for 1 epoch 7565.968233346939 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8290,time:1.5048508644104004sec\n",
      "Epoch 2 Batch 100 Loss 1.2937,time:11.689652442932129sec\n",
      "Epoch 2 Batch 200 Loss 1.5784,time:21.895580291748047sec\n",
      "Epoch 2 Batch 300 Loss 0.8686,time:32.11514329910278sec\n",
      "Epoch 2 Batch 400 Loss 1.6608,time:42.48547863960266sec\n",
      "Epoch 2 Batch 500 Loss 1.0746,time:52.647767066955566sec\n",
      "Epoch 2 Batch 600 Loss 1.5703,time:62.82505941390991sec\n",
      "Epoch 2 Batch 700 Loss 1.1328,time:73.12726187705994sec\n",
      "Epoch 2 Batch 800 Loss 1.2305,time:83.20935368537903sec\n",
      "Epoch 2 Batch 900 Loss 1.3206,time:93.2386121749878sec\n",
      "Epoch 2 Batch 1000 Loss 1.4623,time:103.32788443565369sec\n",
      "Epoch 2 Batch 1100 Loss 0.9517,time:113.51817893981934sec\n",
      "Epoch 2 Batch 1200 Loss 1.0894,time:123.65173077583313sec\n",
      "Epoch 2 Batch 1300 Loss 1.3241,time:133.9918932914734sec\n",
      "Epoch 2 Batch 1400 Loss 1.0263,time:144.11643934249878sec\n",
      "Epoch 2 Batch 1500 Loss 1.2501,time:154.2277159690857sec\n",
      "Epoch 2 Batch 1600 Loss 0.8183,time:164.53403759002686sec\n",
      "Epoch 2 Batch 1700 Loss 0.9318,time:174.73633527755737sec\n",
      "Epoch 2 Batch 1800 Loss 0.9568,time:184.9278540611267sec\n",
      "Epoch 2 Batch 1900 Loss 1.0485,time:195.08548378944397sec\n",
      "Epoch 2 Batch 2000 Loss 1.5024,time:205.14674925804138sec\n",
      "Epoch 2 Batch 2100 Loss 1.1904,time:215.81715202331543sec\n",
      "Epoch 2 Batch 2200 Loss 1.5509,time:226.29551243782043sec\n",
      "Epoch 2 Batch 2300 Loss 1.1966,time:236.601220369339sec\n",
      "Epoch 2 Batch 2400 Loss 1.2938,time:246.83352422714233sec\n",
      "Epoch 2 Batch 2500 Loss 1.3970,time:257.0638282299042sec\n",
      "Epoch 2 Batch 2600 Loss 1.5007,time:267.16110157966614sec\n",
      "Epoch 2 Batch 2700 Loss 1.1277,time:277.2739450931549sec\n",
      "Epoch 2 Batch 2800 Loss 1.5466,time:287.3624620437622sec\n",
      "Epoch 2 Batch 2900 Loss 1.3998,time:297.5677604675293sec\n",
      "Epoch 2 Batch 3000 Loss 0.9591,time:307.7740592956543sec\n",
      "Epoch 2 Batch 3100 Loss 1.1941,time:318.0103642940521sec\n",
      "Epoch 2 Batch 3200 Loss 1.2118,time:328.1367018222809sec\n",
      "Epoch 2 Batch 3300 Loss 1.2712,time:338.29026222229004sec\n",
      "Epoch 2 Batch 3400 Loss 1.1101,time:348.5315682888031sec\n",
      "Epoch 2 Batch 3500 Loss 1.1717,time:358.79888010025024sec\n",
      "Epoch 2 Batch 3600 Loss 1.3880,time:369.2792408466339sec\n",
      "Epoch 2 Batch 3700 Loss 1.2549,time:379.34750843048096sec\n",
      "Epoch 2 Batch 3800 Loss 1.3801,time:389.4371340274811sec\n",
      "Epoch 2 Batch 3900 Loss 2.0643,time:399.5976037979126sec\n",
      "Epoch 2 Batch 4000 Loss 1.0735,time:409.7148826122284sec\n",
      "Epoch 2 Batch 4100 Loss 0.8937,time:419.8131568431854sec\n",
      "Epoch 2 Batch 4200 Loss 1.5016,time:430.0514624118805sec\n",
      "Epoch 2 Batch 4300 Loss 1.6412,time:440.30176997184753sec\n",
      "Epoch 2 Batch 4400 Loss 2.1253,time:450.5457227230072sec\n",
      "Epoch 2 Batch 4500 Loss 1.2056,time:460.9442627429962sec\n",
      "Epoch 2 Batch 4600 Loss 1.1337,time:471.25058460235596sec\n",
      "Epoch 2 Batch 4700 Loss 1.5899,time:481.4998924732208sec\n",
      "Epoch 2 Batch 4800 Loss 1.3778,time:491.8063440322876sec\n",
      "Epoch 2 Batch 4900 Loss 1.2269,time:502.00647830963135sec\n",
      "Epoch 2 Batch 5000 Loss 1.2230,time:512.1696383953094sec\n",
      "Epoch 2 Batch 5100 Loss 1.4086,time:522.2679126262665sec\n",
      "Epoch 2 Batch 5200 Loss 1.1809,time:532.3311786651611sec\n",
      "Epoch 2 Batch 5300 Loss 1.2133,time:542.5384776592255sec\n",
      "Epoch 2 Batch 5400 Loss 1.0030,time:552.851719379425sec\n",
      "Epoch 2 Batch 5500 Loss 1.3253,time:563.08948969841sec\n",
      "Epoch 2 Batch 5600 Loss 0.8337,time:573.4088129997253sec\n",
      "Epoch 2 Batch 5700 Loss 1.6779,time:583.7761480808258sec\n",
      "Epoch 2 Batch 5800 Loss 1.3740,time:594.0314574241638sec\n",
      "Epoch 2 Batch 5900 Loss 1.1104,time:604.1598081588745sec\n",
      "Epoch 2 Batch 6000 Loss 1.4070,time:614.3096289634705sec\n",
      "Epoch 2 Batch 6100 Loss 1.3899,time:624.5929441452026sec\n",
      "Epoch 2 Batch 6200 Loss 1.6979,time:634.6432077884674sec\n",
      "Epoch 2 Batch 6300 Loss 1.4176,time:644.7234773635864sec\n",
      "Epoch 2 Batch 6400 Loss 1.2552,time:654.8390669822693sec\n",
      "Epoch 2 Batch 6500 Loss 1.0559,time:665.1929202079773sec\n",
      "Epoch 2 Batch 6600 Loss 1.4203,time:675.4702348709106sec\n",
      "Epoch 2 Batch 6700 Loss 1.4337,time:685.7295458316803sec\n",
      "Epoch 2 Batch 6800 Loss 1.1876,time:695.9708511829376sec\n",
      "Epoch 2 Batch 6900 Loss 1.0577,time:706.1454558372498sec\n",
      "Epoch 2 Batch 7000 Loss 1.7302,time:716.4138338565826sec\n",
      "Epoch 2 Batch 7100 Loss 1.2796,time:726.6471383571625sec\n",
      "Epoch 2 Batch 7200 Loss 2.0583,time:736.9874670505524sec\n",
      "Epoch 2 Batch 7300 Loss 1.3918,time:747.3242287635803sec\n",
      "Epoch 2 Batch 7400 Loss 1.8220,time:757.4887809753418sec\n",
      "Epoch 2 Batch 7500 Loss 1.0621,time:767.6290438175201sec\n",
      "Epoch 2 Batch 7600 Loss 1.3047,time:777.8063356876373sec\n",
      "Epoch 2 Batch 7700 Loss 1.3183,time:788.0456416606903sec\n",
      "Epoch 2 Batch 7800 Loss 1.3122,time:798.3942124843597sec\n",
      "Epoch 2 Batch 7900 Loss 1.1011,time:808.4717478752136sec\n",
      "Epoch 2 Batch 8000 Loss 1.0637,time:818.6512558460236sec\n",
      "Epoch 2 Batch 8100 Loss 1.4114,time:828.8075428009033sec\n",
      "Epoch 2 Batch 8200 Loss 0.9262,time:838.9598290920258sec\n",
      "Epoch 2 Batch 8300 Loss 1.4391,time:849.3116674423218sec\n",
      "Epoch 2 Batch 8400 Loss 1.4895,time:859.5820980072021sec\n",
      "Epoch 2 Batch 8500 Loss 1.4594,time:869.7323842048645sec\n",
      "Epoch 2 Batch 8600 Loss 1.3627,time:879.9096760749817sec\n",
      "Epoch 2 Batch 8700 Loss 0.8394,time:890.0799663066864sec\n",
      "Epoch 2 Batch 8800 Loss 1.0487,time:900.1423187255859sec\n",
      "Epoch 2 Batch 8900 Loss 1.1521,time:910.471853017807sec\n",
      "Epoch 2 Batch 9000 Loss 1.0944,time:920.9602150917053sec\n",
      "Epoch 2 Batch 9100 Loss 0.7033,time:931.1985204219818sec\n",
      "Epoch 2 Batch 9200 Loss 1.2704,time:941.5278470516205sec\n",
      "Epoch 2 Batch 9300 Loss 1.2482,time:951.6553077697754sec\n",
      "Epoch 2 Batch 9400 Loss 1.2913,time:961.7360298633575sec\n",
      "Epoch 2 Batch 9500 Loss 1.4282,time:972.0023419857025sec\n",
      "Epoch 2 Batch 9600 Loss 1.5386,time:982.2986614704132sec\n",
      "Epoch 2 Batch 9700 Loss 1.4619,time:992.5969800949097sec\n",
      "Epoch 2 Batch 9800 Loss 0.9937,time:1002.8199527263641sec\n",
      "Epoch 2 Batch 9900 Loss 1.0919,time:1013.0896642208099sec\n",
      "Epoch 2 Batch 10000 Loss 1.3684,time:1023.2909615039825sec\n",
      "Epoch 2 Batch 10100 Loss 1.4383,time:1033.3742325305939sec\n",
      "Epoch 2 Batch 10200 Loss 1.6969,time:1043.5390346050262sec\n",
      "Epoch 2 Batch 10300 Loss 1.3930,time:1053.815183877945sec\n",
      "Epoch 2 Batch 10400 Loss 1.3934,time:1064.0324850082397sec\n",
      "Epoch 2 Batch 10500 Loss 1.3057,time:1074.1237578392029sec\n",
      "Epoch 2 Batch 10600 Loss 0.9870,time:1084.315052986145sec\n",
      "Epoch 2 Batch 10700 Loss 1.0800,time:1094.6896903514862sec\n",
      "Epoch 2 Batch 10800 Loss 1.2034,time:1105.0766651630402sec\n",
      "Epoch 2 Batch 10900 Loss 1.2745,time:1115.390988111496sec\n",
      "Epoch 2 Batch 11000 Loss 1.0343,time:1125.66730260849sec\n",
      "Epoch 2 Batch 11100 Loss 1.3943,time:1135.8846035003662sec\n",
      "Epoch 2 Batch 11200 Loss 1.4118,time:1146.0434169769287sec\n",
      "Epoch 2 Batch 11300 Loss 1.1916,time:1156.1285688877106sec\n",
      "Epoch 2 Batch 11400 Loss 1.0551,time:1166.2148406505585sec\n",
      "Epoch 2 Batch 11500 Loss 0.8069,time:1176.37712931633sec\n",
      "Epoch 2 Batch 11600 Loss 1.8093,time:1186.6784491539001sec\n",
      "Epoch 2 Batch 11700 Loss 0.9177,time:1196.7579679489136sec\n",
      "Epoch 2 Batch 11800 Loss 1.3111,time:1206.9390606880188sec\n",
      "Epoch 2 Batch 11900 Loss 1.6079,time:1217.1923701763153sec\n",
      "Epoch 2 Batch 12000 Loss 1.4906,time:1227.5016913414001sec\n",
      "Epoch 2 Batch 12100 Loss 1.4620,time:1238.063799381256sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 12200 Loss 0.7793,time:1248.5703060626984sec\n",
      "Epoch 2 Batch 12300 Loss 1.2175,time:1258.7686023712158sec\n",
      "Epoch 2 Batch 12400 Loss 1.2618,time:1268.9548962116241sec\n",
      "Epoch 2 Batch 12500 Loss 1.6037,time:1279.1781985759735sec\n",
      "Epoch 2 Batch 12600 Loss 0.8081,time:1289.23202085495sec\n",
      "Epoch 2 Batch 12700 Loss 1.0597,time:1299.4163610935211sec\n",
      "Epoch 2 Batch 12800 Loss 1.3980,time:1309.7226822376251sec\n",
      "Epoch 2 Batch 12900 Loss 1.1386,time:1319.8019518852234sec\n",
      "Epoch 2 Batch 13000 Loss 1.4546,time:1329.9552381038666sec\n",
      "Epoch 2 Batch 13100 Loss 1.4739,time:1340.010006904602sec\n",
      "Epoch 2 Batch 13200 Loss 1.0796,time:1350.0408957004547sec\n",
      "Epoch 2 Batch 13300 Loss 1.0566,time:1360.1912956237793sec\n",
      "Epoch 2 Batch 13400 Loss 0.9460,time:1370.401594877243sec\n",
      "Epoch 2 Batch 13500 Loss 1.2085,time:1380.6569049358368sec\n",
      "Epoch 2 Batch 13600 Loss 1.2644,time:1390.964225769043sec\n",
      "Epoch 2 Batch 13700 Loss 0.9851,time:1401.3320837020874sec\n",
      "Epoch 2 Batch 13800 Loss 1.7577,time:1411.5198242664337sec\n",
      "Epoch 2 Batch 13900 Loss 1.4168,time:1421.7831361293793sec\n",
      "Epoch 2 Batch 14000 Loss 0.9662,time:1431.9684295654297sec\n",
      "Epoch 2 Batch 14100 Loss 1.2613,time:1442.5188057422638sec\n",
      "Epoch 2 Batch 14200 Loss 1.1007,time:1452.7651734352112sec\n",
      "Epoch 2 Batch 14300 Loss 1.7454,time:1462.849443435669sec\n",
      "Epoch 2 Batch 14400 Loss 1.6084,time:1473.1207571029663sec\n",
      "Epoch 2 Batch 14500 Loss 0.9477,time:1483.348060131073sec\n",
      "Epoch 2 Batch 14600 Loss 1.3084,time:1493.6137528419495sec\n",
      "Epoch 2 Batch 14700 Loss 1.4058,time:1503.9021277427673sec\n",
      "Epoch 2 Batch 14800 Loss 0.5208,time:1514.0664129257202sec\n",
      "Epoch 2 Batch 14900 Loss 1.2551,time:1524.2327024936676sec\n",
      "Epoch 2 Batch 15000 Loss 1.6367,time:1534.4259979724884sec\n",
      "Epoch 2 Batch 15100 Loss 1.7089,time:1544.6765897274017sec\n",
      "Epoch 2 Batch 15200 Loss 1.2765,time:1554.8884270191193sec\n",
      "Epoch 2 Batch 15300 Loss 1.2024,time:1564.9776995182037sec\n",
      "Epoch 2 Batch 15400 Loss 0.9745,time:1575.0339636802673sec\n",
      "Epoch 2 Batch 15500 Loss 1.1390,time:1585.0632228851318sec\n",
      "Epoch 2 Batch 15600 Loss 1.4679,time:1595.1125609874725sec\n",
      "Epoch 2 Batch 15700 Loss 1.3573,time:1605.4730343818665sec\n",
      "Epoch 2 Batch 15800 Loss 0.9141,time:1615.753350019455sec\n",
      "Epoch 2 Batch 15900 Loss 1.2343,time:1626.1216852664948sec\n",
      "Epoch 2 Batch 16000 Loss 0.8622,time:1636.4840183258057sec\n",
      "Epoch 2 Batch 16100 Loss 1.2632,time:1646.7695922851562sec\n",
      "Epoch 2 Batch 16200 Loss 0.9403,time:1657.0981051921844sec\n",
      "Epoch 2 Batch 16300 Loss 1.0136,time:1667.5554604530334sec\n",
      "Epoch 2 Batch 16400 Loss 1.3149,time:1678.041822195053sec\n",
      "Epoch 2 Batch 16500 Loss 0.8897,time:1688.3991549015045sec\n",
      "Epoch 2 Batch 16600 Loss 1.2016,time:1698.5750131607056sec\n",
      "Epoch 2 Batch 16700 Loss 1.0556,time:1708.7370812892914sec\n",
      "Epoch 2 Batch 16800 Loss 1.2754,time:1718.9533817768097sec\n",
      "Epoch 2 Batch 16900 Loss 0.8605,time:1729.187686920166sec\n",
      "Epoch 2 Batch 17000 Loss 1.5964,time:1739.4940078258514sec\n",
      "Epoch 2 Batch 17100 Loss 1.6486,time:1749.8685023784637sec\n",
      "Epoch 2 Batch 17200 Loss 1.3757,time:1760.1008079051971sec\n",
      "Epoch 2 Batch 17300 Loss 1.5029,time:1770.3021051883698sec\n",
      "Epoch 2 Batch 17400 Loss 1.5426,time:1780.5084035396576sec\n",
      "Epoch 2 Batch 17500 Loss 1.2294,time:1790.6776938438416sec\n",
      "Epoch 2 Batch 17600 Loss 1.0991,time:1801.0302884578705sec\n",
      "Epoch 2 Batch 17700 Loss 1.0233,time:1811.1577589511871sec\n",
      "Epoch 2 Batch 17800 Loss 1.1127,time:1821.3510546684265sec\n",
      "Epoch 2 Batch 17900 Loss 1.7278,time:1831.5793583393097sec\n",
      "Epoch 2 Batch 18000 Loss 0.9777,time:1841.8756766319275sec\n",
      "Epoch 2 Batch 18100 Loss 1.2863,time:1852.3825209140778sec\n",
      "Epoch 2 Batch 18200 Loss 1.3022,time:1862.7533173561096sec\n",
      "Epoch 2 Batch 18300 Loss 1.2282,time:1873.0486359596252sec\n",
      "Epoch 2 Batch 18400 Loss 1.3886,time:1883.3259506225586sec\n",
      "Epoch 2 Batch 18500 Loss 1.3320,time:1893.5962631702423sec\n",
      "Epoch 2 Batch 18600 Loss 1.1052,time:1903.8950703144073sec\n",
      "Epoch 2 Batch 18700 Loss 1.3721,time:1914.3275151252747sec\n",
      "Epoch 2 Batch 18800 Loss 1.3385,time:1924.801873922348sec\n",
      "Epoch 2 Batch 18900 Loss 1.4481,time:1935.0581831932068sec\n",
      "Epoch 2 Batch 19000 Loss 1.3156,time:1945.3274958133698sec\n",
      "Epoch 2 Batch 19100 Loss 1.2742,time:1955.7053372859955sec\n",
      "Epoch 2 Batch 19200 Loss 1.3516,time:1966.093992471695sec\n",
      "Epoch 2 Batch 19300 Loss 0.8760,time:1976.3743078708649sec\n",
      "Epoch 2 Batch 19400 Loss 1.2564,time:1986.6556231975555sec\n",
      "Epoch 2 Batch 19500 Loss 0.9570,time:1996.8629219532013sec\n",
      "Epoch 2 Batch 19600 Loss 1.2549,time:2007.0744533538818sec\n",
      "Epoch 2 Batch 19700 Loss 1.5986,time:2017.3335590362549sec\n",
      "Epoch 2 Batch 19800 Loss 1.5581,time:2027.5908696651459sec\n",
      "Epoch 2 Batch 19900 Loss 1.3688,time:2037.7351541519165sec\n",
      "Epoch 2 Batch 20000 Loss 1.4246,time:2047.9044439792633sec\n",
      "Epoch 2 Batch 20100 Loss 0.8718,time:2058.0901505947113sec\n",
      "Epoch 2 Batch 20200 Loss 1.4540,time:2068.1832461357117sec\n",
      "Epoch 2 Batch 20300 Loss 1.5004,time:2078.3162019252777sec\n",
      "Epoch 2 Batch 20400 Loss 1.1075,time:2088.7045414447784sec\n",
      "Epoch 2 Batch 20500 Loss 1.4291,time:2099.0608739852905sec\n",
      "Epoch 2 Batch 20600 Loss 1.1549,time:2109.4287161827087sec\n",
      "Epoch 2 Batch 20700 Loss 0.8928,time:2120.036552667618sec\n",
      "Epoch 2 Batch 20800 Loss 1.4959,time:2130.3038642406464sec\n",
      "Epoch 2 Batch 20900 Loss 1.1272,time:2140.434145450592sec\n",
      "Epoch 2 Batch 21000 Loss 0.9774,time:2150.6184396743774sec\n",
      "Epoch 2 Batch 21100 Loss 1.8742,time:2160.7922608852386sec\n",
      "Epoch 2 Batch 21200 Loss 1.0969,time:2170.8833923339844sec\n",
      "Epoch 2 Batch 21300 Loss 1.3773,time:2180.9516637325287sec\n",
      "Epoch 2 Batch 21400 Loss 1.2722,time:2191.194970846176sec\n",
      "Epoch 2 Batch 21500 Loss 1.6204,time:2201.443279027939sec\n",
      "Epoch 2 Batch 21600 Loss 1.0008,time:2211.7686038017273sec\n",
      "Epoch 2 Batch 21700 Loss 1.2569,time:2222.196264028549sec\n",
      "Epoch 2 Batch 21800 Loss 1.8509,time:2232.5647938251495sec\n",
      "Epoch 2 Batch 21900 Loss 0.9763,time:2242.888118982315sec\n",
      "Epoch 2 Batch 22000 Loss 1.8769,time:2253.199440717697sec\n",
      "Epoch 2 Batch 22100 Loss 2.1012,time:2263.4387469291687sec\n",
      "Epoch 2 Batch 22200 Loss 1.2757,time:2273.6655576229095sec\n",
      "Epoch 2 Batch 22300 Loss 1.1770,time:2283.829787015915sec\n",
      "Epoch 2 Batch 22400 Loss 1.1609,time:2294.1431097984314sec\n",
      "Epoch 2 Batch 22500 Loss 1.4161,time:2304.338404893875sec\n",
      "Epoch 2 Batch 22600 Loss 1.5662,time:2314.761165857315sec\n",
      "Epoch 2 Batch 22700 Loss 1.2979,time:2324.9530861377716sec\n",
      "Epoch 2 Batch 22800 Loss 1.4029,time:2335.22008395195sec\n",
      "Epoch 2 Batch 22900 Loss 1.3305,time:2345.3783717155457sec\n",
      "Epoch 2 Batch 23000 Loss 1.3226,time:2355.593672275543sec\n",
      "Epoch 2 Batch 23100 Loss 1.2976,time:2365.888014793396sec\n",
      "Epoch 2 Batch 23200 Loss 1.1718,time:2376.3605036735535sec\n",
      "Epoch 2 Batch 23300 Loss 1.3147,time:2386.568650484085sec\n",
      "Epoch 2 Batch 23400 Loss 1.2187,time:2396.777949333191sec\n",
      "Epoch 2 Batch 23500 Loss 1.1555,time:2406.9792473316193sec\n",
      "Epoch 2 Batch 23600 Loss 1.8481,time:2417.257561683655sec\n",
      "Epoch 2 Batch 23700 Loss 2.0521,time:2427.4799349308014sec\n",
      "Epoch 2 Batch 23800 Loss 1.4754,time:2437.6833984851837sec\n",
      "Epoch 2 Batch 23900 Loss 1.2746,time:2447.8036773204803sec\n",
      "Epoch 2 Batch 24000 Loss 1.3400,time:2457.9779691696167sec\n",
      "Epoch 2 Batch 24100 Loss 0.7290,time:2468.325299024582sec\n",
      "Epoch 2 Batch 24200 Loss 1.3812,time:2478.7396261692047sec\n",
      "Epoch 2 Batch 24300 Loss 1.4010,time:2489.256525039673sec\n",
      "Epoch 2 Batch 24400 Loss 1.2050,time:2499.8008999824524sec\n",
      "Epoch 2 Batch 24500 Loss 1.4816,time:2510.1382274627686sec\n",
      "Epoch 2 Batch 24600 Loss 0.8444,time:2520.567335128784sec\n",
      "Epoch 2 Batch 24700 Loss 1.4728,time:2530.9100198745728sec\n",
      "Epoch 2 Batch 24800 Loss 1.3021,time:2541.1693308353424sec\n",
      "Epoch 2 Batch 24900 Loss 1.2915,time:2551.4516456127167sec\n",
      "Epoch 2 Batch 25000 Loss 1.3608,time:2562.0030217170715sec\n",
      "Epoch 2 Batch 25100 Loss 1.5055,time:2572.3117394447327sec\n",
      "Epoch 2 Batch 25200 Loss 1.6747,time:2582.452908039093sec\n",
      "Epoch 2 Batch 25300 Loss 1.6286,time:2592.5701870918274sec\n",
      "Epoch 2 Batch 25400 Loss 1.7734,time:2602.8214948177338sec\n",
      "Epoch 2 Batch 25500 Loss 1.1524,time:2613.064801454544sec\n",
      "Epoch 2 Batch 25600 Loss 1.8209,time:2623.268823862076sec\n",
      "Epoch 2 Batch 25700 Loss 1.2619,time:2633.5850746631622sec\n",
      "Epoch 2 Batch 25800 Loss 0.9599,time:2643.7993745803833sec\n",
      "Epoch 2 Batch 25900 Loss 1.1207,time:2653.9686648845673sec\n",
      "Epoch 2 Batch 26000 Loss 0.9484,time:2664.1929671764374sec\n",
      "Epoch 2 Batch 26100 Loss 1.4274,time:2674.534775018692sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 26200 Loss 1.6682,time:2685.1648514270782sec\n",
      "Epoch 2 Batch 26300 Loss 0.7645,time:2695.3791522979736sec\n",
      "Epoch 2 Batch 26400 Loss 1.6833,time:2705.5774490833282sec\n",
      "Epoch 2 Batch 26500 Loss 1.2922,time:2715.710731267929sec\n",
      "Epoch 2 Batch 26600 Loss 1.1831,time:2725.9744057655334sec\n",
      "Epoch 2 Batch 26700 Loss 1.5626,time:2736.3044908046722sec\n",
      "Epoch 2 Batch 26800 Loss 1.0290,time:2746.5618002414703sec\n",
      "Epoch 2 Batch 26900 Loss 1.5391,time:2756.8141090869904sec\n",
      "Epoch 2 Batch 27000 Loss 0.9059,time:2767.138434410095sec\n",
      "Epoch 2 Batch 27100 Loss 0.6490,time:2777.4192757606506sec\n",
      "Epoch 2 Batch 27200 Loss 1.2806,time:2787.6571819782257sec\n",
      "Epoch 2 Batch 27300 Loss 1.2795,time:2798.0125136375427sec\n",
      "Epoch 2 Batch 27400 Loss 1.1619,time:2808.4948740005493sec\n",
      "Epoch 2 Batch 27500 Loss 1.0869,time:2818.748183488846sec\n",
      "Epoch 2 Batch 27600 Loss 1.0586,time:2828.9207470417023sec\n",
      "Epoch 2 Batch 27700 Loss 0.8568,time:2839.0352435112sec\n",
      "Epoch 2 Batch 27800 Loss 1.0851,time:2849.2335407733917sec\n",
      "Epoch 2 Batch 27900 Loss 0.9099,time:2859.5068542957306sec\n",
      "Epoch 2 Batch 28000 Loss 1.3114,time:2869.9121975898743sec\n",
      "Epoch 2 Batch 28100 Loss 0.7583,time:2880.185779571533sec\n",
      "Epoch 2 Batch 28200 Loss 1.2640,time:2890.312764644623sec\n",
      "Epoch 2 Batch 28300 Loss 1.8428,time:2900.4100387096405sec\n",
      "Epoch 2 Batch 28400 Loss 0.8821,time:2910.6273391246796sec\n",
      "Epoch 2 Batch 28500 Loss 1.1904,time:2920.9936740398407sec\n",
      "Epoch 2 Batch 28600 Loss 1.8661,time:2931.4828550815582sec\n",
      "Epoch 2 Batch 28700 Loss 1.0931,time:2941.746166944504sec\n",
      "Epoch 2 Batch 28800 Loss 1.1548,time:2951.9184572696686sec\n",
      "Epoch 2 Batch 28900 Loss 1.5191,time:2962.285791873932sec\n",
      "Epoch 2 Batch 29000 Loss 0.8443,time:2972.586620092392sec\n",
      "Epoch 2 Batch 29100 Loss 1.0413,time:2983.0111904144287sec\n",
      "Epoch 2 Batch 29200 Loss 1.4055,time:2993.566567659378sec\n",
      "Epoch 2 Batch 29300 Loss 1.4439,time:3003.9299013614655sec\n",
      "Epoch 2 Batch 29400 Loss 1.5780,time:3014.1812102794647sec\n",
      "Epoch 2 Batch 29500 Loss 1.6668,time:3024.400907278061sec\n",
      "Epoch 2 Batch 29600 Loss 1.6595,time:3034.572345018387sec\n",
      "Epoch 2 Batch 29700 Loss 1.3425,time:3044.7898206710815sec\n",
      "Epoch 2 Batch 29800 Loss 1.2021,time:3055.1161465644836sec\n",
      "Epoch 2 Batch 29900 Loss 1.2320,time:3065.319444179535sec\n",
      "Epoch 2 Batch 30000 Loss 1.1630,time:3075.4721524715424sec\n",
      "Epoch 2 Batch 30100 Loss 1.0802,time:3085.7402398586273sec\n",
      "Epoch 2 Batch 30200 Loss 1.2894,time:3096.0275571346283sec\n",
      "Epoch 2 Batch 30300 Loss 1.2389,time:3106.204848051071sec\n",
      "Epoch 2 Batch 30400 Loss 1.4123,time:3116.547176837921sec\n",
      "Epoch 2 Batch 30500 Loss 1.1777,time:3126.721779346466sec\n",
      "Epoch 2 Batch 30600 Loss 1.3759,time:3136.9703907966614sec\n",
      "Epoch 2 Batch 30700 Loss 0.9814,time:3147.1536843776703sec\n",
      "Epoch 2 Batch 30800 Loss 1.3735,time:3157.337977170944sec\n",
      "Epoch 2 Batch 30900 Loss 1.0094,time:3167.692309141159sec\n",
      "Epoch 2 Batch 31000 Loss 1.4177,time:3177.9071860313416sec\n",
      "Epoch 2 Batch 31100 Loss 1.3362,time:3188.2847175598145sec\n",
      "Epoch 2 Batch 31200 Loss 1.1734,time:3198.4750123023987sec\n",
      "Epoch 2 Batch 31300 Loss 1.5637,time:3208.664306640625sec\n",
      "Epoch 2 Batch 31400 Loss 1.6353,time:3218.7715830802917sec\n",
      "Epoch 2 Batch 31500 Loss 1.3567,time:3228.9803018569946sec\n",
      "Epoch 2 Batch 31600 Loss 1.0001,time:3239.1740448474884sec\n",
      "Epoch 2 Batch 31700 Loss 1.4485,time:3249.3843445777893sec\n",
      "Epoch 2 Batch 31800 Loss 1.0149,time:3259.5506331920624sec\n",
      "Epoch 2 Batch 31900 Loss 1.2412,time:3269.6519088745117sec\n",
      "Epoch 2 Batch 32000 Loss 1.2988,time:3279.912364959717sec\n",
      "Epoch 2 Batch 32100 Loss 1.4916,time:3290.1227838993073sec\n",
      "Epoch 2 Batch 32200 Loss 1.2119,time:3300.2230587005615sec\n",
      "Epoch 2 Batch 32300 Loss 1.6379,time:3310.359340906143sec\n",
      "Epoch 2 Batch 32400 Loss 1.0408,time:3320.4366114139557sec\n",
      "Epoch 2 Batch 32500 Loss 0.7439,time:3330.518384695053sec\n",
      "Epoch 2 Batch 32600 Loss 1.5237,time:3340.7156286239624sec\n",
      "Epoch 2 Batch 32700 Loss 1.2527,time:3350.961154460907sec\n",
      "Epoch 2 Batch 32800 Loss 1.4321,time:3361.1854572296143sec\n",
      "Epoch 2 Batch 32900 Loss 1.3933,time:3371.3197391033173sec\n",
      "Epoch 2 Batch 33000 Loss 1.3637,time:3381.3695135116577sec\n",
      "Epoch 2 Batch 33100 Loss 1.2542,time:3391.504174232483sec\n",
      "Epoch 2 Batch 33200 Loss 0.7677,time:3401.873491048813sec\n",
      "Epoch 2 Batch 33300 Loss 1.2427,time:3412.074788570404sec\n",
      "Epoch 2 Batch 33400 Loss 1.5677,time:3422.5061373710632sec\n",
      "Epoch 2 Batch 33500 Loss 1.1542,time:3432.8714718818665sec\n",
      "Epoch 2 Batch 33600 Loss 1.3939,time:3443.189842224121sec\n",
      "Epoch 2 Batch 33700 Loss 1.2066,time:3453.55788397789sec\n",
      "Epoch 2 Batch 33800 Loss 1.4817,time:3463.891211748123sec\n",
      "Epoch 2 Batch 33900 Loss 1.2268,time:3474.244542837143sec\n",
      "Epoch 2 Batch 34000 Loss 0.9345,time:3484.596874475479sec\n",
      "Epoch 2 Batch 34100 Loss 1.3964,time:3494.829578638077sec\n",
      "Epoch 2 Batch 34200 Loss 0.9801,time:3505.0626425743103sec\n",
      "Epoch 2 Batch 34300 Loss 0.9765,time:3515.210927963257sec\n",
      "Epoch 2 Batch 34400 Loss 1.4087,time:3525.417226791382sec\n",
      "Epoch 2 Batch 34500 Loss 1.1926,time:3535.5725140571594sec\n",
      "Epoch 2 Batch 34600 Loss 1.3197,time:3545.849839448929sec\n",
      "Epoch 2 Batch 34700 Loss 1.1532,time:3556.249451637268sec\n",
      "Epoch 2 Batch 34800 Loss 1.4112,time:3566.4687526226044sec\n",
      "Epoch 2 Batch 34900 Loss 1.3556,time:3576.5560245513916sec\n",
      "Epoch 2 Batch 35000 Loss 1.4239,time:3586.8483419418335sec\n",
      "Epoch 2 Batch 35100 Loss 1.5493,time:3597.0876863002777sec\n",
      "Epoch 2 Batch 35200 Loss 1.2551,time:3607.195241689682sec\n",
      "Epoch 2 Batch 35300 Loss 1.7977,time:3617.342526912689sec\n",
      "Epoch 2 Batch 35400 Loss 1.1680,time:3627.4408004283905sec\n",
      "Epoch 2 Batch 35500 Loss 1.6032,time:3637.6831080913544sec\n",
      "Epoch 2 Batch 35600 Loss 1.5581,time:3647.971671819687sec\n",
      "Epoch 2 Batch 35700 Loss 1.4252,time:3658.297328233719sec\n",
      "Epoch 2 Batch 35800 Loss 1.2331,time:3668.49862575531sec\n",
      "Epoch 2 Batch 35900 Loss 1.1637,time:3678.7119262218475sec\n",
      "Epoch 2 Batch 36000 Loss 1.8727,time:3689.186285018921sec\n",
      "Epoch 2 Batch 36100 Loss 1.5753,time:3699.534834623337sec\n",
      "Epoch 2 Batch 36200 Loss 1.6606,time:3709.6688396930695sec\n",
      "Epoch 2 Batch 36300 Loss 1.4918,time:3719.893141746521sec\n",
      "Epoch 2 Batch 36400 Loss 1.3567,time:3730.345496416092sec\n",
      "Epoch 2 Batch 36500 Loss 1.1188,time:3740.724833250046sec\n",
      "Epoch 2 Batch 36600 Loss 1.5700,time:3751.0590946674347sec\n",
      "Epoch 2 Batch 36700 Loss 1.3380,time:3761.42022562027sec\n",
      "Epoch 2 Batch 36800 Loss 1.4960,time:3771.6338584423065sec\n",
      "Epoch 2 Batch 36900 Loss 1.6150,time:3781.9011702537537sec\n",
      "Epoch 2 Batch 37000 Loss 1.5265,time:3792.1894867420197sec\n",
      "Epoch 2 Batch 37100 Loss 1.4079,time:3802.5058102607727sec\n",
      "Epoch 2 Batch 37200 Loss 1.7774,time:3812.720478773117sec\n",
      "Epoch 2 Batch 37300 Loss 1.2658,time:3822.973831176758sec\n",
      "Epoch 2 Batch 37400 Loss 1.4138,time:3833.1651263237sec\n",
      "Epoch 2 Batch 37500 Loss 1.1603,time:3843.501454114914sec\n",
      "Epoch 2 Batch 37600 Loss 1.1445,time:3853.7657656669617sec\n",
      "Epoch 2 Batch 37700 Loss 0.9725,time:3864.0861687660217sec\n",
      "Epoch 2 Batch 37800 Loss 1.1558,time:3874.362289905548sec\n",
      "Epoch 2 Batch 37900 Loss 1.1082,time:3884.83264708519sec\n",
      "Epoch 2 Batch 38000 Loss 1.2080,time:3895.1849796772003sec\n",
      "Epoch 2 Batch 38100 Loss 1.1958,time:3905.4602932929993sec\n",
      "Epoch 2 Batch 38200 Loss 1.2449,time:3915.7193405628204sec\n",
      "Epoch 2 Batch 38300 Loss 1.3544,time:3926.240710735321sec\n",
      "Epoch 2 Batch 38400 Loss 1.5662,time:3936.6290497779846sec\n",
      "Epoch 2 Batch 38500 Loss 1.2496,time:3946.752329349518sec\n",
      "Epoch 2 Batch 38600 Loss 1.1912,time:3956.961116552353sec\n",
      "Epoch 2 Batch 38700 Loss 1.5741,time:3967.0963406562805sec\n",
      "Epoch 2 Batch 38800 Loss 1.3185,time:3977.2428743839264sec\n",
      "Epoch 2 Batch 38900 Loss 1.4070,time:3987.469177007675sec\n",
      "Epoch 2 Batch 39000 Loss 1.1635,time:3997.790501832962sec\n",
      "Epoch 2 Batch 39100 Loss 1.6034,time:4008.2718617916107sec\n",
      "Epoch 2 Batch 39200 Loss 1.3789,time:4018.5427050590515sec\n",
      "Epoch 2 Batch 39300 Loss 1.8197,time:4028.804368495941sec\n",
      "Epoch 2 Batch 39400 Loss 2.0269,time:4039.1116898059845sec\n",
      "Epoch 2 Batch 39500 Loss 1.6729,time:4049.68107008934sec\n",
      "Epoch 2 Batch 39600 Loss 1.4849,time:4059.9253771305084sec\n",
      "Epoch 2 Batch 39700 Loss 0.8899,time:4070.126033306122sec\n",
      "Epoch 2 Batch 39800 Loss 1.0549,time:4080.5898077487946sec\n",
      "Epoch 2 Batch 39900 Loss 1.1347,time:4090.7751014232635sec\n",
      "Epoch 2 Batch 40000 Loss 1.0241,time:4100.910384893417sec\n",
      "Epoch 2 Batch 40100 Loss 1.7981,time:4111.079144954681sec\n",
      "Epoch 2 Batch 40200 Loss 1.3144,time:4121.315485239029sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 40300 Loss 1.4876,time:4131.642810583115sec\n",
      "Epoch 2 Batch 40400 Loss 1.5194,time:4142.243198156357sec\n",
      "Epoch 2 Batch 40500 Loss 0.9215,time:4152.454498052597sec\n",
      "Epoch 2 Batch 40600 Loss 1.2829,time:4162.552051067352sec\n",
      "Epoch 2 Batch 40700 Loss 0.7882,time:4172.613859415054sec\n",
      "Epoch 2 Batch 40800 Loss 1.6658,time:4182.853165149689sec\n",
      "Epoch 2 Batch 40900 Loss 1.2100,time:4193.285516023636sec\n",
      "Epoch 2 Batch 41000 Loss 0.9486,time:4203.4968140125275sec\n",
      "Epoch 2 Batch 41100 Loss 1.4689,time:4213.752606868744sec\n",
      "Epoch 2 Batch 41200 Loss 1.2049,time:4223.950966119766sec\n",
      "Epoch 2 Batch 41300 Loss 1.4233,time:4234.140260696411sec\n",
      "Epoch 2 Batch 41400 Loss 1.4702,time:4244.455584526062sec\n",
      "Epoch 2 Batch 41500 Loss 1.3886,time:4254.799913406372sec\n",
      "Epoch 2 Batch 41600 Loss 1.2412,time:4265.028514146805sec\n",
      "Epoch 2 Batch 41700 Loss 1.1056,time:4275.402652978897sec\n",
      "Epoch 2 Batch 41800 Loss 1.3346,time:4285.620954036713sec\n",
      "Epoch 2 Batch 41900 Loss 1.4090,time:4295.856259584427sec\n",
      "Epoch 2 Batch 42000 Loss 1.1831,time:4306.046554803848sec\n",
      "Epoch 2 Batch 42100 Loss 1.4739,time:4316.10339140892sec\n",
      "Epoch 2 Batch 42200 Loss 1.1457,time:4326.315135955811sec\n",
      "Epoch 2 Batch 42300 Loss 1.6008,time:4336.517432928085sec\n",
      "Epoch 2 Batch 42400 Loss 1.3301,time:4346.721731185913sec\n",
      "Epoch 2 Batch 42500 Loss 0.9057,time:4356.954773187637sec\n",
      "Epoch 2 Batch 42600 Loss 2.1030,time:4367.170789241791sec\n",
      "Epoch 2 Batch 42700 Loss 1.5644,time:4377.676508665085sec\n",
      "Epoch 2 Batch 42800 Loss 1.2605,time:4387.938819885254sec\n",
      "Epoch 2 Batch 42900 Loss 0.9347,time:4398.081103801727sec\n",
      "Epoch 2 Batch 43000 Loss 1.6529,time:4408.296908855438sec\n",
      "Epoch 2 Batch 43100 Loss 0.9791,time:4418.593992471695sec\n",
      "Epoch 2 Batch 43200 Loss 1.0309,time:4428.76128578186sec\n",
      "Epoch 2 Batch 43300 Loss 1.0848,time:4439.084611415863sec\n",
      "Epoch 2 Batch 43400 Loss 1.0451,time:4449.349922418594sec\n",
      "Epoch 2 Batch 43500 Loss 0.9948,time:4459.5712242126465sec\n",
      "Epoch 2 Batch 43600 Loss 0.8369,time:4469.726688861847sec\n",
      "Epoch 2 Batch 43700 Loss 1.0593,time:4480.0924253463745sec\n",
      "Epoch 2 Batch 43800 Loss 1.4628,time:4490.517773151398sec\n",
      "Epoch 2 Batch 43900 Loss 1.0929,time:4500.959124326706sec\n",
      "Epoch 2 Batch 44000 Loss 1.5106,time:4511.267446756363sec\n",
      "Epoch 2 Batch 44100 Loss 1.3457,time:4521.539942026138sec\n",
      "Epoch 2 Batch 44200 Loss 1.2154,time:4531.846282243729sec\n",
      "Epoch 2 Batch 44300 Loss 1.2062,time:4542.064582824707sec\n",
      "Epoch 2 Batch 44400 Loss 1.2491,time:4552.217869520187sec\n",
      "Epoch 2 Batch 44500 Loss 1.4987,time:4562.509187459946sec\n",
      "Epoch 2 Batch 44600 Loss 1.2073,time:4572.717520236969sec\n",
      "Epoch 2 Batch 44700 Loss 1.3053,time:4582.942277908325sec\n",
      "Epoch 2 Batch 44800 Loss 0.8834,time:4593.250598907471sec\n",
      "Epoch 2 Batch 44900 Loss 1.5246,time:4603.43689250946sec\n",
      "Epoch 2 Batch 45000 Loss 1.2515,time:4613.713206768036sec\n",
      "Epoch 2 Batch 45100 Loss 2.1062,time:4624.051661729813sec\n",
      "Epoch 2 Batch 45200 Loss 1.5043,time:4634.29466676712sec\n",
      "Epoch 2 Batch 45300 Loss 1.1478,time:4644.573981046677sec\n",
      "Epoch 2 Batch 45400 Loss 1.4341,time:4654.827290296555sec\n",
      "Epoch 2 Batch 45500 Loss 1.3252,time:4665.232633829117sec\n",
      "Epoch 2 Batch 45600 Loss 1.9043,time:4675.743259429932sec\n",
      "Epoch 2 Batch 45700 Loss 1.4554,time:4686.173096895218sec\n",
      "Epoch 2 Batch 45800 Loss 1.5531,time:4696.506423950195sec\n",
      "Epoch 2 Batch 45900 Loss 1.9171,time:4706.886761903763sec\n",
      "Epoch 2 Batch 46000 Loss 1.1182,time:4717.245094537735sec\n",
      "Epoch 2 Batch 46100 Loss 1.0807,time:4727.472526550293sec\n",
      "Epoch 2 Batch 46200 Loss 1.1475,time:4737.787414550781sec\n",
      "Epoch 2 Batch 46300 Loss 1.2395,time:4747.995712995529sec\n",
      "Epoch 2 Batch 46400 Loss 1.7574,time:4758.5650935173035sec\n",
      "Epoch 2 Batch 46500 Loss 1.0905,time:4768.824404478073sec\n",
      "Epoch 2 Batch 46600 Loss 1.4030,time:4779.048575639725sec\n",
      "Epoch 2 Batch 46700 Loss 1.2655,time:4789.281669855118sec\n",
      "Epoch 2 Batch 46800 Loss 0.9929,time:4799.6301012039185sec\n",
      "Epoch 2 Batch 46900 Loss 0.6100,time:4809.853403329849sec\n",
      "Epoch 2 Batch 47000 Loss 1.2276,time:4820.132717847824sec\n",
      "Epoch 2 Batch 47100 Loss 0.9663,time:4830.359525918961sec\n",
      "Epoch 2 Batch 47200 Loss 1.7336,time:4840.515557527542sec\n",
      "Epoch 2 Batch 47300 Loss 1.1510,time:4850.760786056519sec\n",
      "Epoch 2 Batch 47400 Loss 1.0143,time:4861.1871337890625sec\n",
      "Epoch 2 Batch 47500 Loss 1.3835,time:4871.501456737518sec\n",
      "Epoch 2 Batch 47600 Loss 1.1572,time:4881.635739803314sec\n",
      "Epoch 2 Batch 47700 Loss 1.2995,time:4891.786318540573sec\n",
      "Epoch 2 Batch 47800 Loss 1.7563,time:4901.999226093292sec\n",
      "Epoch 2 Batch 47900 Loss 0.9170,time:4912.28254199028sec\n",
      "Epoch 2 Batch 48000 Loss 0.5146,time:4922.511844873428sec\n",
      "Epoch 2 Batch 48100 Loss 1.2623,time:4932.705141067505sec\n",
      "Epoch 2 Batch 48200 Loss 1.4760,time:4942.893795251846sec\n",
      "Epoch 2 Batch 48300 Loss 1.9235,time:4953.190825462341sec\n",
      "Epoch 2 Batch 48400 Loss 1.0141,time:4963.499146461487sec\n",
      "Epoch 2 Batch 48500 Loss 0.7554,time:4973.847477436066sec\n",
      "Epoch 2 Batch 48600 Loss 1.0177,time:4984.172802209854sec\n",
      "Epoch 2 Batch 48700 Loss 1.1964,time:4994.744430065155sec\n",
      "Epoch 2 Batch 48800 Loss 0.7856,time:5005.105803012848sec\n",
      "Epoch 2 Batch 48900 Loss 1.5650,time:5015.74919962883sec\n",
      "Epoch 2 Batch 49000 Loss 1.2194,time:5026.229559183121sec\n",
      "Epoch 2 Batch 49100 Loss 1.4123,time:5036.428856372833sec\n",
      "Epoch 2 Batch 49200 Loss 1.1421,time:5046.832590818405sec\n",
      "Epoch 2 Batch 49300 Loss 1.4503,time:5057.032219409943sec\n",
      "Epoch 2 Batch 49400 Loss 1.6334,time:5067.181505203247sec\n",
      "Epoch 2 Batch 49500 Loss 1.2462,time:5077.374801158905sec\n",
      "Epoch 2 Batch 49600 Loss 1.0074,time:5087.534088850021sec\n",
      "Epoch 2 Batch 49700 Loss 0.8554,time:5097.663509130478sec\n",
      "Epoch 2 Batch 49800 Loss 1.5200,time:5107.807146549225sec\n",
      "Epoch 2 Batch 49900 Loss 1.1444,time:5118.305510520935sec\n",
      "Epoch 2 Batch 50000 Loss 1.5808,time:5128.605830669403sec\n",
      "Epoch 2 Batch 50100 Loss 1.1259,time:5138.908150196075sec\n",
      "Epoch 2 Batch 50200 Loss 1.0475,time:5149.244684457779sec\n",
      "Epoch 2 Batch 50300 Loss 1.2222,time:5159.453531503677sec\n",
      "Epoch 2 Batch 50400 Loss 1.1455,time:5169.726844787598sec\n",
      "Epoch 2 Batch 50500 Loss 1.2028,time:5179.883131980896sec\n",
      "Epoch 2 Batch 50600 Loss 0.6576,time:5190.121438503265sec\n",
      "Epoch 2 Batch 50700 Loss 1.1946,time:5200.355983734131sec\n",
      "Epoch 2 Batch 50800 Loss 1.1836,time:5210.528871536255sec\n",
      "Epoch 2 Batch 50900 Loss 1.2233,time:5220.750173091888sec\n",
      "Epoch 2 Batch 51000 Loss 1.7470,time:5230.957472085953sec\n",
      "Epoch 2 Batch 51100 Loss 1.2603,time:5241.676886320114sec\n",
      "Epoch 2 Batch 51200 Loss 1.3484,time:5252.125073671341sec\n",
      "Epoch 2 Batch 51300 Loss 1.3986,time:5262.401387453079sec\n",
      "Epoch 2 Batch 51400 Loss 1.3271,time:5272.732714414597sec\n",
      "Epoch 2 Batch 51500 Loss 1.2292,time:5283.002026796341sec\n",
      "Epoch 2 Batch 51600 Loss 1.2950,time:5293.22532916069sec\n",
      "Epoch 2 Batch 51700 Loss 1.0359,time:5303.371709346771sec\n",
      "Epoch 2 Batch 51800 Loss 1.6288,time:5313.721312761307sec\n",
      "Epoch 2 Batch 51900 Loss 1.9146,time:5323.957617759705sec\n",
      "Epoch 2 Batch 52000 Loss 0.7757,time:5334.095900774002sec\n",
      "Epoch 2 Batch 52100 Loss 1.4358,time:5344.199175834656sec\n",
      "Epoch 2 Batch 52200 Loss 1.4005,time:5354.36470580101sec\n",
      "Epoch 2 Batch 52300 Loss 0.8911,time:5364.671212434769sec\n",
      "Epoch 2 Batch 52400 Loss 1.1254,time:5374.849504709244sec\n",
      "Epoch 2 Batch 52500 Loss 0.7750,time:5385.009792089462sec\n",
      "Epoch 2 Batch 52600 Loss 1.1142,time:5395.270678520203sec\n",
      "Epoch 2 Batch 52700 Loss 1.2176,time:5405.516379833221sec\n",
      "Epoch 2 Batch 52800 Loss 0.8762,time:5415.686669826508sec\n",
      "Epoch 2 Batch 52900 Loss 1.6804,time:5425.901969909668sec\n",
      "Epoch 2 Batch 53000 Loss 1.2683,time:5436.400334596634sec\n",
      "Epoch 2 Batch 53100 Loss 1.4411,time:5446.78188419342sec\n",
      "Epoch 2 Batch 53200 Loss 1.6419,time:5457.053754806519sec\n",
      "Epoch 2 Batch 53300 Loss 1.4397,time:5467.507107257843sec\n",
      "Epoch 2 Batch 53400 Loss 1.3011,time:5477.637388706207sec\n",
      "Epoch 2 Batch 53500 Loss 1.5948,time:5487.815680742264sec\n",
      "Epoch 2 Batch 53600 Loss 1.2806,time:5498.127357959747sec\n",
      "Epoch 2 Batch 53700 Loss 1.4472,time:5508.388034582138sec\n",
      "Epoch 2 Batch 53800 Loss 1.4168,time:5518.665349721909sec\n",
      "Epoch 2 Batch 53900 Loss 1.2737,time:5528.988674163818sec\n",
      "Epoch 2 Batch 54000 Loss 1.4249,time:5539.087949037552sec\n",
      "Epoch 2 Batch 54100 Loss 0.9957,time:5549.40980553627sec\n",
      "Epoch 2 Batch 54200 Loss 0.8554,time:5559.5751531124115sec\n",
      "Epoch 2 Batch 54300 Loss 1.2444,time:5569.775468826294sec\n",
      "Epoch 2 Batch 54400 Loss 1.4749,time:5580.085790872574sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 54500 Loss 1.4897,time:5590.6511697769165sec\n",
      "Epoch 2 Batch 54600 Loss 1.4354,time:5600.925483703613sec\n",
      "Epoch 2 Batch 54700 Loss 1.2312,time:5611.175722360611sec\n",
      "Epoch 2 Batch 54800 Loss 1.5519,time:5621.375175476074sec\n",
      "Epoch 2 Batch 54900 Loss 1.1217,time:5631.564470052719sec\n",
      "Epoch 2 Batch 55000 Loss 1.2109,time:5641.72075676918sec\n",
      "Epoch 2 Batch 55100 Loss 1.5207,time:5652.297139167786sec\n",
      "Epoch 2 Batch 55200 Loss 1.2524,time:5662.593337774277sec\n",
      "Epoch 2 Batch 55300 Loss 1.3238,time:5672.8646504879sec\n",
      "Epoch 2 Batch 55400 Loss 1.2083,time:5683.02693939209sec\n",
      "Epoch 2 Batch 55500 Loss 1.4831,time:5693.368267536163sec\n",
      "Epoch 2 Batch 55600 Loss 2.0696,time:5703.758101463318sec\n",
      "Epoch 2 Batch 55700 Loss 0.9684,time:5714.154500484467sec\n",
      "Epoch 2 Batch 55800 Loss 1.4719,time:5724.628862857819sec\n",
      "Epoch 2 Batch 55900 Loss 1.3608,time:5735.162234544754sec\n",
      "Epoch 2 Batch 56000 Loss 1.2072,time:5745.582581758499sec\n",
      "Epoch 2 Batch 56100 Loss 1.3336,time:5756.062347412109sec\n",
      "Epoch 2 Batch 56200 Loss 1.4698,time:5766.383160114288sec\n",
      "Epoch 2 Batch 56300 Loss 1.1704,time:5776.65847492218sec\n",
      "Epoch 2 Batch 56400 Loss 1.4242,time:5786.999802827835sec\n",
      "Epoch 2 Batch 56500 Loss 1.1481,time:5797.240109443665sec\n",
      "Epoch 2 Batch 56600 Loss 1.3337,time:5807.398720979691sec\n",
      "Epoch 2 Batch 56700 Loss 1.1229,time:5817.614035129547sec\n",
      "Epoch 2 Batch 56800 Loss 1.0288,time:5827.743455171585sec\n",
      "Epoch 2 Batch 56900 Loss 1.0291,time:5837.876736879349sec\n",
      "Epoch 2 Batch 57000 Loss 1.5298,time:5848.158052206039sec\n",
      "Epoch 2 Batch 57100 Loss 1.4675,time:5858.542390346527sec\n",
      "Epoch 2 Batch 57200 Loss 1.3927,time:5868.843269824982sec\n",
      "Epoch 2 Batch 57300 Loss 1.6254,time:5879.056573390961sec\n",
      "Epoch 2 Batch 57400 Loss 1.2386,time:5889.301880598068sec\n",
      "Epoch 2 Batch 57500 Loss 1.2580,time:5899.622204780579sec\n",
      "Epoch 2 Batch 57600 Loss 0.9805,time:5910.093590259552sec\n",
      "Epoch 2 Batch 57700 Loss 1.4196,time:5920.275190830231sec\n",
      "Epoch 2 Batch 57800 Loss 1.4350,time:5930.648527383804sec\n",
      "Epoch 2 Batch 57900 Loss 1.7187,time:5940.917839288712sec\n",
      "Epoch 2 Batch 58000 Loss 1.3869,time:5951.152144432068sec\n",
      "Epoch 2 Batch 58100 Loss 1.2655,time:5961.381549358368sec\n",
      "Epoch 2 Batch 58200 Loss 1.4397,time:5971.550838708878sec\n",
      "Epoch 2 Batch 58300 Loss 1.5201,time:5981.781143188477sec\n",
      "Epoch 2 Batch 58400 Loss 1.0967,time:5991.970437526703sec\n",
      "Epoch 2 Batch 58500 Loss 1.3832,time:6002.223130226135sec\n",
      "Epoch 2 Batch 58600 Loss 1.3182,time:6012.4794845581055sec\n",
      "Epoch 2 Batch 58700 Loss 1.5807,time:6022.673533678055sec\n",
      "Epoch 2 Batch 58800 Loss 1.1711,time:6032.911838293076sec\n",
      "Epoch 2 Batch 58900 Loss 1.4663,time:6043.124138832092sec\n",
      "Epoch 2 Batch 59000 Loss 1.4959,time:6053.2474184036255sec\n",
      "Epoch 2 Batch 59100 Loss 0.9799,time:6063.333915472031sec\n",
      "Epoch 2 Batch 59200 Loss 1.2024,time:6073.396475076675sec\n",
      "Epoch 2 Batch 59300 Loss 0.8632,time:6083.476745843887sec\n",
      "Epoch 2 Batch 59400 Loss 2.0911,time:6093.57902097702sec\n",
      "Epoch 2 Batch 59500 Loss 0.9374,time:6103.814326286316sec\n",
      "Epoch 2 Batch 59600 Loss 1.2990,time:6114.091942548752sec\n",
      "Epoch 2 Batch 59700 Loss 1.1932,time:6124.503820180893sec\n",
      "Epoch 2 Batch 59800 Loss 1.2358,time:6134.85515165329sec\n",
      "Epoch 2 Batch 59900 Loss 1.3191,time:6145.5755660533905sec\n",
      "Epoch 2 Batch 60000 Loss 0.9744,time:6155.819872617722sec\n",
      "Epoch 2 Batch 60100 Loss 1.7761,time:6166.003337621689sec\n",
      "Epoch 2 Batch 60200 Loss 1.1956,time:6176.221773862839sec\n",
      "Epoch 2 Batch 60300 Loss 1.6288,time:6186.561101913452sec\n",
      "Epoch 2 Batch 60400 Loss 0.6600,time:6196.962444782257sec\n",
      "Epoch 2 Batch 60500 Loss 1.4967,time:6207.12499666214sec\n",
      "Epoch 2 Batch 60600 Loss 1.1707,time:6217.305825471878sec\n",
      "Epoch 2 Batch 60700 Loss 1.5988,time:6227.47111415863sec\n",
      "Epoch 2 Batch 60800 Loss 1.6197,time:6237.603396177292sec\n",
      "Epoch 2 Batch 60900 Loss 1.1691,time:6247.924720287323sec\n",
      "Epoch 2 Batch 61000 Loss 1.6097,time:6258.404405593872sec\n",
      "Epoch 2 Batch 61100 Loss 1.0690,time:6268.699795246124sec\n",
      "Epoch 2 Batch 61200 Loss 1.0650,time:6278.924508571625sec\n",
      "Epoch 2 Batch 61300 Loss 0.9497,time:6289.214825630188sec\n",
      "Epoch 2 Batch 61400 Loss 1.1974,time:6299.466134786606sec\n",
      "Epoch 2 Batch 61500 Loss 0.9220,time:6309.702467679977sec\n",
      "Epoch 2 Batch 61600 Loss 1.2360,time:6320.03901386261sec\n",
      "Epoch 2 Batch 61700 Loss 0.8779,time:6330.14529299736sec\n",
      "Epoch 2 Batch 61800 Loss 1.0397,time:6340.334587574005sec\n",
      "Epoch 2 Batch 61900 Loss 1.0448,time:6350.519881486893sec\n",
      "Epoch 2 Batch 62000 Loss 0.8919,time:6360.713177204132sec\n",
      "Epoch 2 Batch 62100 Loss 1.2013,time:6371.058488845825sec\n",
      "Epoch 2 Batch 62200 Loss 0.9472,time:6381.314799070358sec\n",
      "Epoch 2 Batch 62300 Loss 0.9229,time:6391.544102430344sec\n",
      "Epoch 2 Batch 62400 Loss 1.3412,time:6401.773405790329sec\n",
      "Epoch 2 Batch 62500 Loss 1.6109,time:6411.861430168152sec\n",
      "Epoch 2 Batch 62600 Loss 1.5373,time:6421.92967748642sec\n",
      "Epoch 2 Batch 62700 Loss 1.0937,time:6432.2440004348755sec\n",
      "Epoch 2 Batch 62800 Loss 0.9625,time:6442.566324472427sec\n",
      "Epoch 2 Batch 62900 Loss 1.2707,time:6452.914631843567sec\n",
      "Epoch 2 Batch 63000 Loss 1.3195,time:6463.25234246254sec\n",
      "Epoch 2 Batch 63100 Loss 0.8129,time:6473.556663036346sec\n",
      "Epoch 2 Batch 63200 Loss 1.0561,time:6483.935000181198sec\n",
      "Epoch 2 Batch 63300 Loss 1.0086,time:6494.43036365509sec\n",
      "Epoch 2 Batch 63400 Loss 1.3526,time:6504.888753890991sec\n",
      "Epoch 2 Batch 63500 Loss 1.4034,time:6515.4326803684235sec\n",
      "Epoch 2 Batch 63600 Loss 0.9399,time:6525.907040119171sec\n",
      "Epoch 2 Batch 63700 Loss 0.8705,time:6536.151346683502sec\n",
      "Epoch 2 Batch 63800 Loss 1.4223,time:6546.271625518799sec\n",
      "Epoch 2 Batch 63900 Loss 1.3151,time:6556.475043296814sec\n",
      "Epoch 2 Batch 64000 Loss 1.5050,time:6566.591663837433sec\n",
      "Epoch 2 Batch 64100 Loss 0.8097,time:6576.753952264786sec\n",
      "Epoch 2 Batch 64200 Loss 1.5089,time:6586.974254369736sec\n",
      "Epoch 2 Batch 64300 Loss 1.2003,time:6597.17355132103sec\n",
      "Epoch 2 Batch 64400 Loss 0.5458,time:6607.423138141632sec\n",
      "Epoch 2 Batch 64500 Loss 0.8381,time:6617.60453248024sec\n",
      "Epoch 2 Batch 64600 Loss 0.9184,time:6627.858251571655sec\n",
      "Epoch 2 Batch 64700 Loss 1.1607,time:6638.176575422287sec\n",
      "Epoch 2 Batch 64800 Loss 1.0919,time:6648.782964229584sec\n",
      "Epoch 2 Batch 64900 Loss 1.2279,time:6659.027921199799sec\n",
      "Epoch 2 Batch 65000 Loss 1.2627,time:6669.127638578415sec\n",
      "Epoch 2 Batch 65100 Loss 1.0470,time:6679.184903383255sec\n",
      "Epoch 2 Batch 65200 Loss 1.4420,time:6689.330188512802sec\n",
      "Epoch 2 Batch 65300 Loss 0.7980,time:6699.542488574982sec\n",
      "Epoch 2 Batch 65400 Loss 1.2974,time:6709.642033100128sec\n",
      "Epoch 2 Batch 65500 Loss 1.3290,time:6719.727815151215sec\n",
      "Epoch 2 Batch 65600 Loss 1.6909,time:6729.959118843079sec\n",
      "Epoch 2 Batch 65700 Loss 1.4376,time:6740.3264536857605sec\n",
      "Epoch 2 Batch 65800 Loss 1.5869,time:6750.630774259567sec\n",
      "Epoch 2 Batch 65900 Loss 1.2226,time:6760.922620773315sec\n",
      "Epoch 2 Batch 66000 Loss 1.0806,time:6771.458209991455sec\n",
      "Epoch 2 Batch 66100 Loss 0.6698,time:6781.952573776245sec\n",
      "Epoch 2 Batch 66200 Loss 1.9634,time:6792.133866548538sec\n",
      "Epoch 2 Batch 66300 Loss 1.7789,time:6802.381174564362sec\n",
      "Epoch 2 Batch 66400 Loss 1.0991,time:6812.630861282349sec\n",
      "Epoch 2 Batch 66500 Loss 1.1531,time:6822.925008535385sec\n",
      "Epoch 2 Batch 66600 Loss 1.0812,time:6833.014280557632sec\n",
      "Epoch 2 Batch 66700 Loss 1.7030,time:6843.210576534271sec\n",
      "Epoch 2 Batch 66800 Loss 1.4593,time:6853.344858884811sec\n",
      "Epoch 2 Batch 66900 Loss 1.2106,time:6863.4706428050995sec\n",
      "Epoch 2 Batch 67000 Loss 1.2299,time:6873.599360227585sec\n",
      "Epoch 2 Batch 67100 Loss 1.1389,time:6883.856670379639sec\n",
      "Epoch 2 Batch 67200 Loss 1.2639,time:6894.140986919403sec\n",
      "Epoch 2 Batch 67300 Loss 1.4185,time:6904.24326133728sec\n",
      "Epoch 2 Batch 67400 Loss 1.2933,time:6914.407054901123sec\n",
      "Epoch 2 Batch 67500 Loss 1.2115,time:6924.6133460998535sec\n",
      "Epoch 2 Batch 67600 Loss 1.5517,time:6934.833647489548sec\n",
      "Epoch 2 Batch 67700 Loss 1.1897,time:6945.017941236496sec\n",
      "Epoch 2 Batch 67800 Loss 1.3229,time:6955.24555683136sec\n",
      "Epoch 2 Batch 67900 Loss 1.5576,time:6965.412217140198sec\n",
      "Epoch 2 Batch 68000 Loss 1.0800,time:6975.654523611069sec\n",
      "Epoch 2 Batch 68100 Loss 1.4880,time:6985.861822128296sec\n",
      "Epoch 2 Batch 68200 Loss 0.9900,time:6996.128134489059sec\n",
      "Epoch 2 Batch 68300 Loss 0.8705,time:7006.520277500153sec\n",
      "Epoch 2 Batch 68400 Loss 1.3467,time:7016.805038452148sec\n",
      "Epoch 2 Batch 68500 Loss 1.3694,time:7027.084654569626sec\n",
      "Epoch 2 Batch 68600 Loss 1.5070,time:7037.277949810028sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 68700 Loss 1.3533,time:7047.48924946785sec\n",
      "Epoch 2 Batch 68800 Loss 1.2698,time:7057.783035039902sec\n",
      "Epoch 2 Batch 68900 Loss 1.2514,time:7067.932955741882sec\n",
      "Epoch 2 Batch 69000 Loss 1.3884,time:7078.297629833221sec\n",
      "Epoch 2 Batch 69100 Loss 1.3048,time:7088.926023244858sec\n",
      "Epoch 2 Batch 69200 Loss 0.9649,time:7099.229343652725sec\n",
      "Epoch 2 Batch 69300 Loss 1.8184,time:7109.4836530685425sec\n",
      "Epoch 2 Batch 69400 Loss 1.3138,time:7119.719070672989sec\n",
      "Epoch 2 Batch 69500 Loss 1.0693,time:7130.030380964279sec\n",
      "Epoch 2 Batch 69600 Loss 1.3517,time:7140.444726705551sec\n",
      "Epoch 2 Batch 69700 Loss 1.2507,time:7150.657027482986sec\n",
      "Epoch 2 Batch 69800 Loss 1.3774,time:7160.844320774078sec\n",
      "Epoch 2 Batch 69900 Loss 1.4890,time:7171.026530981064sec\n",
      "Epoch 2 Batch 70000 Loss 1.2290,time:7181.177547693253sec\n",
      "Epoch 2 Batch 70100 Loss 1.0764,time:7191.422853708267sec\n",
      "Epoch 2 Batch 70200 Loss 0.8971,time:7201.645156860352sec\n",
      "Epoch 2 Batch 70300 Loss 1.0281,time:7211.8260107040405sec\n",
      "Epoch 2 Batch 70400 Loss 1.6950,time:7222.038466691971sec\n",
      "Epoch 2 Batch 70500 Loss 1.4962,time:7232.329784631729sec\n",
      "Epoch 2 Batch 70600 Loss 0.9104,time:7242.621102333069sec\n",
      "Epoch 2 Batch 70700 Loss 1.5839,time:7253.018443822861sec\n",
      "Epoch 2 Batch 70800 Loss 1.2151,time:7263.3511481285095sec\n",
      "Epoch 2 Batch 70900 Loss 0.8030,time:7273.5775141716sec\n",
      "Epoch 2 Batch 71000 Loss 1.0040,time:7283.8317975997925sec\n",
      "Epoch 2 Batch 71100 Loss 1.1244,time:7294.1831295490265sec\n",
      "Epoch 2 Batch 71200 Loss 1.2044,time:7304.505453824997sec\n",
      "Epoch 2 Batch 71300 Loss 1.1578,time:7314.83305644989sec\n",
      "Epoch 2 Batch 71400 Loss 1.3639,time:7325.006970643997sec\n",
      "Epoch 2 Batch 71500 Loss 0.6391,time:7335.108245134354sec\n",
      "Epoch 2 Batch 71600 Loss 1.0410,time:7345.21652173996sec\n",
      "Epoch 2 Batch 71700 Loss 1.2901,time:7355.470343589783sec\n",
      "Epoch 2 Batch 71800 Loss 1.3102,time:7365.611794471741sec\n",
      "Epoch 2 Batch 71900 Loss 1.5545,time:7375.810091257095sec\n",
      "Epoch 2 Batch 72000 Loss 1.1417,time:7386.031392812729sec\n",
      "Epoch 2 Batch 72100 Loss 1.4267,time:7396.3667204380035sec\n",
      "Epoch 2 Batch 72200 Loss 1.7647,time:7406.777305364609sec\n",
      "Epoch 2 Batch 72300 Loss 1.9266,time:7416.920903682709sec\n",
      "Epoch 2 Batch 72400 Loss 1.3733,time:7427.186215639114sec\n",
      "Epoch 2 Batch 72500 Loss 1.1701,time:7437.629567623138sec\n",
      "Epoch 2 Batch 72600 Loss 2.2411,time:7447.943890333176sec\n",
      "Epoch 2 Batch 72700 Loss 1.7060,time:7458.081312656403sec\n",
      "Epoch 2 Loss 1.2870\n",
      "Time taken for 1 epoch 7466.140963792801 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9157,time:1.5313448905944824sec\n",
      "Epoch 3 Batch 100 Loss 1.1479,time:11.726640701293945sec\n",
      "Epoch 3 Batch 200 Loss 1.3397,time:21.937939643859863sec\n",
      "Epoch 3 Batch 300 Loss 0.9430,time:32.1922492980957sec\n",
      "Epoch 3 Batch 400 Loss 1.0724,time:42.4649658203125sec\n",
      "Epoch 3 Batch 500 Loss 1.4333,time:52.73947882652283sec\n",
      "Epoch 3 Batch 600 Loss 1.0260,time:63.02879595756531sec\n",
      "Epoch 3 Batch 700 Loss 1.4645,time:73.380126953125sec\n",
      "Epoch 3 Batch 800 Loss 0.7034,time:83.5584192276001sec\n",
      "Epoch 3 Batch 900 Loss 1.3955,time:93.72572803497314sec\n",
      "Epoch 3 Batch 1000 Loss 0.7683,time:103.86652755737305sec\n",
      "Epoch 3 Batch 1100 Loss 0.9655,time:114.00581073760986sec\n",
      "Epoch 3 Batch 1200 Loss 1.0094,time:124.09908437728882sec\n",
      "Epoch 3 Batch 1300 Loss 0.8717,time:134.33138871192932sec\n",
      "Epoch 3 Batch 1400 Loss 1.0653,time:144.47908067703247sec\n",
      "Epoch 3 Batch 1500 Loss 0.9660,time:154.85326170921326sec\n",
      "Epoch 3 Batch 1600 Loss 1.3924,time:165.1305763721466sec\n",
      "Epoch 3 Batch 1700 Loss 1.1530,time:175.38688564300537sec\n",
      "Epoch 3 Batch 1800 Loss 0.8862,time:185.64319562911987sec\n",
      "Epoch 3 Batch 1900 Loss 1.4181,time:195.9382781982422sec\n",
      "Epoch 3 Batch 2000 Loss 0.9861,time:206.13422179222107sec\n",
      "Epoch 3 Batch 2100 Loss 1.3464,time:216.31851482391357sec\n",
      "Epoch 3 Batch 2200 Loss 1.0761,time:226.55782055854797sec\n",
      "Epoch 3 Batch 2300 Loss 0.9821,time:236.77412176132202sec\n",
      "Epoch 3 Batch 2400 Loss 1.6520,time:247.1843385696411sec\n",
      "Epoch 3 Batch 2500 Loss 0.9755,time:257.2924339771271sec\n",
      "Epoch 3 Batch 2600 Loss 1.0958,time:267.4377191066742sec\n",
      "Epoch 3 Batch 2700 Loss 1.2926,time:277.62301301956177sec\n",
      "Epoch 3 Batch 2800 Loss 1.0752,time:287.9753441810608sec\n",
      "Epoch 3 Batch 2900 Loss 1.4346,time:298.437059879303sec\n",
      "Epoch 3 Batch 3000 Loss 1.5404,time:308.84865498542786sec\n",
      "Epoch 3 Batch 3100 Loss 1.3921,time:319.1289699077606sec\n",
      "Epoch 3 Batch 3200 Loss 0.9134,time:329.49430441856384sec\n",
      "Epoch 3 Batch 3300 Loss 1.2529,time:339.69760251045227sec\n",
      "Epoch 3 Batch 3400 Loss 1.2379,time:349.85374546051025sec\n",
      "Epoch 3 Batch 3500 Loss 1.4907,time:359.98154735565186sec\n",
      "Epoch 3 Batch 3600 Loss 0.9875,time:370.0713794231415sec\n",
      "Epoch 3 Batch 3700 Loss 1.2091,time:380.13864731788635sec\n",
      "Epoch 3 Batch 3800 Loss 1.2668,time:390.23291993141174sec\n",
      "Epoch 3 Batch 3900 Loss 0.8617,time:400.34619760513306sec\n",
      "Epoch 3 Batch 4000 Loss 1.4082,time:410.5324831008911sec\n",
      "Epoch 3 Batch 4100 Loss 1.4463,time:420.6892430782318sec\n",
      "Epoch 3 Batch 4200 Loss 1.0098,time:430.9035441875458sec\n",
      "Epoch 3 Batch 4300 Loss 1.4270,time:441.17385601997375sec\n",
      "Epoch 3 Batch 4400 Loss 1.4434,time:451.36065435409546sec\n",
      "Epoch 3 Batch 4500 Loss 0.8668,time:461.5165147781372sec\n",
      "Epoch 3 Batch 4600 Loss 1.2517,time:471.58812165260315sec\n",
      "Epoch 3 Batch 4700 Loss 1.5903,time:481.80842304229736sec\n",
      "Epoch 3 Batch 4800 Loss 1.7331,time:492.2837824821472sec\n",
      "Epoch 3 Batch 4900 Loss 1.5747,time:502.8481614589691sec\n",
      "Epoch 3 Batch 5000 Loss 1.6537,time:513.659345626831sec\n",
      "Epoch 3 Batch 5100 Loss 1.1888,time:524.1657195091248sec\n",
      "Epoch 3 Batch 5200 Loss 1.6218,time:534.9171409606934sec\n",
      "Epoch 3 Batch 5300 Loss 0.6914,time:545.4600429534912sec\n",
      "Epoch 3 Batch 5400 Loss 0.9914,time:555.7230870723724sec\n",
      "Epoch 3 Batch 5500 Loss 1.0800,time:565.9563915729523sec\n",
      "Epoch 3 Batch 5600 Loss 1.5042,time:576.1296820640564sec\n",
      "Epoch 3 Batch 5700 Loss 1.3147,time:586.2749667167664sec\n",
      "Epoch 3 Batch 5800 Loss 1.5621,time:596.47580909729sec\n",
      "Epoch 3 Batch 5900 Loss 1.4390,time:606.5679740905762sec\n",
      "Epoch 3 Batch 6000 Loss 1.2455,time:616.785275220871sec\n",
      "Epoch 3 Batch 6100 Loss 0.8247,time:626.8855497837067sec\n",
      "Epoch 3 Batch 6200 Loss 1.5688,time:637.0409886837006sec\n",
      "Epoch 3 Batch 6300 Loss 1.2659,time:647.3329224586487sec\n",
      "Epoch 3 Batch 6400 Loss 1.2862,time:657.5722284317017sec\n",
      "Epoch 3 Batch 6500 Loss 1.6501,time:667.8075332641602sec\n",
      "Epoch 3 Batch 6600 Loss 1.0341,time:677.9958276748657sec\n",
      "Epoch 3 Batch 6700 Loss 1.2241,time:688.3176860809326sec\n",
      "Epoch 3 Batch 6800 Loss 1.1720,time:698.5117182731628sec\n",
      "Epoch 3 Batch 6900 Loss 1.6800,time:708.8630495071411sec\n",
      "Epoch 3 Batch 7000 Loss 1.0653,time:719.0563457012177sec\n",
      "Epoch 3 Batch 7100 Loss 1.1304,time:729.1636211872101sec\n",
      "Epoch 3 Batch 7200 Loss 1.1079,time:739.386471748352sec\n",
      "Epoch 3 Batch 7300 Loss 1.4769,time:749.5371880531311sec\n",
      "Epoch 3 Batch 7400 Loss 1.3621,time:759.6164577007294sec\n",
      "Epoch 3 Batch 7500 Loss 1.0322,time:769.7187330722809sec\n",
      "Epoch 3 Batch 7600 Loss 0.5854,time:779.8450133800507sec\n",
      "Epoch 3 Batch 7700 Loss 1.3766,time:790.1751537322998sec\n",
      "Epoch 3 Batch 7800 Loss 0.7852,time:800.5058748722076sec\n",
      "Epoch 3 Batch 7900 Loss 1.5221,time:810.8332002162933sec\n",
      "Epoch 3 Batch 8000 Loss 1.1542,time:821.2035355567932sec\n",
      "Epoch 3 Batch 8100 Loss 1.0995,time:831.5088567733765sec\n",
      "Epoch 3 Batch 8200 Loss 1.3196,time:841.6961505413055sec\n",
      "Epoch 3 Batch 8300 Loss 1.6141,time:851.9423160552979sec\n",
      "Epoch 3 Batch 8400 Loss 1.4628,time:862.1656184196472sec\n",
      "Epoch 3 Batch 8500 Loss 1.2220,time:872.4119260311127sec\n",
      "Epoch 3 Batch 8600 Loss 0.9950,time:882.6422297954559sec\n",
      "Epoch 3 Batch 8700 Loss 0.8864,time:892.9530413150787sec\n",
      "Epoch 3 Batch 8800 Loss 1.4002,time:903.1772389411926sec\n",
      "Epoch 3 Batch 8900 Loss 1.3329,time:913.3623538017273sec\n",
      "Epoch 3 Batch 9000 Loss 1.1089,time:923.4306216239929sec\n",
      "Epoch 3 Batch 9100 Loss 1.0744,time:933.5629043579102sec\n",
      "Epoch 3 Batch 9200 Loss 1.4545,time:943.7752032279968sec\n",
      "Epoch 3 Batch 9300 Loss 1.0409,time:954.1296253204346sec\n",
      "Epoch 3 Batch 9400 Loss 1.5958,time:964.427943944931sec\n",
      "Epoch 3 Batch 9500 Loss 0.9634,time:974.7742738723755sec\n",
      "Epoch 3 Batch 9600 Loss 1.1896,time:985.1926198005676sec\n",
      "Epoch 3 Batch 9700 Loss 1.4538,time:995.592399597168sec\n",
      "Epoch 3 Batch 9800 Loss 1.0892,time:1005.9550886154175sec\n",
      "Epoch 3 Batch 9900 Loss 1.4704,time:1016.0967485904694sec\n",
      "Epoch 3 Batch 10000 Loss 1.2219,time:1026.228030204773sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 10100 Loss 1.1132,time:1036.5423531532288sec\n",
      "Epoch 3 Batch 10200 Loss 1.1885,time:1046.7806594371796sec\n",
      "Epoch 3 Batch 10300 Loss 1.5021,time:1057.0210342407227sec\n",
      "Epoch 3 Batch 10400 Loss 1.0150,time:1067.2033126354218sec\n",
      "Epoch 3 Batch 10500 Loss 1.6466,time:1077.483627796173sec\n",
      "Epoch 3 Batch 10600 Loss 1.4401,time:1087.5969059467316sec\n",
      "Epoch 3 Batch 10700 Loss 1.5274,time:1097.7071824073792sec\n",
      "Epoch 3 Batch 10800 Loss 1.1596,time:1107.961791753769sec\n",
      "Epoch 3 Batch 10900 Loss 1.3253,time:1118.099425315857sec\n",
      "Epoch 3 Batch 11000 Loss 1.3566,time:1128.3797407150269sec\n",
      "Epoch 3 Batch 11100 Loss 0.8018,time:1138.7030658721924sec\n",
      "Epoch 3 Batch 11200 Loss 1.3335,time:1148.9903824329376sec\n",
      "Epoch 3 Batch 11300 Loss 1.1725,time:1159.1788129806519sec\n",
      "Epoch 3 Batch 11400 Loss 0.9374,time:1169.4388489723206sec\n",
      "Epoch 3 Batch 11500 Loss 1.3097,time:1179.6464104652405sec\n",
      "Epoch 3 Batch 11600 Loss 1.2834,time:1190.0057437419891sec\n",
      "Epoch 3 Batch 11700 Loss 1.1522,time:1200.2000398635864sec\n",
      "Epoch 3 Batch 11800 Loss 1.0709,time:1210.4238514900208sec\n",
      "Epoch 3 Batch 11900 Loss 1.4612,time:1220.5245277881622sec\n",
      "Epoch 3 Batch 12000 Loss 1.4261,time:1230.7208242416382sec\n",
      "Epoch 3 Batch 12100 Loss 1.7381,time:1240.9671318531036sec\n",
      "Epoch 3 Batch 12200 Loss 1.2489,time:1251.267451286316sec\n",
      "Epoch 3 Batch 12300 Loss 1.4102,time:1261.4763238430023sec\n",
      "Epoch 3 Batch 12400 Loss 1.5069,time:1271.7494206428528sec\n",
      "Epoch 3 Batch 12500 Loss 1.5082,time:1282.068037033081sec\n",
      "Epoch 3 Batch 12600 Loss 1.1531,time:1292.3523530960083sec\n",
      "Epoch 3 Batch 12700 Loss 1.4479,time:1302.6816790103912sec\n",
      "Epoch 3 Batch 12800 Loss 1.0873,time:1312.9709973335266sec\n",
      "Epoch 3 Batch 12900 Loss 1.3968,time:1323.2614016532898sec\n",
      "Epoch 3 Batch 13000 Loss 1.2495,time:1333.8600268363953sec\n",
      "Epoch 3 Batch 13100 Loss 0.8771,time:1344.1183366775513sec\n",
      "Epoch 3 Batch 13200 Loss 1.1725,time:1354.3296365737915sec\n",
      "Epoch 3 Batch 13300 Loss 1.7011,time:1364.5789449214935sec\n",
      "Epoch 3 Batch 13400 Loss 1.3285,time:1374.6865644454956sec\n",
      "Epoch 3 Batch 13500 Loss 1.3662,time:1384.6984453201294sec\n",
      "Epoch 3 Batch 13600 Loss 1.4461,time:1394.8867394924164sec\n",
      "Epoch 3 Batch 13700 Loss 1.2999,time:1405.1670546531677sec\n",
      "Epoch 3 Batch 13800 Loss 1.1622,time:1415.4108719825745sec\n",
      "Epoch 3 Batch 13900 Loss 1.1459,time:1425.6767845153809sec\n",
      "Epoch 3 Batch 14000 Loss 0.9168,time:1435.892085313797sec\n",
      "Epoch 3 Batch 14100 Loss 1.3766,time:1446.2604196071625sec\n",
      "Epoch 3 Batch 14200 Loss 1.4449,time:1456.5177295207977sec\n",
      "Epoch 3 Batch 14300 Loss 0.7297,time:1466.7414300441742sec\n",
      "Epoch 3 Batch 14400 Loss 1.2754,time:1476.8722593784332sec\n",
      "Epoch 3 Batch 14500 Loss 1.1498,time:1487.0027947425842sec\n",
      "Epoch 3 Batch 14600 Loss 1.5988,time:1497.227097272873sec\n",
      "Epoch 3 Batch 14700 Loss 1.0357,time:1507.431395292282sec\n",
      "Epoch 3 Batch 14800 Loss 1.2356,time:1517.5616772174835sec\n",
      "Epoch 3 Batch 14900 Loss 0.9513,time:1527.6931734085083sec\n",
      "Epoch 3 Batch 15000 Loss 1.2413,time:1538.2517893314362sec\n",
      "Epoch 3 Batch 15100 Loss 1.2430,time:1548.6451296806335sec\n",
      "Epoch 3 Batch 15200 Loss 0.9155,time:1558.8804349899292sec\n",
      "Epoch 3 Batch 15300 Loss 1.0184,time:1569.1064035892487sec\n",
      "Epoch 3 Batch 15400 Loss 0.7252,time:1579.2517113685608sec\n",
      "Epoch 3 Batch 15500 Loss 1.0702,time:1589.473013162613sec\n",
      "Epoch 3 Batch 15600 Loss 1.4294,time:1599.5702867507935sec\n",
      "Epoch 3 Batch 15700 Loss 1.0890,time:1609.6245512962341sec\n",
      "Epoch 3 Batch 15800 Loss 0.6507,time:1619.6504700183868sec\n",
      "Epoch 3 Batch 15900 Loss 1.0924,time:1629.6925973892212sec\n",
      "Epoch 3 Batch 16000 Loss 1.1029,time:1639.8978962898254sec\n",
      "Epoch 3 Batch 16100 Loss 1.1406,time:1649.9731650352478sec\n",
      "Epoch 3 Batch 16200 Loss 2.1480,time:1660.0994448661804sec\n",
      "Epoch 3 Batch 16300 Loss 1.5660,time:1670.2810189723969sec\n",
      "Epoch 3 Batch 16400 Loss 1.3135,time:1680.3672304153442sec\n",
      "Epoch 3 Batch 16500 Loss 1.0702,time:1690.5095143318176sec\n",
      "Epoch 3 Batch 16600 Loss 1.7421,time:1700.7648243904114sec\n",
      "Epoch 3 Batch 16700 Loss 0.8924,time:1711.0611426830292sec\n",
      "Epoch 3 Batch 16800 Loss 1.5985,time:1721.3588309288025sec\n",
      "Epoch 3 Batch 16900 Loss 1.5380,time:1731.9851052761078sec\n",
      "Epoch 3 Batch 17000 Loss 1.3064,time:1742.4384593963623sec\n",
      "Epoch 3 Batch 17100 Loss 0.7392,time:1752.6977694034576sec\n",
      "Epoch 3 Batch 17200 Loss 1.2256,time:1762.9660832881927sec\n",
      "Epoch 3 Batch 17300 Loss 0.9698,time:1773.1335945129395sec\n",
      "Epoch 3 Batch 17400 Loss 1.0974,time:1783.3294286727905sec\n",
      "Epoch 3 Batch 17500 Loss 0.7043,time:1793.5377271175385sec\n",
      "Epoch 3 Batch 17600 Loss 1.5574,time:1803.7300236225128sec\n",
      "Epoch 3 Batch 17700 Loss 1.3681,time:1813.9653277397156sec\n",
      "Epoch 3 Batch 17800 Loss 0.8888,time:1824.0985741615295sec\n",
      "Epoch 3 Batch 17900 Loss 1.2781,time:1834.2174775600433sec\n",
      "Epoch 3 Batch 18000 Loss 1.0132,time:1844.480788230896sec\n",
      "Epoch 3 Batch 18100 Loss 1.3567,time:1854.9881546497345sec\n",
      "Epoch 3 Batch 18200 Loss 0.6486,time:1865.2474648952484sec\n",
      "Epoch 3 Batch 18300 Loss 0.7669,time:1875.4916183948517sec\n",
      "Epoch 3 Batch 18400 Loss 1.3586,time:1885.8437194824219sec\n",
      "Epoch 3 Batch 18500 Loss 1.1432,time:1895.9609978199005sec\n",
      "Epoch 3 Batch 18600 Loss 1.2618,time:1906.1512928009033sec\n",
      "Epoch 3 Batch 18700 Loss 0.9768,time:1916.4816193580627sec\n",
      "Epoch 3 Batch 18800 Loss 1.2995,time:1926.8540380001068sec\n",
      "Epoch 3 Batch 18900 Loss 1.4799,time:1937.076931476593sec\n",
      "Epoch 3 Batch 19000 Loss 1.4545,time:1947.2832298278809sec\n",
      "Epoch 3 Batch 19100 Loss 0.9347,time:1957.3655004501343sec\n",
      "Epoch 3 Batch 19200 Loss 1.1724,time:1967.4537727832794sec\n",
      "Epoch 3 Batch 19300 Loss 1.6351,time:1977.625449180603sec\n",
      "Epoch 3 Batch 19400 Loss 1.6090,time:1987.9749031066895sec\n",
      "Epoch 3 Batch 19500 Loss 1.3392,time:1998.2114307880402sec\n",
      "Epoch 3 Batch 19600 Loss 1.2849,time:2008.5077497959137sec\n",
      "Epoch 3 Batch 19700 Loss 1.1333,time:2018.7460553646088sec\n",
      "Epoch 3 Batch 19800 Loss 0.7895,time:2028.97735953331sec\n",
      "Epoch 3 Batch 19900 Loss 1.7814,time:2039.542706489563sec\n",
      "Epoch 3 Batch 20000 Loss 1.3935,time:2050.0131101608276sec\n",
      "Epoch 3 Batch 20100 Loss 1.0449,time:2060.3834459781647sec\n",
      "Epoch 3 Batch 20200 Loss 1.1599,time:2070.6027467250824sec\n",
      "Epoch 3 Batch 20300 Loss 1.5017,time:2080.7760384082794sec\n",
      "Epoch 3 Batch 20400 Loss 0.9950,time:2090.8597514629364sec\n",
      "Epoch 3 Batch 20500 Loss 1.3405,time:2100.9776389598846sec\n",
      "Epoch 3 Batch 20600 Loss 1.1680,time:2111.2139439582825sec\n",
      "Epoch 3 Batch 20700 Loss 1.3008,time:2121.589280605316sec\n",
      "Epoch 3 Batch 20800 Loss 1.2099,time:2131.8395895957947sec\n",
      "Epoch 3 Batch 20900 Loss 1.2322,time:2141.9675059318542sec\n",
      "Epoch 3 Batch 21000 Loss 1.2323,time:2152.317271232605sec\n",
      "Epoch 3 Batch 21100 Loss 1.1260,time:2162.59658575058sec\n",
      "Epoch 3 Batch 21200 Loss 1.2574,time:2172.77787899971sec\n",
      "Epoch 3 Batch 21300 Loss 1.1086,time:2183.092808008194sec\n",
      "Epoch 3 Batch 21400 Loss 1.5434,time:2193.2436265945435sec\n",
      "Epoch 3 Batch 21500 Loss 1.3858,time:2203.407915353775sec\n",
      "Epoch 3 Batch 21600 Loss 1.4197,time:2213.6122131347656sec\n",
      "Epoch 3 Batch 21700 Loss 1.3565,time:2223.8685228824615sec\n",
      "Epoch 3 Batch 21800 Loss 1.3718,time:2234.1685354709625sec\n",
      "Epoch 3 Batch 21900 Loss 1.2560,time:2244.373039484024sec\n",
      "Epoch 3 Batch 22000 Loss 0.9741,time:2254.6883630752563sec\n",
      "Epoch 3 Batch 22100 Loss 1.2627,time:2265.061698913574sec\n",
      "Epoch 3 Batch 22200 Loss 1.3304,time:2275.236989736557sec\n",
      "Epoch 3 Batch 22300 Loss 1.5103,time:2285.4429659843445sec\n",
      "Epoch 3 Batch 22400 Loss 1.2063,time:2295.7201023101807sec\n",
      "Epoch 3 Batch 22500 Loss 1.5485,time:2306.010420322418sec\n",
      "Epoch 3 Batch 22600 Loss 0.8440,time:2316.2797334194183sec\n",
      "Epoch 3 Batch 22700 Loss 1.1665,time:2326.5030353069305sec\n",
      "Epoch 3 Batch 22800 Loss 1.5144,time:2336.702974796295sec\n",
      "Epoch 3 Batch 22900 Loss 0.9840,time:2346.8634169101715sec\n",
      "Epoch 3 Batch 23000 Loss 1.0499,time:2357.096720933914sec\n",
      "Epoch 3 Batch 23100 Loss 1.3048,time:2367.33402633667sec\n",
      "Epoch 3 Batch 23200 Loss 1.4573,time:2377.455361366272sec\n",
      "Epoch 3 Batch 23300 Loss 1.1157,time:2387.612267971039sec\n",
      "Epoch 3 Batch 23400 Loss 1.4737,time:2397.9395945072174sec\n",
      "Epoch 3 Batch 23500 Loss 1.8377,time:2408.162895679474sec\n",
      "Epoch 3 Batch 23600 Loss 1.1425,time:2418.35018992424sec\n",
      "Epoch 3 Batch 23700 Loss 1.1899,time:2428.570004463196sec\n",
      "Epoch 3 Batch 23800 Loss 1.0446,time:2438.7918379306793sec\n",
      "Epoch 3 Batch 23900 Loss 1.3167,time:2449.138168334961sec\n",
      "Epoch 3 Batch 24000 Loss 1.3367,time:2459.354469060898sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 24100 Loss 1.2104,time:2469.5547654628754sec\n",
      "Epoch 3 Batch 24200 Loss 1.7197,time:2479.691095352173sec\n",
      "Epoch 3 Batch 24300 Loss 0.8252,time:2489.824210882187sec\n",
      "Epoch 3 Batch 24400 Loss 1.3773,time:2499.9784977436066sec\n",
      "Epoch 3 Batch 24500 Loss 1.2175,time:2510.2408089637756sec\n",
      "Epoch 3 Batch 24600 Loss 0.8681,time:2520.4581096172333sec\n",
      "Epoch 3 Batch 24700 Loss 1.7027,time:2530.5899844169617sec\n",
      "Epoch 3 Batch 24800 Loss 1.3688,time:2540.879753112793sec\n",
      "Epoch 3 Batch 24900 Loss 0.7831,time:2551.1840739250183sec\n",
      "Epoch 3 Batch 25000 Loss 1.3040,time:2561.428381204605sec\n",
      "Epoch 3 Batch 25100 Loss 1.2331,time:2571.794715642929sec\n",
      "Epoch 3 Batch 25200 Loss 1.0352,time:2582.174334526062sec\n",
      "Epoch 3 Batch 25300 Loss 1.5851,time:2592.3570437431335sec\n",
      "Epoch 3 Batch 25400 Loss 1.3154,time:2602.517332315445sec\n",
      "Epoch 3 Batch 25500 Loss 1.1897,time:2613.0206978321075sec\n",
      "Epoch 3 Batch 25600 Loss 1.3615,time:2623.396034002304sec\n",
      "Epoch 3 Batch 25700 Loss 1.2516,time:2633.8108036518097sec\n",
      "Epoch 3 Batch 25800 Loss 1.1755,time:2644.249537229538sec\n",
      "Epoch 3 Batch 25900 Loss 1.1691,time:2654.4468336105347sec\n",
      "Epoch 3 Batch 26000 Loss 1.3400,time:2664.6771376132965sec\n",
      "Epoch 3 Batch 26100 Loss 1.8425,time:2674.84694647789sec\n",
      "Epoch 3 Batch 26200 Loss 1.4961,time:2684.8920719623566sec\n",
      "Epoch 3 Batch 26300 Loss 1.1756,time:2695.022474527359sec\n",
      "Epoch 3 Batch 26400 Loss 1.0340,time:2705.1977667808533sec\n",
      "Epoch 3 Batch 26500 Loss 1.4405,time:2715.351052761078sec\n",
      "Epoch 3 Batch 26600 Loss 1.5001,time:2725.860419511795sec\n",
      "Epoch 3 Batch 26700 Loss 1.5284,time:2736.094923734665sec\n",
      "Epoch 3 Batch 26800 Loss 0.9349,time:2746.3292286396027sec\n",
      "Epoch 3 Batch 26900 Loss 1.4797,time:2756.923614501953sec\n",
      "Epoch 3 Batch 27000 Loss 1.4595,time:2767.36296582222sec\n",
      "Epoch 3 Batch 27100 Loss 0.9849,time:2777.7147257328033sec\n",
      "Epoch 3 Batch 27200 Loss 1.2393,time:2788.218930244446sec\n",
      "Epoch 3 Batch 27300 Loss 0.8876,time:2798.585483312607sec\n",
      "Epoch 3 Batch 27400 Loss 1.1533,time:2808.9188103675842sec\n",
      "Epoch 3 Batch 27500 Loss 1.5280,time:2819.2721421718597sec\n",
      "Epoch 3 Batch 27600 Loss 1.7184,time:2829.614979028702sec\n",
      "Epoch 3 Batch 27700 Loss 1.5791,time:2839.8840255737305sec\n",
      "Epoch 3 Batch 27800 Loss 1.5721,time:2850.022210121155sec\n",
      "Epoch 3 Batch 27900 Loss 1.3694,time:2860.3595383167267sec\n",
      "Epoch 3 Batch 28000 Loss 1.2867,time:2870.876907348633sec\n",
      "Epoch 3 Batch 28100 Loss 1.5667,time:2881.212234020233sec\n",
      "Epoch 3 Batch 28200 Loss 0.5388,time:2891.475795984268sec\n",
      "Epoch 3 Batch 28300 Loss 0.9539,time:2901.68318939209sec\n",
      "Epoch 3 Batch 28400 Loss 0.9301,time:2911.9565029144287sec\n",
      "Epoch 3 Batch 28500 Loss 1.3635,time:2922.164801597595sec\n",
      "Epoch 3 Batch 28600 Loss 1.2476,time:2932.466121673584sec\n",
      "Epoch 3 Batch 28700 Loss 0.6367,time:2942.7498009204865sec\n",
      "Epoch 3 Batch 28800 Loss 1.3000,time:2952.9743206501007sec\n",
      "Epoch 3 Batch 28900 Loss 1.4679,time:2963.2356317043304sec\n",
      "Epoch 3 Batch 29000 Loss 1.2076,time:2973.4699363708496sec\n",
      "Epoch 3 Batch 29100 Loss 1.3293,time:2983.615221261978sec\n",
      "Epoch 3 Batch 29200 Loss 1.7994,time:2993.9476838111877sec\n",
      "Epoch 3 Batch 29300 Loss 1.1150,time:3004.19890999794sec\n",
      "Epoch 3 Batch 29400 Loss 1.5270,time:3014.405208349228sec\n",
      "Epoch 3 Batch 29500 Loss 0.9490,time:3024.652515411377sec\n",
      "Epoch 3 Batch 29600 Loss 1.4646,time:3035.0828642845154sec\n",
      "Epoch 3 Batch 29700 Loss 1.0342,time:3045.4316625595093sec\n",
      "Epoch 3 Batch 29800 Loss 1.2174,time:3055.8435740470886sec\n",
      "Epoch 3 Batch 29900 Loss 1.8047,time:3066.1728999614716sec\n",
      "Epoch 3 Batch 30000 Loss 0.8962,time:3076.4282093048096sec\n",
      "Epoch 3 Batch 30100 Loss 0.9471,time:3086.574495077133sec\n",
      "Epoch 3 Batch 30200 Loss 1.1750,time:3096.730734348297sec\n",
      "Epoch 3 Batch 30300 Loss 1.3389,time:3106.9909839630127sec\n",
      "Epoch 3 Batch 30400 Loss 1.3951,time:3117.3083066940308sec\n",
      "Epoch 3 Batch 30500 Loss 1.6073,time:3127.530608892441sec\n",
      "Epoch 3 Batch 30600 Loss 1.0895,time:3137.779420852661sec\n",
      "Epoch 3 Batch 30700 Loss 1.3700,time:3147.8536562919617sec\n",
      "Epoch 3 Batch 30800 Loss 1.0147,time:3157.948894739151sec\n",
      "Epoch 3 Batch 30900 Loss 1.0977,time:3168.0371668338776sec\n",
      "Epoch 3 Batch 31000 Loss 1.9332,time:3178.3854970932007sec\n",
      "Epoch 3 Batch 31100 Loss 1.0166,time:3188.611799955368sec\n",
      "Epoch 3 Batch 31200 Loss 2.0140,time:3198.7500863075256sec\n",
      "Epoch 3 Batch 31300 Loss 1.1905,time:3208.9132540225983sec\n",
      "Epoch 3 Batch 31400 Loss 1.5767,time:3219.339602470398sec\n",
      "Epoch 3 Batch 31500 Loss 1.2265,time:3229.781954050064sec\n",
      "Epoch 3 Batch 31600 Loss 0.6647,time:3240.202300310135sec\n",
      "Epoch 3 Batch 31700 Loss 1.0578,time:3250.609270334244sec\n",
      "Epoch 3 Batch 31800 Loss 1.0207,time:3261.072177171707sec\n",
      "Epoch 3 Batch 31900 Loss 0.9838,time:3271.4285082817078sec\n",
      "Epoch 3 Batch 32000 Loss 1.3639,time:3281.733829975128sec\n",
      "Epoch 3 Batch 32100 Loss 1.2857,time:3292.0061433315277sec\n",
      "Epoch 3 Batch 32200 Loss 1.2145,time:3302.345824241638sec\n",
      "Epoch 3 Batch 32300 Loss 1.5257,time:3312.73792219162sec\n",
      "Epoch 3 Batch 32400 Loss 0.5015,time:3323.007234811783sec\n",
      "Epoch 3 Batch 32500 Loss 1.4026,time:3333.248541355133sec\n",
      "Epoch 3 Batch 32600 Loss 1.4969,time:3343.489848136902sec\n",
      "Epoch 3 Batch 32700 Loss 1.1504,time:3353.58833694458sec\n",
      "Epoch 3 Batch 32800 Loss 0.9132,time:3363.675961256027sec\n",
      "Epoch 3 Batch 32900 Loss 0.9534,time:3373.831248998642sec\n",
      "Epoch 3 Batch 33000 Loss 0.8980,time:3384.0045399665833sec\n",
      "Epoch 3 Batch 33100 Loss 0.7984,time:3394.2268419265747sec\n",
      "Epoch 3 Batch 33200 Loss 1.4905,time:3404.3180668354034sec\n",
      "Epoch 3 Batch 33300 Loss 1.3070,time:3414.428926229477sec\n",
      "Epoch 3 Batch 33400 Loss 1.5618,time:3424.7622532844543sec\n",
      "Epoch 3 Batch 33500 Loss 0.9807,time:3435.148592710495sec\n",
      "Epoch 3 Batch 33600 Loss 0.9773,time:3445.490921974182sec\n",
      "Epoch 3 Batch 33700 Loss 1.7682,time:3455.6516013145447sec\n",
      "Epoch 3 Batch 33800 Loss 1.5291,time:3465.8015151023865sec\n",
      "Epoch 3 Batch 33900 Loss 1.0403,time:3475.914792776108sec\n",
      "Epoch 3 Batch 34000 Loss 0.9144,time:3486.0790815353394sec\n",
      "Epoch 3 Batch 34100 Loss 1.3429,time:3496.251371860504sec\n",
      "Epoch 3 Batch 34200 Loss 1.2597,time:3506.40483045578sec\n",
      "Epoch 3 Batch 34300 Loss 1.5813,time:3516.7735953330994sec\n",
      "Epoch 3 Batch 34400 Loss 1.3340,time:3527.1739377975464sec\n",
      "Epoch 3 Batch 34500 Loss 0.8058,time:3537.4292471408844sec\n",
      "Epoch 3 Batch 34600 Loss 1.6472,time:3547.888602256775sec\n",
      "Epoch 3 Batch 34700 Loss 0.7680,time:3558.51433467865sec\n",
      "Epoch 3 Batch 34800 Loss 1.4390,time:3568.922967195511sec\n",
      "Epoch 3 Batch 34900 Loss 0.8102,time:3579.3673207759857sec\n",
      "Epoch 3 Batch 35000 Loss 1.1243,time:3589.6326315402985sec\n",
      "Epoch 3 Batch 35100 Loss 1.1005,time:3600.548152923584sec\n",
      "Epoch 3 Batch 35200 Loss 1.4388,time:3610.7945396900177sec\n",
      "Epoch 3 Batch 35300 Loss 1.2745,time:3620.99383687973sec\n",
      "Epoch 3 Batch 35400 Loss 0.9269,time:3631.16712808609sec\n",
      "Epoch 3 Batch 35500 Loss 1.0361,time:3641.3664247989655sec\n",
      "Epoch 3 Batch 35600 Loss 1.4260,time:3651.5640654563904sec\n",
      "Epoch 3 Batch 35700 Loss 1.2243,time:3662.037959098816sec\n",
      "Epoch 3 Batch 35800 Loss 1.4727,time:3672.29926943779sec\n",
      "Epoch 3 Batch 35900 Loss 1.0699,time:3682.475561141968sec\n",
      "Epoch 3 Batch 36000 Loss 1.2653,time:3692.859899997711sec\n",
      "Epoch 3 Batch 36100 Loss 1.4780,time:3703.2208585739136sec\n",
      "Epoch 3 Batch 36200 Loss 1.6821,time:3713.4626290798187sec\n",
      "Epoch 3 Batch 36300 Loss 1.3903,time:3723.69593334198sec\n",
      "Epoch 3 Batch 36400 Loss 0.9093,time:3733.9822499752045sec\n",
      "Epoch 3 Batch 36500 Loss 1.0880,time:3744.0825242996216sec\n",
      "Epoch 3 Batch 36600 Loss 1.3155,time:3754.2128059864044sec\n",
      "Epoch 3 Batch 36700 Loss 1.3680,time:3764.311122894287sec\n",
      "Epoch 3 Batch 36800 Loss 1.1479,time:3774.701733827591sec\n",
      "Epoch 3 Batch 36900 Loss 1.0839,time:3784.9130342006683sec\n",
      "Epoch 3 Batch 37000 Loss 1.1558,time:3795.2263565063477sec\n",
      "Epoch 3 Batch 37100 Loss 1.3339,time:3805.493668794632sec\n",
      "Epoch 3 Batch 37200 Loss 1.4070,time:3815.803159236908sec\n",
      "Epoch 3 Batch 37300 Loss 1.4893,time:3826.1052107810974sec\n",
      "Epoch 3 Batch 37400 Loss 1.6684,time:3836.2434933185577sec\n",
      "Epoch 3 Batch 37500 Loss 0.8620,time:3846.4010713100433sec\n",
      "Epoch 3 Batch 37600 Loss 1.4067,time:3856.754137277603sec\n",
      "Epoch 3 Batch 37700 Loss 1.4463,time:3867.230497121811sec\n",
      "Epoch 3 Batch 37800 Loss 1.3944,time:3877.5448195934296sec\n",
      "Epoch 3 Batch 37900 Loss 1.6358,time:3887.782125234604sec\n",
      "Epoch 3 Batch 38000 Loss 1.2277,time:3897.997692346573sec\n",
      "Epoch 3 Batch 38100 Loss 0.8918,time:3908.059954404831sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 38200 Loss 1.0728,time:3918.3282663822174sec\n",
      "Epoch 3 Batch 38300 Loss 1.4685,time:3928.542567014694sec\n",
      "Epoch 3 Batch 38400 Loss 1.3726,time:3938.790382862091sec\n",
      "Epoch 3 Batch 38500 Loss 1.4903,time:3948.9509863853455sec\n",
      "Epoch 3 Batch 38600 Loss 1.8621,time:3959.0353157520294sec\n",
      "Epoch 3 Batch 38700 Loss 1.2768,time:3969.318604707718sec\n",
      "Epoch 3 Batch 38800 Loss 1.8210,time:3979.4798934459686sec\n",
      "Epoch 3 Batch 38900 Loss 1.4987,time:3989.615175962448sec\n",
      "Epoch 3 Batch 39000 Loss 1.2265,time:3999.88671541214sec\n",
      "Epoch 3 Batch 39100 Loss 1.4657,time:4010.1470527648926sec\n",
      "Epoch 3 Batch 39200 Loss 1.5709,time:4020.2703325748444sec\n",
      "Epoch 3 Batch 39300 Loss 1.2886,time:4030.4896337985992sec\n",
      "Epoch 3 Batch 39400 Loss 1.1543,time:4040.665925502777sec\n",
      "Epoch 3 Batch 39500 Loss 1.1458,time:4051.0064845085144sec\n",
      "Epoch 3 Batch 39600 Loss 1.1778,time:4061.3244230747223sec\n",
      "Epoch 3 Batch 39700 Loss 1.4004,time:4071.630744457245sec\n",
      "Epoch 3 Batch 39800 Loss 1.4410,time:4082.189121723175sec\n",
      "Epoch 3 Batch 39900 Loss 1.4154,time:4092.366414308548sec\n",
      "Epoch 3 Batch 40000 Loss 1.2881,time:4102.634994983673sec\n",
      "Epoch 3 Batch 40100 Loss 1.4634,time:4112.870738506317sec\n",
      "Epoch 3 Batch 40200 Loss 1.1670,time:4123.105043172836sec\n",
      "Epoch 3 Batch 40300 Loss 1.6474,time:4133.402362346649sec\n",
      "Epoch 3 Batch 40400 Loss 0.5715,time:4143.515639781952sec\n",
      "Epoch 3 Batch 40500 Loss 1.3517,time:4153.715650558472sec\n",
      "Epoch 3 Batch 40600 Loss 1.5158,time:4163.990967988968sec\n",
      "Epoch 3 Batch 40700 Loss 1.6844,time:4174.15025639534sec\n",
      "Epoch 3 Batch 40800 Loss 1.7703,time:4184.3555545806885sec\n",
      "Epoch 3 Batch 40900 Loss 1.4858,time:4194.631868839264sec\n",
      "Epoch 3 Batch 41000 Loss 1.2120,time:4204.9161767959595sec\n",
      "Epoch 3 Batch 41100 Loss 0.8088,time:4215.325521469116sec\n",
      "Epoch 3 Batch 41200 Loss 1.7807,time:4225.705860137939sec\n",
      "Epoch 3 Batch 41300 Loss 1.3219,time:4235.871148824692sec\n",
      "Epoch 3 Batch 41400 Loss 1.3764,time:4245.943972826004sec\n",
      "Epoch 3 Batch 41500 Loss 1.6478,time:4256.076843738556sec\n",
      "Epoch 3 Batch 41600 Loss 1.4537,time:4266.429174661636sec\n",
      "Epoch 3 Batch 41700 Loss 1.5232,time:4276.724493741989sec\n",
      "Epoch 3 Batch 41800 Loss 1.6234,time:4286.867777585983sec\n",
      "Epoch 3 Batch 41900 Loss 2.0526,time:4297.253233909607sec\n",
      "Epoch 3 Batch 42000 Loss 1.2813,time:4307.523939371109sec\n",
      "Epoch 3 Batch 42100 Loss 1.0314,time:4317.837261915207sec\n",
      "Epoch 3 Batch 42200 Loss 1.4506,time:4328.164587497711sec\n",
      "Epoch 3 Batch 42300 Loss 1.0209,time:4338.349881410599sec\n",
      "Epoch 3 Batch 42400 Loss 1.6024,time:4348.693526029587sec\n",
      "Epoch 3 Batch 42500 Loss 0.9501,time:4358.879844665527sec\n",
      "Epoch 3 Batch 42600 Loss 1.2728,time:4369.084142923355sec\n",
      "Epoch 3 Batch 42700 Loss 1.0643,time:4379.407467365265sec\n",
      "Epoch 3 Batch 42800 Loss 1.1844,time:4389.632770061493sec\n",
      "Epoch 3 Batch 42900 Loss 1.1613,time:4399.728707790375sec\n",
      "Epoch 3 Batch 43000 Loss 1.6453,time:4409.771636724472sec\n",
      "Epoch 3 Batch 43100 Loss 1.2482,time:4420.0552110672sec\n",
      "Epoch 3 Batch 43200 Loss 1.7111,time:4430.3325254917145sec\n",
      "Epoch 3 Batch 43300 Loss 0.8057,time:4440.6738538742065sec\n",
      "Epoch 3 Batch 43400 Loss 1.2607,time:4450.845144987106sec\n",
      "Epoch 3 Batch 43500 Loss 1.3090,time:4461.0399849414825sec\n",
      "Epoch 3 Batch 43600 Loss 1.2399,time:4471.272071361542sec\n",
      "Epoch 3 Batch 43700 Loss 1.4448,time:4481.554386615753sec\n",
      "Epoch 3 Batch 43800 Loss 0.9715,time:4491.774688720703sec\n",
      "Epoch 3 Batch 43900 Loss 1.5324,time:4502.110016345978sec\n",
      "Epoch 3 Batch 44000 Loss 1.2680,time:4512.276255130768sec\n",
      "Epoch 3 Batch 44100 Loss 1.2916,time:4522.459268569946sec\n",
      "Epoch 3 Batch 44200 Loss 1.0643,time:4532.644561290741sec\n",
      "Epoch 3 Batch 44300 Loss 1.4956,time:4542.83885717392sec\n",
      "Epoch 3 Batch 44400 Loss 1.1891,time:4553.091166257858sec\n",
      "Epoch 3 Batch 44500 Loss 1.7803,time:4563.371006965637sec\n",
      "Epoch 3 Batch 44600 Loss 1.5328,time:4573.776649475098sec\n",
      "Epoch 3 Batch 44700 Loss 1.3441,time:4584.3740355968475sec\n",
      "Epoch 3 Batch 44800 Loss 1.2953,time:4594.545326471329sec\n",
      "Epoch 3 Batch 44900 Loss 0.9799,time:4604.716617107391sec\n",
      "Epoch 3 Batch 45000 Loss 1.3363,time:4615.053628921509sec\n",
      "Epoch 3 Batch 45100 Loss 1.8710,time:4625.545315027237sec\n",
      "Epoch 3 Batch 45200 Loss 1.6113,time:4635.832631826401sec\n",
      "Epoch 3 Batch 45300 Loss 0.7924,time:4645.9279062747955sec\n",
      "Epoch 3 Batch 45400 Loss 0.8322,time:4656.127202272415sec\n",
      "Epoch 3 Batch 45500 Loss 1.2154,time:4666.404039382935sec\n",
      "Epoch 3 Batch 45600 Loss 1.5442,time:4676.593155860901sec\n",
      "Epoch 3 Batch 45700 Loss 1.1678,time:4686.7374403476715sec\n",
      "Epoch 3 Batch 45800 Loss 1.8191,time:4696.874723672867sec\n",
      "Epoch 3 Batch 45900 Loss 1.1835,time:4707.025009393692sec\n",
      "Epoch 3 Batch 46000 Loss 1.6249,time:4717.247684240341sec\n",
      "Epoch 3 Batch 46100 Loss 1.1670,time:4727.534660100937sec\n",
      "Epoch 3 Batch 46200 Loss 1.2062,time:4737.956006526947sec\n",
      "Epoch 3 Batch 46300 Loss 1.4593,time:4748.4563710689545sec\n",
      "Epoch 3 Batch 46400 Loss 1.3282,time:4758.679673433304sec\n",
      "Epoch 3 Batch 46500 Loss 0.9454,time:4769.040270805359sec\n",
      "Epoch 3 Batch 46600 Loss 1.5898,time:4779.355534791946sec\n",
      "Epoch 3 Batch 46700 Loss 1.0097,time:4789.609843492508sec\n",
      "Epoch 3 Batch 46800 Loss 1.3571,time:4800.055195808411sec\n",
      "Epoch 3 Batch 46900 Loss 1.3251,time:4810.5005486011505sec\n",
      "Epoch 3 Batch 47000 Loss 1.3301,time:4820.851971626282sec\n",
      "Epoch 3 Batch 47100 Loss 1.1144,time:4831.084831476212sec\n",
      "Epoch 3 Batch 47200 Loss 1.9153,time:4841.300132513046sec\n",
      "Epoch 3 Batch 47300 Loss 1.4344,time:4851.459420204163sec\n",
      "Epoch 3 Batch 47400 Loss 1.1623,time:4861.589701652527sec\n",
      "Epoch 3 Batch 47500 Loss 2.0434,time:4871.868662595749sec\n",
      "Epoch 3 Batch 47600 Loss 1.1296,time:4881.956819534302sec\n",
      "Epoch 3 Batch 47700 Loss 1.3572,time:4892.172119379044sec\n",
      "Epoch 3 Batch 47800 Loss 1.2141,time:4902.433430433273sec\n",
      "Epoch 3 Batch 47900 Loss 1.3440,time:4912.550708770752sec\n",
      "Epoch 3 Batch 48000 Loss 1.3510,time:4922.750254392624sec\n",
      "Epoch 3 Batch 48100 Loss 1.1774,time:4933.456004619598sec\n",
      "Epoch 3 Batch 48200 Loss 1.6668,time:4943.7466542720795sec\n",
      "Epoch 3 Batch 48300 Loss 1.3709,time:4953.994962453842sec\n",
      "Epoch 3 Batch 48400 Loss 0.9368,time:4964.31928730011sec\n",
      "Epoch 3 Batch 48500 Loss 1.6111,time:4974.507581472397sec\n",
      "Epoch 3 Batch 48600 Loss 1.1690,time:4984.601355314255sec\n",
      "Epoch 3 Batch 48700 Loss 1.0971,time:4994.770560979843sec\n",
      "Epoch 3 Batch 48800 Loss 1.2189,time:5004.871834278107sec\n",
      "Epoch 3 Batch 48900 Loss 1.2140,time:5014.999114990234sec\n",
      "Epoch 3 Batch 49000 Loss 0.9336,time:5025.334442138672sec\n",
      "Epoch 3 Batch 49100 Loss 1.2070,time:5035.688054800034sec\n",
      "Epoch 3 Batch 49200 Loss 0.9640,time:5045.954516172409sec\n",
      "Epoch 3 Batch 49300 Loss 1.1430,time:5056.2978456020355sec\n",
      "Epoch 3 Batch 49400 Loss 1.1081,time:5066.657178163528sec\n",
      "Epoch 3 Batch 49500 Loss 0.9361,time:5077.002508878708sec\n",
      "Epoch 3 Batch 49600 Loss 1.0969,time:5087.163300991058sec\n",
      "Epoch 3 Batch 49700 Loss 1.4824,time:5097.407502412796sec\n",
      "Epoch 3 Batch 49800 Loss 1.4920,time:5107.60685133934sec\n",
      "Epoch 3 Batch 49900 Loss 0.9963,time:5117.826153039932sec\n",
      "Epoch 3 Batch 50000 Loss 1.1336,time:5128.05345582962sec\n",
      "Epoch 3 Batch 50100 Loss 1.0888,time:5138.189738512039sec\n",
      "Epoch 3 Batch 50200 Loss 1.7018,time:5148.349275827408sec\n",
      "Epoch 3 Batch 50300 Loss 1.1916,time:5158.380584001541sec\n",
      "Epoch 3 Batch 50400 Loss 1.0892,time:5168.468854427338sec\n",
      "Epoch 3 Batch 50500 Loss 1.3973,time:5178.841190814972sec\n",
      "Epoch 3 Batch 50600 Loss 1.4901,time:5189.167516469955sec\n",
      "Epoch 3 Batch 50700 Loss 0.7976,time:5199.350087404251sec\n",
      "Epoch 3 Batch 50800 Loss 1.8965,time:5209.689547300339sec\n",
      "Epoch 3 Batch 50900 Loss 1.2019,time:5219.875841379166sec\n",
      "Epoch 3 Batch 51000 Loss 1.4568,time:5229.984117507935sec\n",
      "Epoch 3 Batch 51100 Loss 1.0172,time:5240.100395679474sec\n",
      "Epoch 3 Batch 51200 Loss 0.5299,time:5250.226720571518sec\n",
      "Epoch 3 Batch 51300 Loss 1.5935,time:5260.423770189285sec\n",
      "Epoch 3 Batch 51400 Loss 1.2033,time:5270.841313123703sec\n",
      "Epoch 3 Batch 51500 Loss 0.9201,time:5280.948588848114sec\n",
      "Epoch 3 Batch 51600 Loss 1.6589,time:5291.1538870334625sec\n",
      "Epoch 3 Batch 51700 Loss 1.3615,time:5301.564736127853sec\n",
      "Epoch 3 Batch 51800 Loss 1.4371,time:5311.834579944611sec\n",
      "Epoch 3 Batch 51900 Loss 0.9994,time:5322.151850938797sec\n",
      "Epoch 3 Batch 52000 Loss 1.0278,time:5332.510456323624sec\n",
      "Epoch 3 Batch 52100 Loss 1.3352,time:5342.738763332367sec\n",
      "Epoch 3 Batch 52200 Loss 1.2060,time:5353.167111873627sec\n",
      "Epoch 3 Batch 52300 Loss 1.0929,time:5363.434423685074sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 52400 Loss 1.0086,time:5373.657726764679sec\n",
      "Epoch 3 Batch 52500 Loss 1.2159,time:5384.179335594177sec\n",
      "Epoch 3 Batch 52600 Loss 0.9249,time:5394.429644107819sec\n",
      "Epoch 3 Batch 52700 Loss 1.4208,time:5404.516915798187sec\n",
      "Epoch 3 Batch 52800 Loss 1.1589,time:5414.6261920928955sec\n",
      "Epoch 3 Batch 52900 Loss 1.1603,time:5424.6917181015015sec\n",
      "Epoch 3 Batch 53000 Loss 1.2456,time:5435.0509078502655sec\n",
      "Epoch 3 Batch 53100 Loss 1.0326,time:5445.364456176758sec\n",
      "Epoch 3 Batch 53200 Loss 1.8094,time:5455.644771337509sec\n",
      "Epoch 3 Batch 53300 Loss 0.8508,time:5466.0191078186035sec\n",
      "Epoch 3 Batch 53400 Loss 1.0320,time:5476.334940671921sec\n",
      "Epoch 3 Batch 53500 Loss 1.0542,time:5486.671956777573sec\n",
      "Epoch 3 Batch 53600 Loss 1.2998,time:5497.151316642761sec\n",
      "Epoch 3 Batch 53700 Loss 0.8436,time:5507.478642463684sec\n",
      "Epoch 3 Batch 53800 Loss 1.3535,time:5517.830973625183sec\n",
      "Epoch 3 Batch 53900 Loss 1.6060,time:5528.157583236694sec\n",
      "Epoch 3 Batch 54000 Loss 1.3375,time:5538.692459821701sec\n",
      "Epoch 3 Batch 54100 Loss 1.2652,time:5549.076798915863sec\n",
      "Epoch 3 Batch 54200 Loss 1.5898,time:5559.355112791061sec\n",
      "Epoch 3 Batch 54300 Loss 1.2562,time:5569.608421564102sec\n",
      "Epoch 3 Batch 54400 Loss 1.1090,time:5579.840080738068sec\n",
      "Epoch 3 Batch 54500 Loss 1.3375,time:5590.20152592659sec\n",
      "Epoch 3 Batch 54600 Loss 1.4861,time:5600.622872829437sec\n",
      "Epoch 3 Batch 54700 Loss 1.5072,time:5610.919191360474sec\n",
      "Epoch 3 Batch 54800 Loss 1.1256,time:5621.270522594452sec\n",
      "Epoch 3 Batch 54900 Loss 1.5408,time:5631.533015966415sec\n",
      "Epoch 3 Batch 55000 Loss 1.2289,time:5642.125010967255sec\n",
      "Epoch 3 Batch 55100 Loss 1.2668,time:5652.476262331009sec\n",
      "Epoch 3 Batch 55200 Loss 1.4582,time:5662.6615562438965sec\n",
      "Epoch 3 Batch 55300 Loss 0.8989,time:5672.876856803894sec\n",
      "Epoch 3 Batch 55400 Loss 1.2454,time:5683.136168003082sec\n",
      "Epoch 3 Batch 55500 Loss 1.4541,time:5693.329598426819sec\n",
      "Epoch 3 Batch 55600 Loss 1.3938,time:5703.598232746124sec\n",
      "Epoch 3 Batch 55700 Loss 1.4976,time:5713.836538553238sec\n",
      "Epoch 3 Batch 55800 Loss 1.3024,time:5724.074844360352sec\n",
      "Epoch 3 Batch 55900 Loss 1.7235,time:5734.255136489868sec\n",
      "Epoch 3 Batch 56000 Loss 1.2964,time:5744.490718364716sec\n",
      "Epoch 3 Batch 56100 Loss 0.7320,time:5754.811879396439sec\n",
      "Epoch 3 Batch 56200 Loss 1.2373,time:5765.008175373077sec\n",
      "Epoch 3 Batch 56300 Loss 1.2697,time:5775.1724643707275sec\n",
      "Epoch 3 Batch 56400 Loss 0.8907,time:5785.553802251816sec\n",
      "Epoch 3 Batch 56500 Loss 1.2202,time:5795.792681932449sec\n",
      "Epoch 3 Batch 56600 Loss 1.3453,time:5806.055833101273sec\n",
      "Epoch 3 Batch 56700 Loss 1.1502,time:5816.275134563446sec\n",
      "Epoch 3 Batch 56800 Loss 1.5873,time:5826.526443004608sec\n",
      "Epoch 3 Batch 56900 Loss 1.8984,time:5836.799756765366sec\n",
      "Epoch 3 Batch 57000 Loss 1.0054,time:5847.013414621353sec\n",
      "Epoch 3 Batch 57100 Loss 0.8993,time:5857.32568526268sec\n",
      "Epoch 3 Batch 57200 Loss 1.8620,time:5867.492974996567sec\n",
      "Epoch 3 Batch 57300 Loss 1.3514,time:5877.657264709473sec\n",
      "Epoch 3 Batch 57400 Loss 1.3347,time:5887.900570869446sec\n",
      "Epoch 3 Batch 57500 Loss 1.7322,time:5897.991229534149sec\n",
      "Epoch 3 Batch 57600 Loss 0.8713,time:5908.09946846962sec\n",
      "Epoch 3 Batch 57700 Loss 1.2173,time:5918.199743270874sec\n",
      "Epoch 3 Batch 57800 Loss 1.1614,time:5928.348027944565sec\n",
      "Epoch 3 Batch 57900 Loss 1.1670,time:5938.678354740143sec\n",
      "Epoch 3 Batch 58000 Loss 1.3600,time:5948.927051544189sec\n",
      "Epoch 3 Batch 58100 Loss 1.0903,time:5959.168742656708sec\n",
      "Epoch 3 Batch 58200 Loss 1.4722,time:5969.479065179825sec\n",
      "Epoch 3 Batch 58300 Loss 1.2041,time:5979.74337553978sec\n",
      "Epoch 3 Batch 58400 Loss 1.3305,time:5990.156720876694sec\n",
      "Epoch 3 Batch 58500 Loss 1.1359,time:6000.656486988068sec\n",
      "Epoch 3 Batch 58600 Loss 0.7870,time:6010.996378660202sec\n",
      "Epoch 3 Batch 58700 Loss 1.2123,time:6021.115657091141sec\n",
      "Epoch 3 Batch 58800 Loss 1.0328,time:6031.255940437317sec\n",
      "Epoch 3 Batch 58900 Loss 1.6989,time:6041.4332320690155sec\n",
      "Epoch 3 Batch 59000 Loss 1.2370,time:6051.743180274963sec\n",
      "Epoch 3 Batch 59100 Loss 1.6101,time:6062.048891782761sec\n",
      "Epoch 3 Batch 59200 Loss 1.3131,time:6072.367215156555sec\n",
      "Epoch 3 Batch 59300 Loss 1.0082,time:6082.702543020248sec\n",
      "Epoch 3 Batch 59400 Loss 1.4708,time:6092.9448499679565sec\n",
      "Epoch 3 Batch 59500 Loss 1.0512,time:6103.126768112183sec\n",
      "Epoch 3 Batch 59600 Loss 1.1326,time:6113.2997925281525sec\n",
      "Epoch 3 Batch 59700 Loss 1.0810,time:6123.511092662811sec\n",
      "Epoch 3 Batch 59800 Loss 1.0087,time:6133.672380924225sec\n",
      "Epoch 3 Batch 59900 Loss 1.3386,time:6143.766654729843sec\n",
      "Epoch 3 Batch 60000 Loss 1.1442,time:6153.967047214508sec\n",
      "Epoch 3 Batch 60100 Loss 1.4457,time:6164.106750488281sec\n",
      "Epoch 3 Batch 60200 Loss 1.2545,time:6174.1700167655945sec\n",
      "Epoch 3 Batch 60300 Loss 1.8223,time:6184.3042986392975sec\n",
      "Epoch 3 Batch 60400 Loss 1.0355,time:6194.558607578278sec\n",
      "Epoch 3 Batch 60500 Loss 1.0605,time:6204.7294049263sec\n",
      "Epoch 3 Batch 60600 Loss 1.4756,time:6214.908713817596sec\n",
      "Epoch 3 Batch 60700 Loss 1.6024,time:6225.099008560181sec\n",
      "Epoch 3 Batch 60800 Loss 1.3914,time:6235.306307315826sec\n",
      "Epoch 3 Batch 60900 Loss 1.2393,time:6245.617629528046sec\n",
      "Epoch 3 Batch 61000 Loss 1.3483,time:6256.002475976944sec\n",
      "Epoch 3 Batch 61100 Loss 1.1906,time:6266.306275367737sec\n",
      "Epoch 3 Batch 61200 Loss 1.0380,time:6276.715619802475sec\n",
      "Epoch 3 Batch 61300 Loss 1.6301,time:6287.148969173431sec\n",
      "Epoch 3 Batch 61400 Loss 1.3696,time:6297.501301288605sec\n",
      "Epoch 3 Batch 61500 Loss 1.2256,time:6307.877995014191sec\n",
      "Epoch 3 Batch 61600 Loss 1.3443,time:6318.213549613953sec\n",
      "Epoch 3 Batch 61700 Loss 1.5396,time:6328.558878660202sec\n",
      "Epoch 3 Batch 61800 Loss 2.0063,time:6338.7771809101105sec\n",
      "Epoch 3 Batch 61900 Loss 1.0571,time:6348.953472137451sec\n",
      "Epoch 3 Batch 62000 Loss 1.0255,time:6359.172173500061sec\n",
      "Epoch 3 Batch 62100 Loss 1.1640,time:6369.37268948555sec\n",
      "Epoch 3 Batch 62200 Loss 1.3667,time:6379.440956354141sec\n",
      "Epoch 3 Batch 62300 Loss 0.7965,time:6389.579239368439sec\n",
      "Epoch 3 Batch 62400 Loss 1.3491,time:6399.825546979904sec\n",
      "Epoch 3 Batch 62500 Loss 1.4220,time:6410.009363889694sec\n",
      "Epoch 3 Batch 62600 Loss 1.2144,time:6420.165481090546sec\n",
      "Epoch 3 Batch 62700 Loss 1.0068,time:6430.436083078384sec\n",
      "Epoch 3 Batch 62800 Loss 1.3514,time:6440.773411273956sec\n",
      "Epoch 3 Batch 62900 Loss 1.5982,time:6450.956704854965sec\n",
      "Epoch 3 Batch 63000 Loss 1.6794,time:6461.181007146835sec\n",
      "Epoch 3 Batch 63100 Loss 1.4669,time:6471.458607673645sec\n",
      "Epoch 3 Batch 63200 Loss 1.9124,time:6481.611086130142sec\n",
      "Epoch 3 Batch 63300 Loss 1.8689,time:6491.730365991592sec\n",
      "Epoch 3 Batch 63400 Loss 1.6447,time:6501.925661087036sec\n",
      "Epoch 3 Batch 63500 Loss 1.3012,time:6512.02493596077sec\n",
      "Epoch 3 Batch 63600 Loss 1.6671,time:6522.177613019943sec\n",
      "Epoch 3 Batch 63700 Loss 1.5357,time:6532.347496986389sec\n",
      "Epoch 3 Batch 63800 Loss 2.2613,time:6542.596805095673sec\n",
      "Epoch 3 Batch 63900 Loss 1.4356,time:6553.0371561050415sec\n",
      "Epoch 3 Batch 64000 Loss 1.3152,time:6563.470024585724sec\n",
      "Epoch 3 Batch 64100 Loss 1.3180,time:6573.78701877594sec\n",
      "Epoch 3 Batch 64200 Loss 1.1015,time:6584.2953844070435sec\n",
      "Epoch 3 Batch 64300 Loss 1.3968,time:6594.666719913483sec\n",
      "Epoch 3 Batch 64400 Loss 1.2543,time:6604.898024082184sec\n",
      "Epoch 3 Batch 64500 Loss 1.4317,time:6615.106783866882sec\n",
      "Epoch 3 Batch 64600 Loss 1.4314,time:6625.2758140563965sec\n",
      "Epoch 3 Batch 64700 Loss 1.1695,time:6635.4441039562225sec\n",
      "Epoch 3 Batch 64800 Loss 0.7735,time:6645.615394353867sec\n",
      "Epoch 3 Batch 64900 Loss 1.5583,time:6655.71966958046sec\n",
      "Epoch 3 Batch 65000 Loss 1.2315,time:6666.0288417339325sec\n",
      "Epoch 3 Batch 65100 Loss 0.9274,time:6676.258965730667sec\n",
      "Epoch 3 Batch 65200 Loss 1.1474,time:6686.4213235378265sec\n",
      "Epoch 3 Batch 65300 Loss 1.2727,time:6696.936692714691sec\n",
      "Epoch 3 Batch 65400 Loss 1.5726,time:6707.28102183342sec\n",
      "Epoch 3 Batch 65500 Loss 1.5118,time:6717.464542627335sec\n",
      "Epoch 3 Batch 65600 Loss 1.7093,time:6727.68576669693sec\n",
      "Epoch 3 Batch 65700 Loss 1.2615,time:6737.986265420914sec\n",
      "Epoch 3 Batch 65800 Loss 0.8572,time:6748.161556720734sec\n",
      "Epoch 3 Batch 65900 Loss 0.9257,time:6758.467877864838sec\n",
      "Epoch 3 Batch 66000 Loss 1.6814,time:6768.793203115463sec\n",
      "Epoch 3 Batch 66100 Loss 1.0891,time:6779.077692985535sec\n",
      "Epoch 3 Batch 66200 Loss 1.0903,time:6789.305996894836sec\n",
      "Epoch 3 Batch 66300 Loss 2.2726,time:6799.771353006363sec\n",
      "Epoch 3 Batch 66400 Loss 1.2342,time:6810.251713991165sec\n",
      "Epoch 3 Batch 66500 Loss 1.0697,time:6820.560538053513sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 66600 Loss 1.1893,time:6830.832781076431sec\n",
      "Epoch 3 Batch 66700 Loss 1.4921,time:6840.992116212845sec\n",
      "Epoch 3 Batch 66800 Loss 1.1762,time:6851.281433582306sec\n",
      "Epoch 3 Batch 66900 Loss 1.0338,time:6861.549746513367sec\n",
      "Epoch 3 Batch 67000 Loss 1.9200,time:6871.787558555603sec\n",
      "Epoch 3 Batch 67100 Loss 1.5388,time:6882.050358772278sec\n",
      "Epoch 3 Batch 67200 Loss 1.4856,time:6892.515210151672sec\n",
      "Epoch 3 Batch 67300 Loss 1.3780,time:6902.748514890671sec\n",
      "Epoch 3 Batch 67400 Loss 1.6296,time:6913.066838979721sec\n",
      "Epoch 3 Batch 67500 Loss 1.6984,time:6923.268136739731sec\n",
      "Epoch 3 Batch 67600 Loss 0.9934,time:6933.530592679977sec\n",
      "Epoch 3 Batch 67700 Loss 1.0480,time:6943.647187232971sec\n",
      "Epoch 3 Batch 67800 Loss 1.3587,time:6953.76146531105sec\n",
      "Epoch 3 Batch 67900 Loss 0.8227,time:6963.867741107941sec\n",
      "Epoch 3 Batch 68000 Loss 1.3715,time:6973.999022722244sec\n",
      "Epoch 3 Batch 68100 Loss 1.3203,time:6984.098666667938sec\n",
      "Epoch 3 Batch 68200 Loss 1.2231,time:6994.468792676926sec\n",
      "Epoch 3 Batch 68300 Loss 1.0031,time:7004.705097198486sec\n",
      "Epoch 3 Batch 68400 Loss 0.8667,time:7015.041425228119sec\n",
      "Epoch 3 Batch 68500 Loss 1.5831,time:7025.444767713547sec\n",
      "Epoch 3 Batch 68600 Loss 1.1833,time:7035.741005182266sec\n",
      "Epoch 3 Batch 68700 Loss 1.8360,time:7045.885861158371sec\n",
      "Epoch 3 Batch 68800 Loss 1.1190,time:7056.175177812576sec\n",
      "Epoch 3 Batch 68900 Loss 1.2498,time:7066.639534950256sec\n",
      "Epoch 3 Batch 69000 Loss 1.9250,time:7076.955858469009sec\n",
      "Epoch 3 Batch 69100 Loss 1.1755,time:7087.268374919891sec\n",
      "Epoch 3 Batch 69200 Loss 1.0363,time:7097.404795885086sec\n",
      "Epoch 3 Batch 69300 Loss 1.0847,time:7107.5840883255005sec\n",
      "Epoch 3 Batch 69400 Loss 1.2215,time:7117.64235329628sec\n",
      "Epoch 3 Batch 69500 Loss 1.6767,time:7127.8096425533295sec\n",
      "Epoch 3 Batch 69600 Loss 1.6180,time:7137.911426544189sec\n",
      "Epoch 3 Batch 69700 Loss 1.7903,time:7148.004568576813sec\n",
      "Epoch 3 Batch 69800 Loss 0.9588,time:7158.271373510361sec\n",
      "Epoch 3 Batch 69900 Loss 0.9516,time:7168.523683309555sec\n",
      "Epoch 3 Batch 70000 Loss 1.0827,time:7178.846007108688sec\n",
      "Epoch 3 Batch 70100 Loss 1.6823,time:7189.138637065887sec\n",
      "Epoch 3 Batch 70200 Loss 0.9520,time:7199.399419546127sec\n",
      "Epoch 3 Batch 70300 Loss 1.1631,time:7209.525700330734sec\n",
      "Epoch 3 Batch 70400 Loss 0.9803,time:7219.6959908008575sec\n",
      "Epoch 3 Batch 70500 Loss 1.1913,time:7229.835274219513sec\n",
      "Epoch 3 Batch 70600 Loss 0.5448,time:7239.939549446106sec\n",
      "Epoch 3 Batch 70700 Loss 1.0709,time:7250.196785211563sec\n",
      "Epoch 3 Batch 70800 Loss 1.1060,time:7260.513704776764sec\n",
      "Epoch 3 Batch 70900 Loss 1.3125,time:7270.708000659943sec\n",
      "Epoch 3 Batch 71000 Loss 1.0400,time:7280.939305305481sec\n",
      "Epoch 3 Batch 71100 Loss 1.2882,time:7291.141602039337sec\n",
      "Epoch 3 Batch 71200 Loss 1.5573,time:7301.433403968811sec\n",
      "Epoch 3 Batch 71300 Loss 0.8598,time:7311.745103597641sec\n",
      "Epoch 3 Batch 71400 Loss 1.3161,time:7322.07643032074sec\n",
      "Epoch 3 Batch 71500 Loss 1.8400,time:7332.377750873566sec\n",
      "Epoch 3 Batch 71600 Loss 0.6980,time:7342.624057769775sec\n",
      "Epoch 3 Batch 71700 Loss 1.3223,time:7352.787805318832sec\n",
      "Epoch 3 Batch 71800 Loss 1.2273,time:7363.058524847031sec\n",
      "Epoch 3 Batch 71900 Loss 1.2058,time:7373.21081161499sec\n",
      "Epoch 3 Batch 72000 Loss 1.0472,time:7383.331090688705sec\n",
      "Epoch 3 Batch 72100 Loss 1.4551,time:7393.805450201035sec\n",
      "Epoch 3 Batch 72200 Loss 1.4326,time:7403.995167732239sec\n",
      "Epoch 3 Batch 72300 Loss 1.8679,time:7414.209599733353sec\n",
      "Epoch 3 Batch 72400 Loss 0.8271,time:7424.452906847sec\n",
      "Epoch 3 Batch 72500 Loss 1.7304,time:7434.577186822891sec\n",
      "Epoch 3 Batch 72600 Loss 1.1537,time:7444.749477863312sec\n",
      "Epoch 3 Batch 72700 Loss 1.0466,time:7454.89781665802sec\n",
      "Epoch 3 Loss 1.2792\n",
      "Time taken for 1 epoch 7462.837040901184 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3542,time:1.513340950012207sec\n",
      "Epoch 4 Batch 100 Loss 1.7088,time:11.809659719467163sec\n",
      "Epoch 4 Batch 200 Loss 1.0042,time:22.2910213470459sec\n",
      "Epoch 4 Batch 300 Loss 0.7154,time:32.726370334625244sec\n",
      "Epoch 4 Batch 400 Loss 0.8473,time:43.052818059921265sec\n",
      "Epoch 4 Batch 500 Loss 1.5040,time:53.29164123535156sec\n",
      "Epoch 4 Batch 600 Loss 1.1012,time:63.547950744628906sec\n",
      "Epoch 4 Batch 700 Loss 1.0725,time:73.99930453300476sec\n",
      "Epoch 4 Batch 800 Loss 1.3258,time:84.1275863647461sec\n",
      "Epoch 4 Batch 900 Loss 1.0535,time:94.3724582195282sec\n",
      "Epoch 4 Batch 1000 Loss 1.0508,time:104.62403297424316sec\n",
      "Epoch 4 Batch 1100 Loss 1.0418,time:114.87234163284302sec\n",
      "Epoch 4 Batch 1200 Loss 1.4845,time:125.17766213417053sec\n",
      "Epoch 4 Batch 1300 Loss 1.8576,time:135.32694792747498sec\n",
      "Epoch 4 Batch 1400 Loss 0.8106,time:145.50032782554626sec\n",
      "Epoch 4 Batch 1500 Loss 1.1250,time:155.81594848632812sec\n",
      "Epoch 4 Batch 1600 Loss 1.1769,time:166.0042428970337sec\n",
      "Epoch 4 Batch 1700 Loss 1.0335,time:176.27455592155457sec\n",
      "Epoch 4 Batch 1800 Loss 1.6366,time:186.37383031845093sec\n",
      "Epoch 4 Batch 1900 Loss 1.6641,time:196.41909337043762sec\n",
      "Epoch 4 Batch 2000 Loss 1.2984,time:206.53901743888855sec\n",
      "Epoch 4 Batch 2100 Loss 1.5088,time:216.5552887916565sec\n",
      "Epoch 4 Batch 2200 Loss 1.3776,time:226.7075753211975sec\n",
      "Epoch 4 Batch 2300 Loss 1.2538,time:236.88186645507812sec\n",
      "Epoch 4 Batch 2400 Loss 1.4745,time:246.97013807296753sec\n",
      "Epoch 4 Batch 2500 Loss 1.2569,time:257.19759249687195sec\n",
      "Epoch 4 Batch 2600 Loss 1.4420,time:267.37437653541565sec\n",
      "Epoch 4 Batch 2700 Loss 1.2042,time:277.4886543750763sec\n",
      "Epoch 4 Batch 2800 Loss 1.2619,time:287.6819498538971sec\n",
      "Epoch 4 Batch 2900 Loss 1.0106,time:297.96926641464233sec\n",
      "Epoch 4 Batch 3000 Loss 0.8856,time:308.46817564964294sec\n",
      "Epoch 4 Batch 3100 Loss 0.7504,time:318.8551149368286sec\n",
      "Epoch 4 Batch 3200 Loss 1.2310,time:329.14143228530884sec\n",
      "Epoch 4 Batch 3300 Loss 0.8972,time:339.4477527141571sec\n",
      "Epoch 4 Batch 3400 Loss 0.9864,time:349.75607419013977sec\n",
      "Epoch 4 Batch 3500 Loss 1.3785,time:360.0566689968109sec\n",
      "Epoch 4 Batch 3600 Loss 0.9765,time:370.342964887619sec\n",
      "Epoch 4 Batch 3700 Loss 1.4089,time:380.5922725200653sec\n",
      "Epoch 4 Batch 3800 Loss 1.4764,time:390.7895698547363sec\n",
      "Epoch 4 Batch 3900 Loss 0.9648,time:400.9918670654297sec\n",
      "Epoch 4 Batch 4000 Loss 1.0413,time:411.2625517845154sec\n",
      "Epoch 4 Batch 4100 Loss 1.3352,time:421.4502296447754sec\n",
      "Epoch 4 Batch 4200 Loss 1.1716,time:431.6985375881195sec\n",
      "Epoch 4 Batch 4300 Loss 1.5000,time:441.860826253891sec\n",
      "Epoch 4 Batch 4400 Loss 0.9718,time:452.0571229457855sec\n",
      "Epoch 4 Batch 4500 Loss 1.3671,time:462.27042388916016sec\n",
      "Epoch 4 Batch 4600 Loss 1.3857,time:472.373468875885sec\n",
      "Epoch 4 Batch 4700 Loss 1.0067,time:482.6197762489319sec\n",
      "Epoch 4 Batch 4800 Loss 1.1428,time:492.9000914096832sec\n",
      "Epoch 4 Batch 4900 Loss 1.4260,time:503.0623800754547sec\n",
      "Epoch 4 Batch 5000 Loss 0.8282,time:513.3580257892609sec\n",
      "Epoch 4 Batch 5100 Loss 1.0020,time:523.6398038864136sec\n",
      "Epoch 4 Batch 5200 Loss 1.4128,time:533.9441239833832sec\n",
      "Epoch 4 Batch 5300 Loss 1.1414,time:544.2244400978088sec\n",
      "Epoch 4 Batch 5400 Loss 1.6316,time:554.6957976818085sec\n",
      "Epoch 4 Batch 5500 Loss 1.0586,time:564.872166633606sec\n",
      "Epoch 4 Batch 5600 Loss 1.0114,time:575.0254254341125sec\n",
      "Epoch 4 Batch 5700 Loss 1.5289,time:585.2977390289307sec\n",
      "Epoch 4 Batch 5800 Loss 0.9687,time:595.5060379505157sec\n",
      "Epoch 4 Batch 5900 Loss 0.8761,time:605.9533908367157sec\n",
      "Epoch 4 Batch 6000 Loss 1.3525,time:616.3552405834198sec\n",
      "Epoch 4 Batch 6100 Loss 1.3672,time:626.9897894859314sec\n",
      "Epoch 4 Batch 6200 Loss 0.7564,time:637.1850855350494sec\n",
      "Epoch 4 Batch 6300 Loss 1.0084,time:647.4073870182037sec\n",
      "Epoch 4 Batch 6400 Loss 1.6868,time:657.634690284729sec\n",
      "Epoch 4 Batch 6500 Loss 1.1481,time:667.9086918830872sec\n",
      "Epoch 4 Batch 6600 Loss 1.4234,time:678.4584457874298sec\n",
      "Epoch 4 Batch 6700 Loss 1.3672,time:688.8157787322998sec\n",
      "Epoch 4 Batch 6800 Loss 1.5792,time:699.074089050293sec\n",
      "Epoch 4 Batch 6900 Loss 1.3155,time:709.383410692215sec\n",
      "Epoch 4 Batch 7000 Loss 1.0471,time:719.6130464076996sec\n",
      "Epoch 4 Batch 7100 Loss 1.6921,time:729.834468126297sec\n",
      "Epoch 4 Batch 7200 Loss 1.4069,time:740.1157834529877sec\n",
      "Epoch 4 Batch 7300 Loss 1.5077,time:750.2780723571777sec\n",
      "Epoch 4 Batch 7400 Loss 1.6596,time:760.4873714447021sec\n",
      "Epoch 4 Batch 7500 Loss 1.0617,time:771.1109757423401sec\n",
      "Epoch 4 Batch 7600 Loss 0.8643,time:781.3759150505066sec\n",
      "Epoch 4 Batch 7700 Loss 1.1027,time:791.7615127563477sec\n",
      "Epoch 4 Batch 7800 Loss 1.8567,time:802.338894367218sec\n",
      "Epoch 4 Batch 7900 Loss 1.3477,time:812.8032512664795sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 8000 Loss 1.0369,time:823.2496032714844sec\n",
      "Epoch 4 Batch 8100 Loss 1.5975,time:833.3596439361572sec\n",
      "Epoch 4 Batch 8200 Loss 1.3167,time:843.6239557266235sec\n",
      "Epoch 4 Batch 8300 Loss 1.2979,time:853.8692631721497sec\n",
      "Epoch 4 Batch 8400 Loss 0.8945,time:864.3776288032532sec\n",
      "Epoch 4 Batch 8500 Loss 1.2743,time:874.7549667358398sec\n",
      "Epoch 4 Batch 8600 Loss 1.0300,time:884.9838423728943sec\n",
      "Epoch 4 Batch 8700 Loss 1.4692,time:895.2310054302216sec\n",
      "Epoch 4 Batch 8800 Loss 1.4226,time:905.3462834358215sec\n",
      "Epoch 4 Batch 8900 Loss 1.5479,time:915.529577255249sec\n",
      "Epoch 4 Batch 9000 Loss 1.3849,time:925.8168935775757sec\n",
      "Epoch 4 Batch 9100 Loss 0.7998,time:936.1284573078156sec\n",
      "Epoch 4 Batch 9200 Loss 1.6569,time:946.660834312439sec\n",
      "Epoch 4 Batch 9300 Loss 1.5360,time:957.1481957435608sec\n",
      "Epoch 4 Batch 9400 Loss 0.9904,time:967.3424916267395sec\n",
      "Epoch 4 Batch 9500 Loss 1.0433,time:977.6398100852966sec\n",
      "Epoch 4 Batch 9600 Loss 1.2966,time:987.80357837677sec\n",
      "Epoch 4 Batch 9700 Loss 1.3990,time:997.9338698387146sec\n",
      "Epoch 4 Batch 9800 Loss 1.2183,time:1008.1531715393066sec\n",
      "Epoch 4 Batch 9900 Loss 1.4105,time:1018.3444662094116sec\n",
      "Epoch 4 Batch 10000 Loss 1.6265,time:1028.9058451652527sec\n",
      "Epoch 4 Batch 10100 Loss 1.1298,time:1039.4424798488617sec\n",
      "Epoch 4 Batch 10200 Loss 1.3025,time:1049.4898891448975sec\n",
      "Epoch 4 Batch 10300 Loss 1.3771,time:1059.7291960716248sec\n",
      "Epoch 4 Batch 10400 Loss 1.3599,time:1069.9474964141846sec\n",
      "Epoch 4 Batch 10500 Loss 1.7225,time:1080.1347906589508sec\n",
      "Epoch 4 Batch 10600 Loss 1.4584,time:1090.4403660297394sec\n",
      "Epoch 4 Batch 10700 Loss 1.3374,time:1100.696876525879sec\n",
      "Epoch 4 Batch 10800 Loss 1.1339,time:1110.9061760902405sec\n",
      "Epoch 4 Batch 10900 Loss 1.3110,time:1121.2224996089935sec\n",
      "Epoch 4 Batch 11000 Loss 1.0971,time:1131.5568263530731sec\n",
      "Epoch 4 Batch 11100 Loss 1.2153,time:1141.684873342514sec\n",
      "Epoch 4 Batch 11200 Loss 0.9923,time:1151.8782918453217sec\n",
      "Epoch 4 Batch 11300 Loss 1.2476,time:1162.03457903862sec\n",
      "Epoch 4 Batch 11400 Loss 1.2998,time:1172.1828649044037sec\n",
      "Epoch 4 Batch 11500 Loss 1.1738,time:1182.6892302036285sec\n",
      "Epoch 4 Batch 11600 Loss 1.2436,time:1192.987973690033sec\n",
      "Epoch 4 Batch 11700 Loss 1.3400,time:1203.071453332901sec\n",
      "Epoch 4 Batch 11800 Loss 1.1777,time:1213.516886472702sec\n",
      "Epoch 4 Batch 11900 Loss 1.5307,time:1223.7641944885254sec\n",
      "Epoch 4 Batch 12000 Loss 1.2334,time:1234.0765163898468sec\n",
      "Epoch 4 Batch 12100 Loss 1.3524,time:1244.277813911438sec\n",
      "Epoch 4 Batch 12200 Loss 0.8345,time:1254.6052567958832sec\n",
      "Epoch 4 Batch 12300 Loss 1.6111,time:1265.1480131149292sec\n",
      "Epoch 4 Batch 12400 Loss 1.4731,time:1275.4243268966675sec\n",
      "Epoch 4 Batch 12500 Loss 1.3760,time:1285.6366274356842sec\n",
      "Epoch 4 Batch 12600 Loss 1.0611,time:1295.7349014282227sec\n",
      "Epoch 4 Batch 12700 Loss 1.4618,time:1305.9977278709412sec\n",
      "Epoch 4 Batch 12800 Loss 1.4053,time:1316.426965713501sec\n",
      "Epoch 4 Batch 12900 Loss 1.2343,time:1326.5862526893616sec\n",
      "Epoch 4 Batch 13000 Loss 1.9172,time:1336.6805267333984sec\n",
      "Epoch 4 Batch 13100 Loss 1.3800,time:1346.8718218803406sec\n",
      "Epoch 4 Batch 13200 Loss 1.0699,time:1357.1226522922516sec\n",
      "Epoch 4 Batch 13300 Loss 1.6975,time:1367.4630563259125sec\n",
      "Epoch 4 Batch 13400 Loss 1.8958,time:1377.7273678779602sec\n",
      "Epoch 4 Batch 13500 Loss 1.2941,time:1387.9366664886475sec\n",
      "Epoch 4 Batch 13600 Loss 1.3745,time:1398.182974100113sec\n",
      "Epoch 4 Batch 13700 Loss 1.4106,time:1408.3679502010345sec\n",
      "Epoch 4 Batch 13800 Loss 1.2983,time:1418.6394345760345sec\n",
      "Epoch 4 Batch 13900 Loss 1.0426,time:1428.9047462940216sec\n",
      "Epoch 4 Batch 14000 Loss 0.9702,time:1439.1000413894653sec\n",
      "Epoch 4 Batch 14100 Loss 1.2686,time:1449.6594202518463sec\n",
      "Epoch 4 Batch 14200 Loss 1.6913,time:1460.2371954917908sec\n",
      "Epoch 4 Batch 14300 Loss 0.7436,time:1470.4458966255188sec\n",
      "Epoch 4 Batch 14400 Loss 1.2320,time:1480.6872026920319sec\n",
      "Epoch 4 Batch 14500 Loss 1.1789,time:1490.8945016860962sec\n",
      "Epoch 4 Batch 14600 Loss 0.7870,time:1501.0387859344482sec\n",
      "Epoch 4 Batch 14700 Loss 1.1296,time:1511.4815485477448sec\n",
      "Epoch 4 Batch 14800 Loss 1.4583,time:1522.0922849178314sec\n",
      "Epoch 4 Batch 14900 Loss 1.6566,time:1532.479623556137sec\n",
      "Epoch 4 Batch 15000 Loss 1.3648,time:1542.7899458408356sec\n",
      "Epoch 4 Batch 15100 Loss 1.1527,time:1553.1952891349792sec\n",
      "Epoch 4 Batch 15200 Loss 1.0466,time:1563.5379333496094sec\n",
      "Epoch 4 Batch 15300 Loss 1.4231,time:1573.8042464256287sec\n",
      "Epoch 4 Batch 15400 Loss 1.3534,time:1584.1475756168365sec\n",
      "Epoch 4 Batch 15500 Loss 1.1110,time:1594.442893743515sec\n",
      "Epoch 4 Batch 15600 Loss 1.3947,time:1604.7812218666077sec\n",
      "Epoch 4 Batch 15700 Loss 1.3329,time:1615.0285289287567sec\n",
      "Epoch 4 Batch 15800 Loss 1.1087,time:1625.3879401683807sec\n",
      "Epoch 4 Batch 15900 Loss 1.5148,time:1635.8786163330078sec\n",
      "Epoch 4 Batch 16000 Loss 1.3654,time:1646.119922876358sec\n",
      "Epoch 4 Batch 16100 Loss 0.7063,time:1656.365229845047sec\n",
      "Epoch 4 Batch 16200 Loss 1.2586,time:1666.594170331955sec\n",
      "Epoch 4 Batch 16300 Loss 1.2098,time:1676.9393939971924sec\n",
      "Epoch 4 Batch 16400 Loss 1.2853,time:1687.1846613883972sec\n",
      "Epoch 4 Batch 16500 Loss 1.8896,time:1697.3689546585083sec\n",
      "Epoch 4 Batch 16600 Loss 1.3774,time:1707.59725856781sec\n",
      "Epoch 4 Batch 16700 Loss 1.5625,time:1717.899578332901sec\n",
      "Epoch 4 Batch 16800 Loss 1.0014,time:1728.2532920837402sec\n",
      "Epoch 4 Batch 16900 Loss 0.9187,time:1738.4373469352722sec\n",
      "Epoch 4 Batch 17000 Loss 1.2752,time:1748.5746295452118sec\n",
      "Epoch 4 Batch 17100 Loss 1.6759,time:1758.8069343566895sec\n",
      "Epoch 4 Batch 17200 Loss 1.0539,time:1768.9972290992737sec\n",
      "Epoch 4 Batch 17300 Loss 1.5962,time:1779.3939096927643sec\n",
      "Epoch 4 Batch 17400 Loss 1.4332,time:1789.6581983566284sec\n",
      "Epoch 4 Batch 17500 Loss 1.2680,time:1799.7634737491608sec\n",
      "Epoch 4 Batch 17600 Loss 1.0333,time:1809.9257633686066sec\n",
      "Epoch 4 Batch 17700 Loss 0.7466,time:1820.370114326477sec\n",
      "Epoch 4 Batch 17800 Loss 1.2923,time:1830.5698926448822sec\n",
      "Epoch 4 Batch 17900 Loss 1.4686,time:1841.0212543010712sec\n",
      "Epoch 4 Batch 18000 Loss 1.2633,time:1851.4806096553802sec\n",
      "Epoch 4 Batch 18100 Loss 1.4436,time:1861.727917432785sec\n",
      "Epoch 4 Batch 18200 Loss 1.6778,time:1872.1332604885101sec\n",
      "Epoch 4 Batch 18300 Loss 1.1809,time:1882.8826353549957sec\n",
      "Epoch 4 Batch 18400 Loss 1.3340,time:1893.2180998325348sec\n",
      "Epoch 4 Batch 18500 Loss 0.8870,time:1903.3703863620758sec\n",
      "Epoch 4 Batch 18600 Loss 1.0397,time:1913.4836628437042sec\n",
      "Epoch 4 Batch 18700 Loss 0.8026,time:1923.6239473819733sec\n",
      "Epoch 4 Batch 18800 Loss 1.1438,time:1933.818822145462sec\n",
      "Epoch 4 Batch 18900 Loss 1.7724,time:1943.8788313865662sec\n",
      "Epoch 4 Batch 19000 Loss 1.2210,time:1954.083129644394sec\n",
      "Epoch 4 Batch 19100 Loss 1.2529,time:1964.2664232254028sec\n",
      "Epoch 4 Batch 19200 Loss 1.4872,time:1974.4947266578674sec\n",
      "Epoch 4 Batch 19300 Loss 1.2567,time:1984.805973291397sec\n",
      "Epoch 4 Batch 19400 Loss 1.4623,time:1994.9850659370422sec\n",
      "Epoch 4 Batch 19500 Loss 1.3392,time:2005.1398615837097sec\n",
      "Epoch 4 Batch 19600 Loss 1.5237,time:2015.3051509857178sec\n",
      "Epoch 4 Batch 19700 Loss 1.4194,time:2025.4534361362457sec\n",
      "Epoch 4 Batch 19800 Loss 1.2625,time:2035.58371758461sec\n",
      "Epoch 4 Batch 19900 Loss 1.5057,time:2045.8241636753082sec\n",
      "Epoch 4 Batch 20000 Loss 1.6869,time:2056.328361272812sec\n",
      "Epoch 4 Batch 20100 Loss 1.4732,time:2066.6426844596863sec\n",
      "Epoch 4 Batch 20200 Loss 1.4492,time:2076.9059948921204sec\n",
      "Epoch 4 Batch 20300 Loss 1.2836,time:2087.297335624695sec\n",
      "Epoch 4 Batch 20400 Loss 0.7444,time:2097.506398677826sec\n",
      "Epoch 4 Batch 20500 Loss 1.5794,time:2107.7471027374268sec\n",
      "Epoch 4 Batch 20600 Loss 1.3048,time:2118.0144143104553sec\n",
      "Epoch 4 Batch 20700 Loss 1.5517,time:2128.2147119045258sec\n",
      "Epoch 4 Batch 20800 Loss 1.1487,time:2138.5160319805145sec\n",
      "Epoch 4 Batch 20900 Loss 0.9865,time:2148.8754892349243sec\n",
      "Epoch 4 Batch 21000 Loss 1.0721,time:2159.184317111969sec\n",
      "Epoch 4 Batch 21100 Loss 1.2548,time:2169.410620689392sec\n",
      "Epoch 4 Batch 21200 Loss 1.2153,time:2179.7869567871094sec\n",
      "Epoch 4 Batch 21300 Loss 1.1065,time:2190.0802755355835sec\n",
      "Epoch 4 Batch 21400 Loss 2.0080,time:2200.1765744686127sec\n",
      "Epoch 4 Batch 21500 Loss 1.0369,time:2210.3422458171844sec\n",
      "Epoch 4 Batch 21600 Loss 0.8037,time:2220.4755280017853sec\n",
      "Epoch 4 Batch 21700 Loss 1.3908,time:2230.5808041095734sec\n",
      "Epoch 4 Batch 21800 Loss 1.3071,time:2240.8331129550934sec\n",
      "Epoch 4 Batch 21900 Loss 1.2581,time:2251.259516477585sec\n",
      "Epoch 4 Batch 22000 Loss 1.2536,time:2261.359792947769sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 22100 Loss 1.4929,time:2271.4370622634888sec\n",
      "Epoch 4 Batch 22200 Loss 1.2246,time:2281.6283571720123sec\n",
      "Epoch 4 Batch 22300 Loss 1.0100,time:2291.7146286964417sec\n",
      "Epoch 4 Batch 22400 Loss 1.1996,time:2301.9384422302246sec\n",
      "Epoch 4 Batch 22500 Loss 1.9257,time:2312.2563881874084sec\n",
      "Epoch 4 Batch 22600 Loss 1.2364,time:2322.3896906375885sec\n",
      "Epoch 4 Batch 22700 Loss 1.3561,time:2332.6119928359985sec\n",
      "Epoch 4 Batch 22800 Loss 1.1809,time:2342.693263053894sec\n",
      "Epoch 4 Batch 22900 Loss 1.1401,time:2352.8505516052246sec\n",
      "Epoch 4 Batch 23000 Loss 1.1762,time:2362.993851184845sec\n",
      "Epoch 4 Batch 23100 Loss 1.9516,time:2373.4057087898254sec\n",
      "Epoch 4 Batch 23200 Loss 1.1526,time:2383.743037223816sec\n",
      "Epoch 4 Batch 23300 Loss 1.0817,time:2394.110371828079sec\n",
      "Epoch 4 Batch 23400 Loss 1.5950,time:2404.369681596756sec\n",
      "Epoch 4 Batch 23500 Loss 1.2297,time:2414.6748588085175sec\n",
      "Epoch 4 Batch 23600 Loss 0.9815,time:2425.0712072849274sec\n",
      "Epoch 4 Batch 23700 Loss 1.1310,time:2435.408535718918sec\n",
      "Epoch 4 Batch 23800 Loss 0.9904,time:2445.7218582630157sec\n",
      "Epoch 4 Batch 23900 Loss 1.4661,time:2455.872143507004sec\n",
      "Epoch 4 Batch 24000 Loss 1.1677,time:2466.0691273212433sec\n",
      "Epoch 4 Batch 24100 Loss 1.5731,time:2476.424556016922sec\n",
      "Epoch 4 Batch 24200 Loss 1.1822,time:2486.7708864212036sec\n",
      "Epoch 4 Batch 24300 Loss 1.5658,time:2496.896166563034sec\n",
      "Epoch 4 Batch 24400 Loss 1.3356,time:2507.051453590393sec\n",
      "Epoch 4 Batch 24500 Loss 1.4375,time:2517.1847734451294sec\n",
      "Epoch 4 Batch 24600 Loss 1.1015,time:2527.4999573230743sec\n",
      "Epoch 4 Batch 24700 Loss 1.3648,time:2537.7122571468353sec\n",
      "Epoch 4 Batch 24800 Loss 1.4417,time:2547.870544910431sec\n",
      "Epoch 4 Batch 24900 Loss 1.6994,time:2558.0778431892395sec\n",
      "Epoch 4 Batch 25000 Loss 1.0885,time:2568.3786911964417sec\n",
      "Epoch 4 Batch 25100 Loss 1.3031,time:2578.7140204906464sec\n",
      "Epoch 4 Batch 25200 Loss 1.3528,time:2589.0393464565277sec\n",
      "Epoch 4 Batch 25300 Loss 0.8600,time:2599.3346643447876sec\n",
      "Epoch 4 Batch 25400 Loss 1.3673,time:2609.792019844055sec\n",
      "Epoch 4 Batch 25500 Loss 1.7848,time:2620.3431022167206sec\n",
      "Epoch 4 Batch 25600 Loss 1.1245,time:2630.694566965103sec\n",
      "Epoch 4 Batch 25700 Loss 1.2462,time:2641.014890909195sec\n",
      "Epoch 4 Batch 25800 Loss 1.2320,time:2651.15017414093sec\n",
      "Epoch 4 Batch 25900 Loss 1.5765,time:2661.310461997986sec\n",
      "Epoch 4 Batch 26000 Loss 1.2350,time:2671.5048048496246sec\n",
      "Epoch 4 Batch 26100 Loss 1.2204,time:2681.785036802292sec\n",
      "Epoch 4 Batch 26200 Loss 1.2697,time:2691.9823336601257sec\n",
      "Epoch 4 Batch 26300 Loss 1.2138,time:2702.4226846694946sec\n",
      "Epoch 4 Batch 26400 Loss 1.4315,time:2712.6209814548492sec\n",
      "Epoch 4 Batch 26500 Loss 1.6356,time:2722.8666775226593sec\n",
      "Epoch 4 Batch 26600 Loss 1.3468,time:2733.0802352428436sec\n",
      "Epoch 4 Batch 26700 Loss 1.5457,time:2743.312800168991sec\n",
      "Epoch 4 Batch 26800 Loss 1.2846,time:2753.5040953159332sec\n",
      "Epoch 4 Batch 26900 Loss 1.5275,time:2763.9284431934357sec\n",
      "Epoch 4 Batch 27000 Loss 1.1336,time:2774.231278896332sec\n",
      "Epoch 4 Batch 27100 Loss 1.1405,time:2784.4815583229065sec\n",
      "Epoch 4 Batch 27200 Loss 0.8654,time:2794.7069647312164sec\n",
      "Epoch 4 Batch 27300 Loss 1.2840,time:2804.790235519409sec\n",
      "Epoch 4 Batch 27400 Loss 1.1706,time:2814.9385209083557sec\n",
      "Epoch 4 Batch 27500 Loss 1.2108,time:2825.0177907943726sec\n",
      "Epoch 4 Batch 27600 Loss 1.0881,time:2835.2412824630737sec\n",
      "Epoch 4 Batch 27700 Loss 1.2098,time:2845.609055995941sec\n",
      "Epoch 4 Batch 27800 Loss 0.8985,time:2855.73433637619sec\n",
      "Epoch 4 Batch 27900 Loss 1.2846,time:2866.1556832790375sec\n",
      "Epoch 4 Batch 28000 Loss 1.0841,time:2876.6200404167175sec\n",
      "Epoch 4 Batch 28100 Loss 0.9559,time:2886.9064903259277sec\n",
      "Epoch 4 Batch 28200 Loss 0.9782,time:2897.19517326355sec\n",
      "Epoch 4 Batch 28300 Loss 1.0342,time:2907.4184753894806sec\n",
      "Epoch 4 Batch 28400 Loss 1.3900,time:2917.595767736435sec\n",
      "Epoch 4 Batch 28500 Loss 1.8158,time:2927.8290724754333sec\n",
      "Epoch 4 Batch 28600 Loss 1.6403,time:2938.0282855033875sec\n",
      "Epoch 4 Batch 28700 Loss 1.0339,time:2948.0697903633118sec\n",
      "Epoch 4 Batch 28800 Loss 1.3938,time:2958.4241223335266sec\n",
      "Epoch 4 Batch 28900 Loss 0.8862,time:2968.637422800064sec\n",
      "Epoch 4 Batch 29000 Loss 1.2511,time:2978.964748620987sec\n",
      "Epoch 4 Batch 29100 Loss 1.3220,time:2989.15433883667sec\n",
      "Epoch 4 Batch 29200 Loss 1.3603,time:2999.323436975479sec\n",
      "Epoch 4 Batch 29300 Loss 1.9314,time:3009.667766571045sec\n",
      "Epoch 4 Batch 29400 Loss 1.2549,time:3019.9100732803345sec\n",
      "Epoch 4 Batch 29500 Loss 1.0989,time:3030.1303749084473sec\n",
      "Epoch 4 Batch 29600 Loss 1.5770,time:3040.5622549057007sec\n",
      "Epoch 4 Batch 29700 Loss 1.0609,time:3050.8997378349304sec\n",
      "Epoch 4 Batch 29800 Loss 1.1555,time:3061.342089653015sec\n",
      "Epoch 4 Batch 29900 Loss 1.1318,time:3071.767437696457sec\n",
      "Epoch 4 Batch 30000 Loss 1.1388,time:3082.160910844803sec\n",
      "Epoch 4 Batch 30100 Loss 1.2493,time:3092.32639169693sec\n",
      "Epoch 4 Batch 30200 Loss 1.6750,time:3102.475677251816sec\n",
      "Epoch 4 Batch 30300 Loss 1.3736,time:3112.637966156006sec\n",
      "Epoch 4 Batch 30400 Loss 1.5112,time:3123.2513558864594sec\n",
      "Epoch 4 Batch 30500 Loss 1.4235,time:3133.6562094688416sec\n",
      "Epoch 4 Batch 30600 Loss 1.0374,time:3143.8303196430206sec\n",
      "Epoch 4 Batch 30700 Loss 1.2675,time:3154.0797290802sec\n",
      "Epoch 4 Batch 30800 Loss 1.4513,time:3164.1800043582916sec\n",
      "Epoch 4 Batch 30900 Loss 1.7645,time:3174.2862803936005sec\n",
      "Epoch 4 Batch 31000 Loss 1.0764,time:3184.4325654506683sec\n",
      "Epoch 4 Batch 31100 Loss 1.1029,time:3194.789875268936sec\n",
      "Epoch 4 Batch 31200 Loss 1.6354,time:3205.1193907260895sec\n",
      "Epoch 4 Batch 31300 Loss 1.0122,time:3215.2236919403076sec\n",
      "Epoch 4 Batch 31400 Loss 0.7496,time:3225.421987771988sec\n",
      "Epoch 4 Batch 31500 Loss 0.9272,time:3235.5472683906555sec\n",
      "Epoch 4 Batch 31600 Loss 1.4220,time:3245.651543855667sec\n",
      "Epoch 4 Batch 31700 Loss 0.8710,time:3256.177854537964sec\n",
      "Epoch 4 Batch 31800 Loss 1.1285,time:3266.422143936157sec\n",
      "Epoch 4 Batch 31900 Loss 1.6363,time:3276.6494476795197sec\n",
      "Epoch 4 Batch 32000 Loss 1.3237,time:3286.900755405426sec\n",
      "Epoch 4 Batch 32100 Loss 1.6417,time:3297.131061077118sec\n",
      "Epoch 4 Batch 32200 Loss 1.0361,time:3307.3354671001434sec\n",
      "Epoch 4 Batch 32300 Loss 1.6831,time:3317.5199472904205sec\n",
      "Epoch 4 Batch 32400 Loss 1.2565,time:3327.810264825821sec\n",
      "Epoch 4 Batch 32500 Loss 1.3773,time:3338.103582382202sec\n",
      "Epoch 4 Batch 32600 Loss 1.4829,time:3348.3528912067413sec\n",
      "Epoch 4 Batch 32700 Loss 1.2994,time:3358.651513814926sec\n",
      "Epoch 4 Batch 32800 Loss 0.9198,time:3368.9453160762787sec\n",
      "Epoch 4 Batch 32900 Loss 1.5276,time:3379.2981560230255sec\n",
      "Epoch 4 Batch 33000 Loss 0.7570,time:3389.608477830887sec\n",
      "Epoch 4 Batch 33100 Loss 1.2392,time:3400.047829389572sec\n",
      "Epoch 4 Batch 33200 Loss 1.2618,time:3410.601710319519sec\n",
      "Epoch 4 Batch 33300 Loss 1.1244,time:3420.7979300022125sec\n",
      "Epoch 4 Batch 33400 Loss 0.9074,time:3431.020124435425sec\n",
      "Epoch 4 Batch 33500 Loss 1.2478,time:3441.143404006958sec\n",
      "Epoch 4 Batch 33600 Loss 1.4573,time:3451.48473238945sec\n",
      "Epoch 4 Batch 33700 Loss 1.2060,time:3461.621015071869sec\n",
      "Epoch 4 Batch 33800 Loss 1.2677,time:3471.780796766281sec\n",
      "Epoch 4 Batch 33900 Loss 1.6109,time:3481.9704823493958sec\n",
      "Epoch 4 Batch 34000 Loss 1.2017,time:3492.2037875652313sec\n",
      "Epoch 4 Batch 34100 Loss 1.1861,time:3502.4530956745148sec\n",
      "Epoch 4 Batch 34200 Loss 1.6194,time:3512.710406064987sec\n",
      "Epoch 4 Batch 34300 Loss 1.0032,time:3522.9307782649994sec\n",
      "Epoch 4 Batch 34400 Loss 1.0090,time:3533.090694665909sec\n",
      "Epoch 4 Batch 34500 Loss 0.8196,time:3543.3970165252686sec\n",
      "Epoch 4 Batch 34600 Loss 1.4606,time:3553.6343216896057sec\n",
      "Epoch 4 Batch 34700 Loss 1.1784,time:3563.663579940796sec\n",
      "Epoch 4 Batch 34800 Loss 1.0599,time:3573.7844290733337sec\n",
      "Epoch 4 Batch 34900 Loss 1.0476,time:3583.8902428150177sec\n",
      "Epoch 4 Batch 35000 Loss 1.2467,time:3594.1865618228912sec\n",
      "Epoch 4 Batch 35100 Loss 1.1898,time:3604.3678543567657sec\n",
      "Epoch 4 Batch 35200 Loss 1.1006,time:3614.599666118622sec\n",
      "Epoch 4 Batch 35300 Loss 1.5677,time:3624.992197036743sec\n",
      "Epoch 4 Batch 35400 Loss 1.6943,time:3635.6095881462097sec\n",
      "Epoch 4 Batch 35500 Loss 0.8685,time:3646.134958267212sec\n",
      "Epoch 4 Batch 35600 Loss 1.5578,time:3656.5012924671173sec\n",
      "Epoch 4 Batch 35700 Loss 1.1715,time:3666.871129989624sec\n",
      "Epoch 4 Batch 35800 Loss 1.1132,time:3677.132673740387sec\n",
      "Epoch 4 Batch 35900 Loss 1.1876,time:3687.383982896805sec\n",
      "Epoch 4 Batch 36000 Loss 1.3572,time:3697.548271894455sec\n",
      "Epoch 4 Batch 36100 Loss 1.5191,time:3707.7885780334473sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 36200 Loss 1.2361,time:3717.96542263031sec\n",
      "Epoch 4 Batch 36300 Loss 1.2875,time:3728.167849779129sec\n",
      "Epoch 4 Batch 36400 Loss 0.9171,time:3738.53018283844sec\n",
      "Epoch 4 Batch 36500 Loss 1.3141,time:3748.654462814331sec\n",
      "Epoch 4 Batch 36600 Loss 1.2553,time:3758.7687401771545sec\n",
      "Epoch 4 Batch 36700 Loss 1.2460,time:3768.894343852997sec\n",
      "Epoch 4 Batch 36800 Loss 1.8255,time:3778.9744033813477sec\n",
      "Epoch 4 Batch 36900 Loss 1.4925,time:3789.120688676834sec\n",
      "Epoch 4 Batch 37000 Loss 1.4397,time:3799.3289873600006sec\n",
      "Epoch 4 Batch 37100 Loss 1.1105,time:3809.5152814388275sec\n",
      "Epoch 4 Batch 37200 Loss 0.9415,time:3819.68013715744sec\n",
      "Epoch 4 Batch 37300 Loss 1.3307,time:3830.0438120365143sec\n",
      "Epoch 4 Batch 37400 Loss 1.3997,time:3840.505168199539sec\n",
      "Epoch 4 Batch 37500 Loss 1.2181,time:3850.6324484348297sec\n",
      "Epoch 4 Batch 37600 Loss 0.9098,time:3860.7137184143066sec\n",
      "Epoch 4 Batch 37700 Loss 1.0226,time:3870.90509390831sec\n",
      "Epoch 4 Batch 37800 Loss 1.4151,time:3881.197747707367sec\n",
      "Epoch 4 Batch 37900 Loss 1.1352,time:3891.516070842743sec\n",
      "Epoch 4 Batch 38000 Loss 1.5204,time:3901.977427005768sec\n",
      "Epoch 4 Batch 38100 Loss 0.9853,time:3912.27574634552sec\n",
      "Epoch 4 Batch 38200 Loss 1.4884,time:3922.509383201599sec\n",
      "Epoch 4 Batch 38300 Loss 1.2535,time:3932.7851424217224sec\n",
      "Epoch 4 Batch 38400 Loss 1.4209,time:3942.9884402751923sec\n",
      "Epoch 4 Batch 38500 Loss 1.4290,time:3953.1927382946014sec\n",
      "Epoch 4 Batch 38600 Loss 0.8856,time:3963.5830783843994sec\n",
      "Epoch 4 Batch 38700 Loss 1.4464,time:3973.752119064331sec\n",
      "Epoch 4 Batch 38800 Loss 1.2545,time:3983.935201406479sec\n",
      "Epoch 4 Batch 38900 Loss 1.1872,time:3994.084487438202sec\n",
      "Epoch 4 Batch 39000 Loss 0.8862,time:4004.2087676525116sec\n",
      "Epoch 4 Batch 39100 Loss 0.9829,time:4014.3300466537476sec\n",
      "Epoch 4 Batch 39200 Loss 1.0546,time:4024.394114255905sec\n",
      "Epoch 4 Batch 39300 Loss 1.4221,time:4034.6348645687103sec\n",
      "Epoch 4 Batch 39400 Loss 0.9056,time:4044.7361397743225sec\n",
      "Epoch 4 Batch 39500 Loss 0.7999,time:4054.814409017563sec\n",
      "Epoch 4 Batch 39600 Loss 1.8484,time:4064.983699798584sec\n",
      "Epoch 4 Batch 39700 Loss 1.0155,time:4075.098966360092sec\n",
      "Epoch 4 Batch 39800 Loss 0.9013,time:4085.2703096866608sec\n",
      "Epoch 4 Batch 39900 Loss 1.2205,time:4095.494611978531sec\n",
      "Epoch 4 Batch 40000 Loss 1.0946,time:4105.628893852234sec\n",
      "Epoch 4 Batch 40100 Loss 1.4466,time:4115.769177913666sec\n",
      "Epoch 4 Batch 40200 Loss 1.0208,time:4125.928975105286sec\n",
      "Epoch 4 Batch 40300 Loss 0.8689,time:4136.20071721077sec\n",
      "Epoch 4 Batch 40400 Loss 1.2250,time:4146.445024967194sec\n",
      "Epoch 4 Batch 40500 Loss 1.7687,time:4156.815360546112sec\n",
      "Epoch 4 Batch 40600 Loss 0.8891,time:4167.171692371368sec\n",
      "Epoch 4 Batch 40700 Loss 1.2212,time:4177.396178007126sec\n",
      "Epoch 4 Batch 40800 Loss 1.4749,time:4187.568189620972sec\n",
      "Epoch 4 Batch 40900 Loss 1.4902,time:4197.699470996857sec\n",
      "Epoch 4 Batch 41000 Loss 1.1449,time:4207.8227508068085sec\n",
      "Epoch 4 Batch 41100 Loss 1.1968,time:4218.083061218262sec\n",
      "Epoch 4 Batch 41200 Loss 1.0630,time:4228.410614967346sec\n",
      "Epoch 4 Batch 41300 Loss 1.0212,time:4238.696931362152sec\n",
      "Epoch 4 Batch 41400 Loss 1.5626,time:4249.054264307022sec\n",
      "Epoch 4 Batch 41500 Loss 1.5624,time:4259.3365795612335sec\n",
      "Epoch 4 Batch 41600 Loss 1.2509,time:4269.658407688141sec\n",
      "Epoch 4 Batch 41700 Loss 1.5594,time:4279.931837558746sec\n",
      "Epoch 4 Batch 41800 Loss 1.3003,time:4290.14013671875sec\n",
      "Epoch 4 Batch 41900 Loss 1.3394,time:4300.502470493317sec\n",
      "Epoch 4 Batch 42000 Loss 1.3060,time:4310.667759656906sec\n",
      "Epoch 4 Batch 42100 Loss 1.1235,time:4320.9485449790955sec\n",
      "Epoch 4 Batch 42200 Loss 1.4299,time:4331.118704557419sec\n",
      "Epoch 4 Batch 42300 Loss 1.5803,time:4341.3820378780365sec\n",
      "Epoch 4 Batch 42400 Loss 1.1251,time:4351.566330909729sec\n",
      "Epoch 4 Batch 42500 Loss 0.9092,time:4361.833642959595sec\n",
      "Epoch 4 Batch 42600 Loss 1.4477,time:4372.323005437851sec\n",
      "Epoch 4 Batch 42700 Loss 1.6723,time:4382.613288402557sec\n",
      "Epoch 4 Batch 42800 Loss 1.1884,time:4392.847466945648sec\n",
      "Epoch 4 Batch 42900 Loss 1.0708,time:4403.102776527405sec\n",
      "Epoch 4 Batch 43000 Loss 1.1540,time:4413.339082241058sec\n",
      "Epoch 4 Batch 43100 Loss 0.9584,time:4423.5953912734985sec\n",
      "Epoch 4 Batch 43200 Loss 1.0227,time:4433.936435699463sec\n",
      "Epoch 4 Batch 43300 Loss 1.2735,time:4444.171741724014sec\n",
      "Epoch 4 Batch 43400 Loss 1.2882,time:4454.4710602760315sec\n",
      "Epoch 4 Batch 43500 Loss 1.6530,time:4464.658354043961sec\n",
      "Epoch 4 Batch 43600 Loss 1.0339,time:4474.792178630829sec\n",
      "Epoch 4 Batch 43700 Loss 1.1060,time:4485.02586555481sec\n",
      "Epoch 4 Batch 43800 Loss 1.2945,time:4495.1921553611755sec\n",
      "Epoch 4 Batch 43900 Loss 1.4059,time:4505.448464393616sec\n",
      "Epoch 4 Batch 44000 Loss 1.5012,time:4515.606752157211sec\n",
      "Epoch 4 Batch 44100 Loss 1.1677,time:4525.865325450897sec\n",
      "Epoch 4 Batch 44200 Loss 0.8410,time:4536.0407021045685sec\n",
      "Epoch 4 Batch 44300 Loss 0.7716,time:4546.303012371063sec\n",
      "Epoch 4 Batch 44400 Loss 0.9296,time:4556.638339757919sec\n",
      "Epoch 4 Batch 44500 Loss 0.8842,time:4566.90665268898sec\n",
      "Epoch 4 Batch 44600 Loss 1.2970,time:4577.193443775177sec\n",
      "Epoch 4 Batch 44700 Loss 1.8697,time:4587.530211925507sec\n",
      "Epoch 4 Batch 44800 Loss 1.7128,time:4597.853537321091sec\n",
      "Epoch 4 Batch 44900 Loss 1.1537,time:4608.110847234726sec\n",
      "Epoch 4 Batch 45000 Loss 1.5429,time:4618.278136730194sec\n",
      "Epoch 4 Batch 45100 Loss 1.1981,time:4628.506786346436sec\n",
      "Epoch 4 Batch 45200 Loss 1.4646,time:4638.897133588791sec\n",
      "Epoch 4 Batch 45300 Loss 1.1632,time:4649.199454069138sec\n",
      "Epoch 4 Batch 45400 Loss 1.1475,time:4659.424756288528sec\n",
      "Epoch 4 Batch 45500 Loss 1.2579,time:4669.703577280045sec\n",
      "Epoch 4 Batch 45600 Loss 1.0247,time:4679.935969352722sec\n",
      "Epoch 4 Batch 45700 Loss 1.6078,time:4690.190286874771sec\n",
      "Epoch 4 Batch 45800 Loss 1.0736,time:4700.358575582504sec\n",
      "Epoch 4 Batch 45900 Loss 1.0007,time:4710.487857103348sec\n",
      "Epoch 4 Batch 46000 Loss 1.3243,time:4720.688154459sec\n",
      "Epoch 4 Batch 46100 Loss 1.2588,time:4730.818408489227sec\n",
      "Epoch 4 Batch 46200 Loss 1.5942,time:4740.912681102753sec\n",
      "Epoch 4 Batch 46300 Loss 1.1539,time:4751.066968917847sec\n",
      "Epoch 4 Batch 46400 Loss 1.2065,time:4761.316276311874sec\n",
      "Epoch 4 Batch 46500 Loss 1.2975,time:4771.502077817917sec\n",
      "Epoch 4 Batch 46600 Loss 1.3951,time:4781.601492404938sec\n",
      "Epoch 4 Batch 46700 Loss 1.4078,time:4791.723785161972sec\n",
      "Epoch 4 Batch 46800 Loss 1.5476,time:4801.8890743255615sec\n",
      "Epoch 4 Batch 46900 Loss 1.4423,time:4812.302419424057sec\n",
      "Epoch 4 Batch 47000 Loss 1.2144,time:4822.564730644226sec\n",
      "Epoch 4 Batch 47100 Loss 1.3644,time:4832.939387083054sec\n",
      "Epoch 4 Batch 47200 Loss 1.1142,time:4843.350274324417sec\n",
      "Epoch 4 Batch 47300 Loss 1.0571,time:4853.834635734558sec\n",
      "Epoch 4 Batch 47400 Loss 1.4588,time:4864.203970909119sec\n",
      "Epoch 4 Batch 47500 Loss 0.9744,time:4874.580803871155sec\n",
      "Epoch 4 Batch 47600 Loss 0.7286,time:4884.941824436188sec\n",
      "Epoch 4 Batch 47700 Loss 1.2755,time:4895.348272800446sec\n",
      "Epoch 4 Batch 47800 Loss 1.4398,time:4905.642590999603sec\n",
      "Epoch 4 Batch 47900 Loss 0.8945,time:4915.92390704155sec\n",
      "Epoch 4 Batch 48000 Loss 1.5792,time:4926.119202613831sec\n",
      "Epoch 4 Batch 48100 Loss 1.4190,time:4936.372504711151sec\n",
      "Epoch 4 Batch 48200 Loss 1.0505,time:4946.559090852737sec\n",
      "Epoch 4 Batch 48300 Loss 1.1421,time:4956.670367717743sec\n",
      "Epoch 4 Batch 48400 Loss 0.8291,time:4966.864663600922sec\n",
      "Epoch 4 Batch 48500 Loss 1.6833,time:4977.151981115341sec\n",
      "Epoch 4 Batch 48600 Loss 1.3194,time:4987.356642246246sec\n",
      "Epoch 4 Batch 48700 Loss 1.2239,time:4997.490870952606sec\n",
      "Epoch 4 Batch 48800 Loss 1.2929,time:5007.693168401718sec\n",
      "Epoch 4 Batch 48900 Loss 1.4963,time:5017.821448564529sec\n",
      "Epoch 4 Batch 49000 Loss 1.3022,time:5027.893717050552sec\n",
      "Epoch 4 Batch 49100 Loss 0.8302,time:5038.059731721878sec\n",
      "Epoch 4 Batch 49200 Loss 0.9664,time:5048.234814882278sec\n",
      "Epoch 4 Batch 49300 Loss 1.0710,time:5058.337090015411sec\n",
      "Epoch 4 Batch 49400 Loss 1.4753,time:5068.402356624603sec\n",
      "Epoch 4 Batch 49500 Loss 1.0192,time:5078.471623420715sec\n",
      "Epoch 4 Batch 49600 Loss 1.5577,time:5088.646896600723sec\n",
      "Epoch 4 Batch 49700 Loss 0.8607,time:5098.762002944946sec\n",
      "Epoch 4 Batch 49800 Loss 1.2299,time:5108.955298423767sec\n",
      "Epoch 4 Batch 49900 Loss 0.8768,time:5119.199605703354sec\n",
      "Epoch 4 Batch 50000 Loss 1.5173,time:5129.411905050278sec\n",
      "Epoch 4 Batch 50100 Loss 1.6135,time:5139.779747247696sec\n",
      "Epoch 4 Batch 50200 Loss 1.3340,time:5150.039309978485sec\n",
      "Epoch 4 Batch 50300 Loss 1.1437,time:5160.299620628357sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 50400 Loss 1.2195,time:5170.564932107925sec\n",
      "Epoch 4 Batch 50500 Loss 1.5545,time:5180.709216594696sec\n",
      "Epoch 4 Batch 50600 Loss 1.3277,time:5190.877970457077sec\n",
      "Epoch 4 Batch 50700 Loss 1.2385,time:5201.036976337433sec\n",
      "Epoch 4 Batch 50800 Loss 1.2847,time:5211.192263126373sec\n",
      "Epoch 4 Batch 50900 Loss 1.0386,time:5221.321544408798sec\n",
      "Epoch 4 Batch 51000 Loss 1.2244,time:5231.660872459412sec\n",
      "Epoch 4 Batch 51100 Loss 1.0739,time:5241.763639688492sec\n",
      "Epoch 4 Batch 51200 Loss 1.7261,time:5251.941028356552sec\n",
      "Epoch 4 Batch 51300 Loss 1.7894,time:5262.072309494019sec\n",
      "Epoch 4 Batch 51400 Loss 0.7765,time:5272.252602100372sec\n",
      "Epoch 4 Batch 51500 Loss 1.2217,time:5282.347875833511sec\n",
      "Epoch 4 Batch 51600 Loss 1.3614,time:5292.5284214019775sec\n",
      "Epoch 4 Batch 51700 Loss 1.5371,time:5302.638924598694sec\n",
      "Epoch 4 Batch 51800 Loss 1.4904,time:5312.865227460861sec\n",
      "Epoch 4 Batch 51900 Loss 0.9629,time:5323.20755648613sec\n",
      "Epoch 4 Batch 52000 Loss 0.9238,time:5333.609899044037sec\n",
      "Epoch 4 Batch 52100 Loss 1.8313,time:5343.953445672989sec\n",
      "Epoch 4 Batch 52200 Loss 1.1256,time:5354.30207324028sec\n",
      "Epoch 4 Batch 52300 Loss 1.0202,time:5364.355337142944sec\n",
      "Epoch 4 Batch 52400 Loss 1.5097,time:5374.492620229721sec\n",
      "Epoch 4 Batch 52500 Loss 1.2482,time:5384.800941228867sec\n",
      "Epoch 4 Batch 52600 Loss 1.3831,time:5395.0678124427795sec\n",
      "Epoch 4 Batch 52700 Loss 1.3536,time:5405.234975099564sec\n",
      "Epoch 4 Batch 52800 Loss 1.4169,time:5415.711602687836sec\n",
      "Epoch 4 Batch 52900 Loss 1.7925,time:5425.928903102875sec\n",
      "Epoch 4 Batch 53000 Loss 0.8786,time:5436.17821097374sec\n",
      "Epoch 4 Batch 53100 Loss 1.0190,time:5446.379449367523sec\n",
      "Epoch 4 Batch 53200 Loss 1.5255,time:5456.688651800156sec\n",
      "Epoch 4 Batch 53300 Loss 1.4425,time:5466.887948274612sec\n",
      "Epoch 4 Batch 53400 Loss 1.2302,time:5477.042235374451sec\n",
      "Epoch 4 Batch 53500 Loss 2.0495,time:5487.160514116287sec\n",
      "Epoch 4 Batch 53600 Loss 1.3376,time:5497.304797887802sec\n",
      "Epoch 4 Batch 53700 Loss 1.5815,time:5507.588942289352sec\n",
      "Epoch 4 Batch 53800 Loss 1.0920,time:5517.765415668488sec\n",
      "Epoch 4 Batch 53900 Loss 1.2189,time:5527.910700559616sec\n",
      "Epoch 4 Batch 54000 Loss 1.3854,time:5538.143005132675sec\n",
      "Epoch 4 Batch 54100 Loss 1.7663,time:5548.252281665802sec\n",
      "Epoch 4 Batch 54200 Loss 1.2585,time:5558.4435777664185sec\n",
      "Epoch 4 Batch 54300 Loss 0.8613,time:5568.603547096252sec\n",
      "Epoch 4 Batch 54400 Loss 1.1415,time:5578.686818122864sec\n",
      "Epoch 4 Batch 54500 Loss 1.0495,time:5588.8941168785095sec\n",
      "Epoch 4 Batch 54600 Loss 1.3263,time:5599.162429571152sec\n",
      "Epoch 4 Batch 54700 Loss 1.0948,time:5609.247624397278sec\n",
      "Epoch 4 Batch 54800 Loss 1.5059,time:5619.4973793029785sec\n",
      "Epoch 4 Batch 54900 Loss 1.2877,time:5629.6436648368835sec\n",
      "Epoch 4 Batch 55000 Loss 1.1733,time:5639.908976078033sec\n",
      "Epoch 4 Batch 55100 Loss 1.4210,time:5650.126278162003sec\n",
      "Epoch 4 Batch 55200 Loss 1.5612,time:5660.409662246704sec\n",
      "Epoch 4 Batch 55300 Loss 1.8491,time:5670.59942483902sec\n",
      "Epoch 4 Batch 55400 Loss 1.1906,time:5680.736708164215sec\n",
      "Epoch 4 Batch 55500 Loss 1.0587,time:5690.9109988212585sec\n",
      "Epoch 4 Batch 55600 Loss 1.1981,time:5700.985267877579sec\n",
      "Epoch 4 Batch 55700 Loss 0.9661,time:5711.093909025192sec\n",
      "Epoch 4 Batch 55800 Loss 1.0695,time:5721.499603033066sec\n",
      "Epoch 4 Batch 55900 Loss 0.8625,time:5731.665892362595sec\n",
      "Epoch 4 Batch 56000 Loss 1.7100,time:5741.9572105407715sec\n",
      "Epoch 4 Batch 56100 Loss 1.6775,time:5752.0664875507355sec\n",
      "Epoch 4 Batch 56200 Loss 1.3107,time:5762.297343015671sec\n",
      "Epoch 4 Batch 56300 Loss 1.0841,time:5772.5668840408325sec\n",
      "Epoch 4 Batch 56400 Loss 1.3397,time:5782.9032118320465sec\n",
      "Epoch 4 Batch 56500 Loss 1.8936,time:5793.235538482666sec\n",
      "Epoch 4 Batch 56600 Loss 1.1423,time:5803.490849018097sec\n",
      "Epoch 4 Batch 56700 Loss 1.1958,time:5813.762734413147sec\n",
      "Epoch 4 Batch 56800 Loss 1.2915,time:5823.893419742584sec\n",
      "Epoch 4 Batch 56900 Loss 1.5526,time:5834.086715936661sec\n",
      "Epoch 4 Batch 57000 Loss 1.3679,time:5844.182989835739sec\n",
      "Epoch 4 Batch 57100 Loss 1.1001,time:5854.388287782669sec\n",
      "Epoch 4 Batch 57200 Loss 0.8648,time:5864.626228570938sec\n",
      "Epoch 4 Batch 57300 Loss 1.5094,time:5874.764036417007sec\n",
      "Epoch 4 Batch 57400 Loss 1.2280,time:5884.9655249118805sec\n",
      "Epoch 4 Batch 57500 Loss 1.1027,time:5895.257859706879sec\n",
      "Epoch 4 Batch 57600 Loss 1.0570,time:5905.447153806686sec\n",
      "Epoch 4 Batch 57700 Loss 1.1690,time:5915.619445323944sec\n",
      "Epoch 4 Batch 57800 Loss 1.6451,time:5925.7717316150665sec\n",
      "Epoch 4 Batch 57900 Loss 1.1782,time:5936.129357337952sec\n",
      "Epoch 4 Batch 58000 Loss 1.0150,time:5946.3136603832245sec\n",
      "Epoch 4 Batch 58100 Loss 1.6758,time:5956.529960393906sec\n",
      "Epoch 4 Batch 58200 Loss 1.3721,time:5966.627234220505sec\n",
      "Epoch 4 Batch 58300 Loss 0.8765,time:5976.7405116558075sec\n",
      "Epoch 4 Batch 58400 Loss 1.7106,time:5986.883969545364sec\n",
      "Epoch 4 Batch 58500 Loss 1.3512,time:5997.050883293152sec\n",
      "Epoch 4 Batch 58600 Loss 1.3381,time:6007.05913734436sec\n",
      "Epoch 4 Batch 58700 Loss 1.4835,time:6017.284440755844sec\n",
      "Epoch 4 Batch 58800 Loss 1.2317,time:6027.41472196579sec\n",
      "Epoch 4 Batch 58900 Loss 1.0770,time:6037.656505584717sec\n",
      "Epoch 4 Batch 59000 Loss 1.2262,time:6047.961248397827sec\n",
      "Epoch 4 Batch 59100 Loss 1.4379,time:6058.212557077408sec\n",
      "Epoch 4 Batch 59200 Loss 1.6807,time:6068.437859773636sec\n",
      "Epoch 4 Batch 59300 Loss 1.2353,time:6078.929222822189sec\n",
      "Epoch 4 Batch 59400 Loss 1.5313,time:6089.256448507309sec\n",
      "Epoch 4 Batch 59500 Loss 0.9390,time:6099.446897268295sec\n",
      "Epoch 4 Batch 59600 Loss 1.0053,time:6109.544171094894sec\n",
      "Epoch 4 Batch 59700 Loss 1.5177,time:6119.739466667175sec\n",
      "Epoch 4 Batch 59800 Loss 0.9201,time:6129.98677444458sec\n",
      "Epoch 4 Batch 59900 Loss 1.4150,time:6140.252166986465sec\n",
      "Epoch 4 Batch 60000 Loss 1.2159,time:6150.576167345047sec\n",
      "Epoch 4 Batch 60100 Loss 1.5385,time:6160.837478160858sec\n",
      "Epoch 4 Batch 60200 Loss 1.5482,time:6171.138797998428sec\n",
      "Epoch 4 Batch 60300 Loss 1.3452,time:6181.3010866642sec\n",
      "Epoch 4 Batch 60400 Loss 1.3660,time:6191.45144367218sec\n",
      "Epoch 4 Batch 60500 Loss 0.6853,time:6201.640701055527sec\n",
      "Epoch 4 Batch 60600 Loss 1.4870,time:6211.79198718071sec\n",
      "Epoch 4 Batch 60700 Loss 1.3578,time:6222.076303243637sec\n",
      "Epoch 4 Batch 60800 Loss 2.0068,time:6232.257596492767sec\n",
      "Epoch 4 Batch 60900 Loss 0.9175,time:6242.73112988472sec\n",
      "Epoch 4 Batch 61000 Loss 1.0278,time:6252.868873596191sec\n",
      "Epoch 4 Batch 61100 Loss 1.0946,time:6262.909319400787sec\n",
      "Epoch 4 Batch 61200 Loss 1.3213,time:6273.087611436844sec\n",
      "Epoch 4 Batch 61300 Loss 1.3342,time:6283.25090098381sec\n",
      "Epoch 4 Batch 61400 Loss 1.4283,time:6293.464200496674sec\n",
      "Epoch 4 Batch 61500 Loss 0.9587,time:6303.607053279877sec\n",
      "Epoch 4 Batch 61600 Loss 1.1063,time:6313.692235946655sec\n",
      "Epoch 4 Batch 61700 Loss 1.3930,time:6324.012560844421sec\n",
      "Epoch 4 Batch 61800 Loss 1.4826,time:6334.278872728348sec\n",
      "Epoch 4 Batch 61900 Loss 1.2153,time:6344.519179105759sec\n",
      "Epoch 4 Batch 62000 Loss 0.9625,time:6354.7872405052185sec\n",
      "Epoch 4 Batch 62100 Loss 1.1526,time:6364.950911998749sec\n",
      "Epoch 4 Batch 62200 Loss 0.8884,time:6375.231227636337sec\n",
      "Epoch 4 Batch 62300 Loss 1.2503,time:6385.511542797089sec\n",
      "Epoch 4 Batch 62400 Loss 1.2473,time:6395.767852544785sec\n",
      "Epoch 4 Batch 62500 Loss 1.6969,time:6406.035493135452sec\n",
      "Epoch 4 Batch 62600 Loss 0.8837,time:6416.27770614624sec\n",
      "Epoch 4 Batch 62700 Loss 1.0898,time:6426.558021306992sec\n",
      "Epoch 4 Batch 62800 Loss 1.2480,time:6436.6712992191315sec\n",
      "Epoch 4 Batch 62900 Loss 1.1589,time:6446.798579692841sec\n",
      "Epoch 4 Batch 63000 Loss 1.0182,time:6456.900297880173sec\n",
      "Epoch 4 Batch 63100 Loss 0.9864,time:6467.142288208008sec\n",
      "Epoch 4 Batch 63200 Loss 0.7740,time:6477.294574260712sec\n",
      "Epoch 4 Batch 63300 Loss 1.5203,time:6487.336835861206sec\n",
      "Epoch 4 Batch 63400 Loss 0.9984,time:6497.369094371796sec\n",
      "Epoch 4 Batch 63500 Loss 0.6517,time:6507.519021034241sec\n",
      "Epoch 4 Batch 63600 Loss 1.2072,time:6517.722519636154sec\n",
      "Epoch 4 Batch 63700 Loss 1.2541,time:6528.00183391571sec\n",
      "Epoch 4 Batch 63800 Loss 1.5186,time:6538.187128305435sec\n",
      "Epoch 4 Batch 63900 Loss 1.5072,time:6548.324411392212sec\n",
      "Epoch 4 Batch 64000 Loss 1.0769,time:6558.449741840363sec\n",
      "Epoch 4 Batch 64100 Loss 1.7185,time:6568.592340230942sec\n",
      "Epoch 4 Batch 64200 Loss 1.7308,time:6578.756629228592sec\n",
      "Epoch 4 Batch 64300 Loss 1.0862,time:6588.9169182777405sec\n",
      "Epoch 4 Batch 64400 Loss 1.2373,time:6599.064202547073sec\n",
      "Epoch 4 Batch 64500 Loss 1.5108,time:6609.143291473389sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 64600 Loss 1.3983,time:6619.294127702713sec\n",
      "Epoch 4 Batch 64700 Loss 1.2608,time:6629.645458459854sec\n",
      "Epoch 4 Batch 64800 Loss 0.9894,time:6640.012794017792sec\n",
      "Epoch 4 Batch 64900 Loss 1.1133,time:6650.363123893738sec\n",
      "Epoch 4 Batch 65000 Loss 1.0852,time:6660.735538482666sec\n",
      "Epoch 4 Batch 65100 Loss 1.1846,time:6671.13560962677sec\n",
      "Epoch 4 Batch 65200 Loss 0.9113,time:6681.465936183929sec\n",
      "Epoch 4 Batch 65300 Loss 1.3426,time:6691.699240922928sec\n",
      "Epoch 4 Batch 65400 Loss 1.4482,time:6701.869530916214sec\n",
      "Epoch 4 Batch 65500 Loss 1.0983,time:6711.994402170181sec\n",
      "Epoch 4 Batch 65600 Loss 1.2144,time:6722.06103014946sec\n",
      "Epoch 4 Batch 65700 Loss 1.6906,time:6732.271328449249sec\n",
      "Epoch 4 Batch 65800 Loss 1.1276,time:6742.449620723724sec\n",
      "Epoch 4 Batch 65900 Loss 1.0921,time:6752.496883392334sec\n",
      "Epoch 4 Batch 66000 Loss 1.6489,time:6762.5503969192505sec\n",
      "Epoch 4 Batch 66100 Loss 1.1071,time:6772.6416873931885sec\n",
      "Epoch 4 Batch 66200 Loss 1.0944,time:6782.676947116852sec\n",
      "Epoch 4 Batch 66300 Loss 0.9999,time:6792.91225194931sec\n",
      "Epoch 4 Batch 66400 Loss 0.9717,time:6803.142555952072sec\n",
      "Epoch 4 Batch 66500 Loss 0.9296,time:6813.284977197647sec\n",
      "Epoch 4 Batch 66600 Loss 1.1030,time:6823.403115987778sec\n",
      "Epoch 4 Batch 66700 Loss 1.3093,time:6833.555402994156sec\n",
      "Epoch 4 Batch 66800 Loss 1.3191,time:6843.726694107056sec\n",
      "Epoch 4 Batch 66900 Loss 0.7781,time:6853.842971563339sec\n",
      "Epoch 4 Batch 67000 Loss 1.1138,time:6863.929373502731sec\n",
      "Epoch 4 Batch 67100 Loss 1.3584,time:6874.054429769516sec\n",
      "Epoch 4 Batch 67200 Loss 1.0632,time:6884.148361444473sec\n",
      "Epoch 4 Batch 67300 Loss 0.6523,time:6894.433677911758sec\n",
      "Epoch 4 Batch 67400 Loss 1.0884,time:6904.661980867386sec\n",
      "Epoch 4 Batch 67500 Loss 1.1720,time:6914.951297521591sec\n",
      "Epoch 4 Batch 67600 Loss 1.1386,time:6925.222734928131sec\n",
      "Epoch 4 Batch 67700 Loss 1.2474,time:6935.377679109573sec\n",
      "Epoch 4 Batch 67800 Loss 1.1925,time:6945.617985010147sec\n",
      "Epoch 4 Batch 67900 Loss 0.8143,time:6955.836285829544sec\n",
      "Epoch 4 Batch 68000 Loss 1.6414,time:6965.984571695328sec\n",
      "Epoch 4 Batch 68100 Loss 1.2965,time:6976.176363706589sec\n",
      "Epoch 4 Batch 68200 Loss 1.4442,time:6986.342209100723sec\n",
      "Epoch 4 Batch 68300 Loss 1.2416,time:6996.491494417191sec\n",
      "Epoch 4 Batch 68400 Loss 1.3459,time:7006.602771520615sec\n",
      "Epoch 4 Batch 68500 Loss 1.8637,time:7016.666038036346sec\n",
      "Epoch 4 Batch 68600 Loss 1.5512,time:7026.787530660629sec\n",
      "Epoch 4 Batch 68700 Loss 1.0020,time:7036.977952957153sec\n",
      "Epoch 4 Batch 68800 Loss 0.8573,time:7047.157245397568sec\n",
      "Epoch 4 Batch 68900 Loss 1.2298,time:7057.277524471283sec\n",
      "Epoch 4 Batch 69000 Loss 1.3669,time:7067.42480969429sec\n",
      "Epoch 4 Batch 69100 Loss 1.4489,time:7077.596330165863sec\n",
      "Epoch 4 Batch 69200 Loss 0.7868,time:7087.972152709961sec\n",
      "Epoch 4 Batch 69300 Loss 1.2579,time:7098.287476062775sec\n",
      "Epoch 4 Batch 69400 Loss 1.5553,time:7108.707823038101sec\n",
      "Epoch 4 Batch 69500 Loss 1.2645,time:7119.027146816254sec\n",
      "Epoch 4 Batch 69600 Loss 1.5685,time:7129.360037565231sec\n",
      "Epoch 4 Batch 69700 Loss 1.2621,time:7139.660997390747sec\n",
      "Epoch 4 Batch 69800 Loss 1.4481,time:7149.930309534073sec\n",
      "Epoch 4 Batch 69900 Loss 1.1594,time:7160.123605728149sec\n",
      "Epoch 4 Batch 70000 Loss 1.5481,time:7170.442929267883sec\n",
      "Epoch 4 Batch 70100 Loss 1.5817,time:7180.633402347565sec\n",
      "Epoch 4 Batch 70200 Loss 1.0068,time:7190.71356678009sec\n",
      "Epoch 4 Batch 70300 Loss 1.9263,time:7200.837850093842sec\n",
      "Epoch 4 Batch 70400 Loss 1.3489,time:7210.963130712509sec\n",
      "Epoch 4 Batch 70500 Loss 0.9849,time:7221.099413156509sec\n",
      "Epoch 4 Batch 70600 Loss 1.1862,time:7231.346720695496sec\n",
      "Epoch 4 Batch 70700 Loss 0.9970,time:7241.923905611038sec\n",
      "Epoch 4 Batch 70800 Loss 1.4851,time:7252.264938831329sec\n",
      "Epoch 4 Batch 70900 Loss 1.1343,time:7262.472237586975sec\n",
      "Epoch 4 Batch 71000 Loss 0.9758,time:7272.646528482437sec\n",
      "Epoch 4 Batch 71100 Loss 1.4530,time:7282.8298218250275sec\n",
      "Epoch 4 Batch 71200 Loss 1.6776,time:7292.987185716629sec\n",
      "Epoch 4 Batch 71300 Loss 0.9433,time:7302.998767614365sec\n",
      "Epoch 4 Batch 71400 Loss 0.9525,time:7313.088040351868sec\n",
      "Epoch 4 Batch 71500 Loss 1.4958,time:7323.152305841446sec\n",
      "Epoch 4 Batch 71600 Loss 1.3208,time:7333.4316210746765sec\n",
      "Epoch 4 Batch 71700 Loss 0.8523,time:7343.588168859482sec\n",
      "Epoch 4 Batch 71800 Loss 1.2741,time:7353.665567159653sec\n",
      "Epoch 4 Batch 71900 Loss 0.9632,time:7363.773844003677sec\n",
      "Epoch 4 Batch 72000 Loss 1.6167,time:7374.147179841995sec\n",
      "Epoch 4 Batch 72100 Loss 1.3107,time:7384.34547662735sec\n",
      "Epoch 4 Batch 72200 Loss 0.9767,time:7394.576902151108sec\n",
      "Epoch 4 Batch 72300 Loss 1.2155,time:7404.7649528980255sec\n",
      "Epoch 4 Batch 72400 Loss 1.6503,time:7414.942245006561sec\n",
      "Epoch 4 Batch 72500 Loss 1.3267,time:7425.199555397034sec\n",
      "Epoch 4 Batch 72600 Loss 0.8119,time:7435.359843254089sec\n",
      "Epoch 4 Batch 72700 Loss 1.2709,time:7445.585483551025sec\n",
      "Epoch 4 Loss 1.2769\n",
      "Time taken for 1 epoch 7453.645253419876 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the epochs for training\n",
    "EPOCHS = 4\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f},time:{}sec'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy(),(time.time() - start)))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/sentiment-analysis\\ckpt-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x22e5ff28040>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inp, enc_hidden):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp, targ) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions = label_decode(label_encoder, predictions)\n",
    "#     predictions[np.where(predictions < 0.5)] = 0\n",
    "#     predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_pred, attention_weights = evaluate(vali_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vali = label_decode(label_encoder, y_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5432323530725183\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', (vali_pred == y_vali).sum() / len(y_vali))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'sadness', 'joy', ..., 'disgust', 'sadness', 'sadness'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions = label_decode(label_encoder, predictions)\n",
    "#     predictions[np.where(predictions < 0.5)] = 0\n",
    "#     predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred, attention_weights = evaluate_test(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans  = pd.DataFrame(test_pred)\n",
    "df_test['emotion']  = df_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159364</th>\n",
       "      <td>0x246dcf</td>\n",
       "      <td>test</td>\n",
       "      <td>Does using \"just\" in email make you sound less...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0x246dcf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233130</th>\n",
       "      <td>0x1f8151</td>\n",
       "      <td>test</td>\n",
       "      <td>When one of your exes is a new hire at your wo...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0x1f8151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311720</th>\n",
       "      <td>0x31c2fc</td>\n",
       "      <td>test</td>\n",
       "      <td>Just came off a LIVE BROADCAST on WEALTH CREAT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x31c2fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428024</th>\n",
       "      <td>0x2bead5</td>\n",
       "      <td>test</td>\n",
       "      <td>@NerdyWonka Probably the children of all the b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x2bead5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424428</th>\n",
       "      <td>0x29d045</td>\n",
       "      <td>test</td>\n",
       "      <td>Am I on something or were there actually cloud...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x29d045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233327</th>\n",
       "      <td>0x2c89e1</td>\n",
       "      <td>test</td>\n",
       "      <td>Feels like I should have been further but my f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x2c89e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776233</th>\n",
       "      <td>0x359ea7</td>\n",
       "      <td>test</td>\n",
       "      <td>@andybevanitv Wow. Just wow. We can do this. &lt;LH&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x359ea7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222789</th>\n",
       "      <td>0x2952f5</td>\n",
       "      <td>test</td>\n",
       "      <td>The car in front of me at On The Grind just pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x2952f5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704139</th>\n",
       "      <td>0x21bec9</td>\n",
       "      <td>test</td>\n",
       "      <td>I don't understand why people still try to giv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x21bec9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948549</th>\n",
       "      <td>0x376e20</td>\n",
       "      <td>test</td>\n",
       "      <td>@GOtransit As a friend once said,â€itâ€™s just no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x376e20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "159364   0x246dcf           test   \n",
       "233130   0x1f8151           test   \n",
       "1311720  0x31c2fc           test   \n",
       "428024   0x2bead5           test   \n",
       "424428   0x29d045           test   \n",
       "...           ...            ...   \n",
       "1233327  0x2c89e1           test   \n",
       "776233   0x359ea7           test   \n",
       "1222789  0x2952f5           test   \n",
       "1704139  0x21bec9           test   \n",
       "948549   0x376e20           test   \n",
       "\n",
       "                                                      text       emotion  \\\n",
       "159364   Does using \"just\" in email make you sound less...           joy   \n",
       "233130   When one of your exes is a new hire at your wo...  anticipation   \n",
       "1311720  Just came off a LIVE BROADCAST on WEALTH CREAT...           NaN   \n",
       "428024   @NerdyWonka Probably the children of all the b...           NaN   \n",
       "424428   Am I on something or were there actually cloud...           NaN   \n",
       "...                                                    ...           ...   \n",
       "1233327  Feels like I should have been further but my f...           NaN   \n",
       "776233   @andybevanitv Wow. Just wow. We can do this. <LH>           NaN   \n",
       "1222789  The car in front of me at On The Grind just pa...           NaN   \n",
       "1704139  I don't understand why people still try to giv...           NaN   \n",
       "948549   @GOtransit As a friend once said,â€itâ€™s just no...           NaN   \n",
       "\n",
       "               id  \n",
       "159364   0x246dcf  \n",
       "233130   0x1f8151  \n",
       "1311720  0x31c2fc  \n",
       "428024   0x2bead5  \n",
       "424428   0x29d045  \n",
       "...           ...  \n",
       "1233327  0x2c89e1  \n",
       "776233   0x359ea7  \n",
       "1222789  0x2952f5  \n",
       "1704139  0x21bec9  \n",
       "948549   0x376e20  \n",
       "\n",
       "[411972 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['id'] = df_test['tweet_id']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = df_test[['id','emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159364</th>\n",
       "      <td>0x246dcf</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233130</th>\n",
       "      <td>0x1f8151</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311720</th>\n",
       "      <td>0x31c2fc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428024</th>\n",
       "      <td>0x2bead5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424428</th>\n",
       "      <td>0x29d045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233327</th>\n",
       "      <td>0x2c89e1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776233</th>\n",
       "      <td>0x359ea7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222789</th>\n",
       "      <td>0x2952f5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704139</th>\n",
       "      <td>0x21bec9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948549</th>\n",
       "      <td>0x376e20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       emotion\n",
       "159364   0x246dcf           joy\n",
       "233130   0x1f8151  anticipation\n",
       "1311720  0x31c2fc           NaN\n",
       "428024   0x2bead5           NaN\n",
       "424428   0x29d045           NaN\n",
       "...           ...           ...\n",
       "1233327  0x2c89e1           NaN\n",
       "776233   0x359ea7           NaN\n",
       "1222789  0x2952f5           NaN\n",
       "1704139  0x21bec9           NaN\n",
       "948549   0x376e20           NaN\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd.to_csv('pred.csv',sep=\",\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "the best result is from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (1455563, 4)\n",
      "Shape of Testing df:  (411972, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", df_train.shape)\n",
    "print(\"Shape of Testing df: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 554582     disgust\n",
      "1035980        joy\n",
      "1384065       fear\n",
      "64845        trust\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1455563,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the positive with 1, replace the negative with 0\n",
    "y = df_train['emotion']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y[0:4])\n",
    "print('\\ny_train.shape: ', y.shape)\n",
    "y = label_encode(label_encoder, y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=['anger','anticipation','disgust','fear','joy','sadness','surprise','trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Does anyone in #London actually #work? all I see is another #protest every day? #wtf?',\n",
       " 'labels': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# dataset = load_dataset('text', data_files={'train': train, 'test': test})\n",
    "dataset = Dataset.from_dict({'text': df_train['text'], 'labels':  y})\n",
    "dataset_test = Dataset.from_dict({'text': df_test['text']})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@dahvieinsanity music - still the best cure for writers block since they invented Irish coffee &lt;3 <LH>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seperate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7896612304122aca0ec6f7ee69f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1456 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfd910b34e244eb97fdd0c55ec08cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_fn, batched=True)\n",
    "tokenized_dataset_test = dataset_test.map(preprocess_fn, batched=True)\n",
    "train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.3).values()\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)\n",
    "# train_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "# eval_dataset = tokenized## tokenize_dataset.shuffle(seed=42).select(range(100))\n",
    "test_dataset = tokenized_dataset_test\n",
    "# train_dataset['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.3).values()\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0e7ddd50584586adccea9d2bc280ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1456 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5e857e510342e8b320304515d97e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_dataset = dataset.map(preprocess_fn, batched=True)\n",
    "tokenized_dataset_test = dataset_test.map(preprocess_fn, batched=True)\n",
    "train_dataset, eval_dataset = tokenized_dataset.train_test_split(test_size=0.5).values()\n",
    "\n",
    "train_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "eval_dataset = tokenized_dataset.shuffle(seed=42).select(range(100))\n",
    "test_dataset = tokenized_dataset_test\n",
    "# train_dataset['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special tokens\n",
    "tokenizer(\"[CLS][SEP][UNK][PAD][MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model ## load model= AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base',num_labels = 8)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./test_trainer/checkpoint-70000')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score \n",
    "\n",
    "# metric = load_metric(\"glue\", \"mrpc\")\n",
    "# accuracy = load_metric(\"accuracy\")\n",
    "# f1_score = load_metric(\"f1\")\n",
    "# precision = load_metric(\"precision\")\n",
    "# recall = load_metric(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "#     predictions = logits\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels = np.argmax(labels,axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_pred=predictions, y_true=labels),\n",
    "        \"f1\": f1_score(y_pred=predictions, y_true=labels, average='macro' ),\n",
    "        \"precision\": precision_score(y_pred=predictions, y_true=labels, average='weighted'),\n",
    "        \"recall\": recall_score(y_pred=predictions, y_true=labels, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=5000,\n",
       "evaluation_strategy=IntervalStrategy.STEPS,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=True,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=test_trainer\\runs\\Jan07_07-59-47_DESKTOP-L6L4G5P,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=5000,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=f1,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "output_dir=test_trainer,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=test_trainer,\n",
       "save_on_each_node=False,\n",
       "save_steps=5000,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.5,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy='steps', # epoch\n",
    "    eval_steps=5000,\n",
    "    save_steps=5000,\n",
    "    logging_steps=5000,\n",
    "    dataloader_drop_last =False,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'f1',\n",
    "    weight_decay = 0.5\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics,callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 1018894\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63681\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='63681' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/63681 3:34:09 < 7:47:46, 1.56 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.160492</td>\n",
       "      <td>0.730858</td>\n",
       "      <td>0.665050</td>\n",
       "      <td>0.732754</td>\n",
       "      <td>0.730858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.171443</td>\n",
       "      <td>0.708308</td>\n",
       "      <td>0.639632</td>\n",
       "      <td>0.718370</td>\n",
       "      <td>0.708308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.689055</td>\n",
       "      <td>0.613497</td>\n",
       "      <td>0.700689</td>\n",
       "      <td>0.689055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>0.179649</td>\n",
       "      <td>0.689889</td>\n",
       "      <td>0.611178</td>\n",
       "      <td>0.693682</td>\n",
       "      <td>0.689889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 436669\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer\\checkpoint-5000\n",
      "Configuration saved in test_trainer\\checkpoint-5000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 436669\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer\\checkpoint-10000\n",
      "Configuration saved in test_trainer\\checkpoint-10000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 436669\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer\\checkpoint-15000\n",
      "Configuration saved in test_trainer\\checkpoint-15000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 436669\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer\\checkpoint-20000\n",
      "Configuration saved in test_trainer\\checkpoint-20000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from test_trainer\\checkpoint-5000 (score: 0.6650504435271739).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20000, training_loss=0.17536494140625, metrics={'train_runtime': 12849.9916, 'train_samples_per_second': 79.291, 'train_steps_per_second': 4.956, 'total_flos': 2.105001836544e+16, 'train_loss': 0.17536494140625, 'epoch': 0.31})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 727782\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142470' max='90973' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90973/90973 1:31:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14616753160953522,\n",
       " 'eval_accuracy': 0.7743843623502642,\n",
       " 'eval_f1': 0.7249269794314556,\n",
       " 'eval_precision': 0.7885322978807812,\n",
       " 'eval_recall': 0.7743843623502642,\n",
       " 'eval_runtime': 3522.4761,\n",
       " 'eval_samples_per_second': 206.611,\n",
       " 'eval_steps_per_second': 25.826,\n",
       " 'epoch': 0.88}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict and output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 411972\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51497' max='51497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51497/51497 35:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits, tweet_id, metrics = trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = label_decode(label_encoder, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trust', 'joy', 'joy', 'trust', 'trust', 'trust', 'sadness', 'joy',\n",
       "       'joy', 'sadness'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2a370d</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x233082</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x326f71</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x38c4a1</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x270864</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x28336f</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2d34e6</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x2d5f67</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x1df874</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x1c8e54</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       emotion\n",
       "0       0x2a370d           joy\n",
       "1       0x233082           joy\n",
       "2       0x326f71           joy\n",
       "3       0x38c4a1  anticipation\n",
       "4       0x270864       disgust\n",
       "...          ...           ...\n",
       "411967  0x28336f         trust\n",
       "411968  0x2d34e6       sadness\n",
       "411969  0x2d5f67           joy\n",
       "411970  0x1df874           joy\n",
       "411971  0x1c8e54       sadness\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(predictions)\n",
    "ans['pred'] = predictions\n",
    "ans1 = ans[['tweet_id','pred']]\n",
    "ans1 = ans1.rename(columns={\n",
    "    'tweet_id':'id',\n",
    "    'pred':'emotion'\n",
    "})\n",
    "ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1.to_csv('pre.csv',header = True,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
